{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Hybrid CNN+MLP Training (V3) with Context Windows and Probabilities\n",
    "\n",
    "Training enhanced version of Hybrid CNN+MLP model with probability outputs using extended context windows:\n",
    "- **Uses data with ±100ms context windows** (from `02.1_d-t_dl_data_preparation_with_context.ipynb`)\n",
    "- Enhanced CNN: 64→128→256→512 channels with channel attention\n",
    "- Enhanced MLP: 512→512→256→128 neurons\n",
    "- Residual connections with attention in CNN branch\n",
    "- Improved fusion layers: 512+128→512→256→128→64→2\n",
    "- Better training: 100-120 epochs, warmup, cosine annealing, gradient clipping\n",
    "- **Saves probabilities for each phoneme for error analysis**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "Data directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context\n",
      "Features directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context/features\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import math\n",
    "\n",
    "# Project root\n",
    "# Determine project root (parent of notebooks directory)\nPROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "\n",
    "# Data directory (with context)\n",
    "DATA_DIR = PROJECT_ROOT / 'artifacts' / 'd-t_dl_models_with_context'\n",
    "FEATURES_DIR = DATA_DIR / 'features'\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU device\")\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Features directory: {FEATURES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data with Context Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (132992, 112)\n",
      "Feature columns: 109\n",
      "\n",
      "Metadata columns present: ['phoneme_id', 'class', 'duration_ms']\n",
      "Dataset after filtering to d/t: 132992 samples\n",
      "\n",
      "Class encoding: {'d': np.int64(0), 't': np.int64(1)}\n",
      "Class distribution:\n",
      "class\n",
      "t    74454\n",
      "d    58538\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Split distribution:\n",
      "split\n",
      "train    93147\n",
      "test     19949\n",
      "val      19896\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectrograms: 100%|██████████| 132992/132992 [00:09<00:00, 14617.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 132,992 spectrograms\n",
      "Spectrogram shape: (128, 7)\n",
      "\n",
      "Dataset after filtering for spectrograms: 132992 samples\n"
     ]
    }
   ],
   "source": [
    "# Load feature columns\n",
    "with open(DATA_DIR / 'feature_cols.json', 'r') as f:\n",
    "    feature_cols = json.load(f)\n",
    "\n",
    "# Load feature scaler\n",
    "feature_scaler = joblib.load(DATA_DIR / 'feature_scaler.joblib')\n",
    "\n",
    "# Load class weights\n",
    "with open(DATA_DIR / 'class_weights.json', 'r') as f:\n",
    "    class_weights_dict = json.load(f)\n",
    "\n",
    "# Load features DataFrame (already contains all phoneme metadata with context windows)\n",
    "# This file was created in 02.1 and includes phoneme_id, class, duration_ms, and all extracted features\n",
    "df = pd.read_parquet(FEATURES_DIR / 'features.parquet')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Feature columns: {len(feature_cols)}\")\n",
    "\n",
    "# Check what metadata columns we have\n",
    "metadata_cols = ['phoneme_id', 'class', 'duration_ms', 'phoneme', 'utterance_id']\n",
    "present_metadata = [col for col in metadata_cols if col in df.columns]\n",
    "print(f\"\\nMetadata columns present: {present_metadata}\")\n",
    "\n",
    "# Note: features.parquet from 02.1 contains phoneme_id and class, but may not have utterance_id and phoneme\n",
    "# We need to check if we need to load additional metadata or if it's already there\n",
    "# For now, we'll work with what we have - phoneme_id and class should be sufficient\n",
    "\n",
    "# Handle class column - it should be in features.parquet from 02.1\n",
    "if 'class' not in df.columns:\n",
    "    if 'phoneme' in df.columns:\n",
    "        df['class'] = df['phoneme']\n",
    "        print(\"Created 'class' column from 'phoneme'\")\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'class' nor 'phoneme' column found in features.parquet. Make sure 02.1 was run successfully.\")\n",
    "\n",
    "# Filter to only d and t classes\n",
    "if 'd' in df['class'].values or 't' in df['class'].values:\n",
    "    df = df[df['class'].isin(['d', 't'])].copy()\n",
    "    print(f\"Dataset after filtering to d/t: {len(df)} samples\")\n",
    "\n",
    "# Encode target\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['class_encoded'] = le.fit_transform(df['class'])  # d=0, t=1\n",
    "print(f\"\\nClass encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "print(f\"Class distribution:\\n{df['class'].value_counts()}\")\n",
    "\n",
    "# Load split indices (these are DataFrame indices from 02.1)\n",
    "# After merge, indices may have changed, so we need to reset index first\n",
    "with open(DATA_DIR / 'split_indices.json', 'r') as f:\n",
    "    split_indices = json.load(f)\n",
    "\n",
    "# Reset index to ensure we can use the indices from split_indices\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Create split column based on indices\n",
    "df['split'] = 'train'\n",
    "if len(df) > max(split_indices['val'] + split_indices['test']):\n",
    "    # Indices are valid, use them directly\n",
    "    df.loc[split_indices['val'], 'split'] = 'val'\n",
    "    df.loc[split_indices['test'], 'split'] = 'test'\n",
    "else:\n",
    "    # If indices don't match (e.g., after filtering), use phoneme_id matching\n",
    "    # This is a fallback - normally indices should match\n",
    "    print(\"Warning: Split indices may not match DataFrame indices. Using phoneme_id matching...\")\n",
    "    val_ids = set(df.loc[split_indices['val'], 'phoneme_id'].values) if len(df) > max(split_indices['val']) else set()\n",
    "    test_ids = set(df.loc[split_indices['test'], 'phoneme_id'].values) if len(df) > max(split_indices['test']) else set()\n",
    "    df.loc[df['phoneme_id'].isin(val_ids), 'split'] = 'val'\n",
    "    df.loc[df['phoneme_id'].isin(test_ids), 'split'] = 'test'\n",
    "\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df['split'].value_counts())\n",
    "\n",
    "# Load spectrograms\n",
    "# Note: phoneme_id in h5 file might be string, so we keep it as string for matching\n",
    "spectrograms_dict = {}\n",
    "with h5py.File(FEATURES_DIR / 'spectrograms.h5', 'r') as f:\n",
    "    phoneme_ids = list(f.keys())\n",
    "    for phoneme_id in tqdm(phoneme_ids, desc=\"Loading spectrograms\"):\n",
    "        # Keep phoneme_id as string to match with DataFrame\n",
    "        spectrograms_dict[phoneme_id] = f[phoneme_id][:]\n",
    "\n",
    "print(f\"\\nLoaded {len(spectrograms_dict):,} spectrograms\")\n",
    "if spectrograms_dict:\n",
    "    print(f\"Spectrogram shape: {list(spectrograms_dict.values())[0].shape}\")\n",
    "\n",
    "# Filter to only phonemes with spectrograms\n",
    "# Convert phoneme_id to string for matching if needed\n",
    "df['phoneme_id_str'] = df['phoneme_id'].astype(str)\n",
    "df['has_spectrogram'] = df['phoneme_id_str'].isin(spectrograms_dict.keys())\n",
    "df = df[df['has_spectrogram']].copy()\n",
    "print(f\"\\nDataset after filtering for spectrograms: {len(df)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Classes and DataLoaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count mismatch detected: 108 features in DataFrame vs 109 in scaler\n",
      "Retraining scaler on train split with current features...\n",
      "Scaler retrained on 108 features\n",
      "Train dataset: 93147 samples\n",
      "Val dataset: 19896 samples\n",
      "Test dataset: 19949 samples\n",
      "\n",
      "Train batches: 1456\n",
      "Val batches: 311\n",
      "Test batches: 312\n",
      "\n",
      "Sample batch - Spectrogram shape: torch.Size([64, 1, 128, 7]), Features shape: torch.Size([64, 108]), Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    \"\"\"Dataset for hybrid models using both spectrograms and features\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, feature_cols, scaler=None, split='train', fit_scaler=False, transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.feature_cols = feature_cols\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract and scale features\n",
    "        X_features = self.df[feature_cols].values.astype(np.float32)\n",
    "        X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_features = self.scaler.fit_transform(X_features)\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            X_features = self.scaler.transform(X_features)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.X_features = torch.from_numpy(X_features)\n",
    "        self.y = torch.from_numpy(self.df['class_encoded'].values).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        # Get spectrogram\n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        if len(spectrogram.shape) == 2:\n",
    "            spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        features = self.X_features[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        return (torch.from_numpy(spectrogram), features), label\n",
    "\n",
    "# Create datasets\n",
    "train_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='train')\n",
    "val_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='val')\n",
    "test_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='test')\n",
    "\n",
    "print(f\"Train dataset: {len(train_hybrid_ds)} samples\")\n",
    "print(f\"Val dataset: {len(val_hybrid_ds)} samples\")\n",
    "print(f\"Test dataset: {len(test_hybrid_ds)} samples\")\n",
    "\n",
    "# Create weighted sampler for training\n",
    "train_labels = df[df['split'] == 'train']['class_encoded'].values\n",
    "class_weights_array = np.array([class_weights_dict.get(str(i), class_weights_dict.get(i, 1.0)) for i in range(2)])\n",
    "sample_weights = np.array([class_weights_array[label] for label in train_labels])\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_hybrid_loader = DataLoader(train_hybrid_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_hybrid_loader = DataLoader(val_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_hybrid_loader = DataLoader(test_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_hybrid_loader)}\")\n",
    "print(f\"Val batches: {len(val_hybrid_loader)}\")\n",
    "print(f\"Test batches: {len(test_hybrid_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "sample_batch = next(iter(train_hybrid_loader))\n",
    "print(f\"\\nSample batch - Spectrogram shape: {sample_batch[0][0].shape}, Features shape: {sample_batch[0][1].shape}, Labels shape: {sample_batch[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define Residual Block for CNN\n",
    "class ResidualBlock2D(nn.Module):\n",
    "    \"\"\"Residual block for CNN branch\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention module\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        out = avg_out + max_out\n",
    "        return x * out.view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "# Define Hybrid CNN+MLP Model V3\n",
    "class HybridCNNMLP_V3(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Hybrid model: CNN for spectrograms + MLP for features\n",
    "    Version 3: \n",
    "    - Deeper CNN: 64->128->256->512 channels with attention\n",
    "    - Enhanced MLP: 512->512->256->128 neurons\n",
    "    - Attention mechanism in CNN branch\n",
    "    - Improved fusion with attention\n",
    "    Input: (spectrogram: batch, 1, 128, 7), (features: batch, n_features)\n",
    "    Output: (batch, 2) - binary classification logits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features=109, num_classes=2, dropout=0.3):\n",
    "        super(HybridCNNMLP_V3, self).__init__()\n",
    "        \n",
    "        # Enhanced CNN branch with attention\n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),  # (64, 64, 3)\n",
    "            \n",
    "            ResidualBlock2D(64, 128),\n",
    "            ChannelAttention(128),\n",
    "            nn.MaxPool2d(2, 2),  # (128, 32, 1)\n",
    "            \n",
    "            ResidualBlock2D(128, 256),\n",
    "            ChannelAttention(256),\n",
    "            ResidualBlock2D(256, 512),\n",
    "            ChannelAttention(512),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Enhanced MLP branch\n",
    "        self.mlp_branch = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(512, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.75),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            \n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "        \n",
    "        # Enhanced Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 128, 512),  # CNN output (512) + MLP output (128)\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.75),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.25),\n",
    "            \n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        spectrogram, features = x\n",
    "        \n",
    "        # CNN branch\n",
    "        cnn_out = self.cnn_branch(spectrogram)  # (batch, 512)\n",
    "        \n",
    "        # MLP branch\n",
    "        mlp_out = self.mlp_branch(features)  # (batch, 128)\n",
    "        \n",
    "        # Concatenate\n",
    "        fused = torch.cat([cnn_out, mlp_out], dim=1)  # (batch, 640)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.fusion(fused)  # (batch, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return model configuration\"\"\"\n",
    "        return {\n",
    "            'model_type': 'HybridCNNMLP_V3',\n",
    "            'num_classes': 2,\n",
    "            'n_features': 109,\n",
    "            'input_shapes': {\n",
    "                'spectrogram': (1, 128, 7),\n",
    "                'features': (109,)\n",
    "            },\n",
    "            'version': '3.0'\n",
    "        }\n",
    "\n",
    "print(\"Model architecture defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Training utilities\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, max_grad_norm=None):\n",
    "    \"\"\"Train for one epoch with optional gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        # Handle hybrid model inputs: ((spectrograms, features), labels)\n",
    "        if isinstance(batch[0], (tuple, list)) and len(batch[0]) == 2:\n",
    "            inputs = tuple(x.to(device) for x in batch[0])\n",
    "        else:\n",
    "            inputs = batch[0].to(device)\n",
    "        \n",
    "        labels = batch[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        if max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
    "            # Handle hybrid model inputs: ((spectrograms, features), labels)\n",
    "            if isinstance(batch[0], (tuple, list)) and len(batch[0]) == 2:\n",
    "                inputs = tuple(x.to(device) for x in batch[0])\n",
    "            else:\n",
    "                inputs = batch[0].to(device)\n",
    "            \n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_labels, np.array(all_probs)[:, 1])\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds, all_labels, all_probs\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                device, num_epochs, save_dir, model_name, early_stopping_patience=10, max_grad_norm=None):\n",
    "    \"\"\"Train model with early stopping and optional gradient clipping\"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    training_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, max_grad_norm)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update learning rate\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Log metrics\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'val_roc_auc': val_metrics['roc_auc'],\n",
    "            'learning_rate': current_lr\n",
    "        }\n",
    "        training_history.append(epoch_metrics)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val F1: {val_metrics['f1']:.4f}, Val ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save model\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': best_val_f1,\n",
    "                'val_metrics': val_metrics\n",
    "            }, save_dir / 'best_model.pt')\n",
    "            \n",
    "            print(f\"✓ New best model saved! (F1: {best_val_f1:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                print(f\"Best F1: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
    "                break\n",
    "    \n",
    "    # Save training history\n",
    "    with open(save_dir / 'training_history.json', 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    # Save config\n",
    "    config = model.get_config() if hasattr(model, 'get_config') else {}\n",
    "    config.update({\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'num_epochs': num_epochs\n",
    "    })\n",
    "    with open(save_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    return training_history, best_epoch\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    metrics, preds, labels, probs = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(labels, preds, average=None, zero_division=0)\n",
    "    \n",
    "    metrics['precision_d'] = float(precision_per_class[0])\n",
    "    metrics['precision_t'] = float(precision_per_class[1])\n",
    "    metrics['recall_d'] = float(recall_per_class[0])\n",
    "    metrics['recall_t'] = float(recall_per_class[1])\n",
    "    metrics['f1_d'] = float(f1_per_class[0])\n",
    "    metrics['f1_t'] = float(f1_per_class[1])\n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds).tolist()\n",
    "    \n",
    "    return metrics, preds, labels, probs\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    \"\"\"Label smoothing cross entropy loss\"\"\"\n",
    "    def __init__(self, smoothing=0.1, weight=None):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        log_prob = F.log_softmax(pred, dim=1)\n",
    "        nll_loss = -log_prob.gather(dim=1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        if self.weight is not None:\n",
    "            nll_loss = nll_loss * self.weight[target]\n",
    "        \n",
    "        smooth_loss = -log_prob.mean(dim=1)\n",
    "        if self.weight is not None:\n",
    "            smooth_loss = smooth_loss * self.weight.mean()\n",
    "        \n",
    "        loss = (1.0 - self.smoothing) * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class WarmupCosineScheduler:\n",
    "    \"\"\"Learning rate scheduler with warmup and cosine annealing\"\"\"\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.min_lr = min_lr\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "        if self.current_epoch <= self.warmup_epochs:\n",
    "            # Warmup phase: linear increase\n",
    "            lr = self.base_lr * (self.current_epoch / self.warmup_epochs)\n",
    "        else:\n",
    "            # Cosine annealing phase\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def get_last_lr(self):\n",
    "        return [self.optimizer.param_groups[0]['lr']]\n",
    "\n",
    "print(\"Training utilities defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: HybridCNNMLP_V3\n",
      "Total parameters: 5,856,706\n",
      "Trainable parameters: 5,856,706\n",
      "\n",
      "Training configuration:\n",
      "- Epochs: 100\n",
      "- Warmup epochs: 5\n",
      "- Initial LR: 0.001\n",
      "- Label smoothing: 0.1\n",
      "- Gradient clipping: 1.0\n",
      "- Early stopping patience: 15\n",
      "- Dropout: 0.3\n",
      "- Context windows: ±100ms\n",
      "- Save directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context/improved_models/hybrid_cnn_mlp_v3_with_context\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "model = HybridCNNMLP_V3(n_features=len(feature_cols), num_classes=2, dropout=0.3).to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {model.get_config()['model_type']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Prepare class weights for loss function\n",
    "class_weights = torch.tensor([\n",
    "    class_weights_dict.get('0', class_weights_dict.get(0, 1.0)), \n",
    "    class_weights_dict.get('1', class_weights_dict.get(1, 1.0))\n",
    "], dtype=torch.float32).to(device)\n",
    "\n",
    "# Loss function with label smoothing\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1, weight=class_weights)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler with warmup and cosine annealing\n",
    "num_epochs = 100\n",
    "warmup_epochs = 5\n",
    "scheduler = WarmupCosineScheduler(optimizer, warmup_epochs=warmup_epochs, total_epochs=num_epochs, min_lr=1e-6)\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = DATA_DIR / 'improved_models'\n",
    "save_dir = OUTPUT_DIR / 'hybrid_cnn_mlp_v3_with_context'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"- Epochs: {num_epochs}\")\n",
    "print(f\"- Warmup epochs: {warmup_epochs}\")\n",
    "print(f\"- Initial LR: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"- Label smoothing: 0.1\")\n",
    "print(f\"- Gradient clipping: 1.0\")\n",
    "print(f\"- Early stopping patience: 15\")\n",
    "print(f\"- Dropout: 0.3\")\n",
    "print(f\"- Context windows: ±100ms\")\n",
    "print(f\"- Save directory: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3388, Train Acc: 0.9158\n",
      "Val Loss: 0.3267, Val Acc: 0.9282\n",
      "Val F1: 0.9280, Val ROC-AUC: 0.9792\n",
      "Learning Rate: 0.000200\n",
      "✓ New best model saved! (F1: 0.9280)\n",
      "\n",
      "Epoch 2/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2993, Train Acc: 0.9423\n",
      "Val Loss: 0.2935, Val Acc: 0.9432\n",
      "Val F1: 0.9433, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000400\n",
      "✓ New best model saved! (F1: 0.9433)\n",
      "\n",
      "Epoch 3/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2972, Train Acc: 0.9435\n",
      "Val Loss: 0.2849, Val Acc: 0.9494\n",
      "Val F1: 0.9494, Val ROC-AUC: 0.9879\n",
      "Learning Rate: 0.000600\n",
      "✓ New best model saved! (F1: 0.9494)\n",
      "\n",
      "Epoch 4/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2928, Train Acc: 0.9464\n",
      "Val Loss: 0.2900, Val Acc: 0.9466\n",
      "Val F1: 0.9467, Val ROC-AUC: 0.9874\n",
      "Learning Rate: 0.000800\n",
      "\n",
      "Epoch 5/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2929, Train Acc: 0.9471\n",
      "Val Loss: 0.2890, Val Acc: 0.9467\n",
      "Val F1: 0.9468, Val ROC-AUC: 0.9874\n",
      "Learning Rate: 0.001000\n",
      "\n",
      "Epoch 6/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2900, Train Acc: 0.9477\n",
      "Val Loss: 0.2859, Val Acc: 0.9483\n",
      "Val F1: 0.9484, Val ROC-AUC: 0.9875\n",
      "Learning Rate: 0.001000\n",
      "\n",
      "Epoch 7/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2874, Train Acc: 0.9495\n",
      "Val Loss: 0.2847, Val Acc: 0.9478\n",
      "Val F1: 0.9478, Val ROC-AUC: 0.9885\n",
      "Learning Rate: 0.000999\n",
      "\n",
      "Epoch 8/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2830, Train Acc: 0.9523\n",
      "Val Loss: 0.2807, Val Acc: 0.9533\n",
      "Val F1: 0.9533, Val ROC-AUC: 0.9890\n",
      "Learning Rate: 0.000998\n",
      "✓ New best model saved! (F1: 0.9533)\n",
      "\n",
      "Epoch 9/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2786, Train Acc: 0.9552\n",
      "Val Loss: 0.2938, Val Acc: 0.9401\n",
      "Val F1: 0.9403, Val ROC-AUC: 0.9896\n",
      "Learning Rate: 0.000996\n",
      "\n",
      "Epoch 10/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2754, Train Acc: 0.9570\n",
      "Val Loss: 0.2916, Val Acc: 0.9431\n",
      "Val F1: 0.9432, Val ROC-AUC: 0.9876\n",
      "Learning Rate: 0.000993\n",
      "\n",
      "Epoch 11/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2734, Train Acc: 0.9582\n",
      "Val Loss: 0.2732, Val Acc: 0.9573\n",
      "Val F1: 0.9574, Val ROC-AUC: 0.9907\n",
      "Learning Rate: 0.000990\n",
      "✓ New best model saved! (F1: 0.9574)\n",
      "\n",
      "Epoch 12/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2699, Train Acc: 0.9600\n",
      "Val Loss: 0.2877, Val Acc: 0.9512\n",
      "Val F1: 0.9511, Val ROC-AUC: 0.9902\n",
      "Learning Rate: 0.000987\n",
      "\n",
      "Epoch 13/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2696, Train Acc: 0.9606\n",
      "Val Loss: 0.2743, Val Acc: 0.9566\n",
      "Val F1: 0.9566, Val ROC-AUC: 0.9910\n",
      "Learning Rate: 0.000983\n",
      "\n",
      "Epoch 14/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2659, Train Acc: 0.9626\n",
      "Val Loss: 0.2766, Val Acc: 0.9529\n",
      "Val F1: 0.9530, Val ROC-AUC: 0.9908\n",
      "Learning Rate: 0.000978\n",
      "\n",
      "Epoch 15/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2627, Train Acc: 0.9648\n",
      "Val Loss: 0.2782, Val Acc: 0.9547\n",
      "Val F1: 0.9546, Val ROC-AUC: 0.9906\n",
      "Learning Rate: 0.000973\n",
      "\n",
      "Epoch 16/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2629, Train Acc: 0.9643\n",
      "Val Loss: 0.2790, Val Acc: 0.9526\n",
      "Val F1: 0.9526, Val ROC-AUC: 0.9907\n",
      "Learning Rate: 0.000967\n",
      "\n",
      "Epoch 17/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2588, Train Acc: 0.9673\n",
      "Val Loss: 0.2697, Val Acc: 0.9593\n",
      "Val F1: 0.9594, Val ROC-AUC: 0.9919\n",
      "Learning Rate: 0.000961\n",
      "✓ New best model saved! (F1: 0.9594)\n",
      "\n",
      "Epoch 18/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2567, Train Acc: 0.9682\n",
      "Val Loss: 0.2736, Val Acc: 0.9565\n",
      "Val F1: 0.9566, Val ROC-AUC: 0.9911\n",
      "Learning Rate: 0.000955\n",
      "\n",
      "Epoch 19/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2562, Train Acc: 0.9686\n",
      "Val Loss: 0.2748, Val Acc: 0.9558\n",
      "Val F1: 0.9558, Val ROC-AUC: 0.9905\n",
      "Learning Rate: 0.000947\n",
      "\n",
      "Epoch 20/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2549, Train Acc: 0.9696\n",
      "Val Loss: 0.2723, Val Acc: 0.9585\n",
      "Val F1: 0.9585, Val ROC-AUC: 0.9912\n",
      "Learning Rate: 0.000940\n",
      "\n",
      "Epoch 21/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2538, Train Acc: 0.9701\n",
      "Val Loss: 0.2725, Val Acc: 0.9585\n",
      "Val F1: 0.9586, Val ROC-AUC: 0.9905\n",
      "Learning Rate: 0.000932\n",
      "\n",
      "Epoch 22/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2526, Train Acc: 0.9710\n",
      "Val Loss: 0.2711, Val Acc: 0.9591\n",
      "Val F1: 0.9591, Val ROC-AUC: 0.9914\n",
      "Learning Rate: 0.000923\n",
      "\n",
      "Epoch 23/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2501, Train Acc: 0.9717\n",
      "Val Loss: 0.2723, Val Acc: 0.9566\n",
      "Val F1: 0.9566, Val ROC-AUC: 0.9917\n",
      "Learning Rate: 0.000914\n",
      "\n",
      "Epoch 24/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2488, Train Acc: 0.9730\n",
      "Val Loss: 0.2691, Val Acc: 0.9589\n",
      "Val F1: 0.9590, Val ROC-AUC: 0.9924\n",
      "Learning Rate: 0.000905\n",
      "\n",
      "Epoch 25/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2469, Train Acc: 0.9738\n",
      "Val Loss: 0.2715, Val Acc: 0.9597\n",
      "Val F1: 0.9597, Val ROC-AUC: 0.9914\n",
      "Learning Rate: 0.000895\n",
      "✓ New best model saved! (F1: 0.9597)\n",
      "\n",
      "Epoch 26/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2457, Train Acc: 0.9749\n",
      "Val Loss: 0.2776, Val Acc: 0.9561\n",
      "Val F1: 0.9561, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000884\n",
      "\n",
      "Epoch 27/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2461, Train Acc: 0.9749\n",
      "Val Loss: 0.2706, Val Acc: 0.9609\n",
      "Val F1: 0.9610, Val ROC-AUC: 0.9914\n",
      "Learning Rate: 0.000874\n",
      "✓ New best model saved! (F1: 0.9610)\n",
      "\n",
      "Epoch 28/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2430, Train Acc: 0.9764\n",
      "Val Loss: 0.2701, Val Acc: 0.9603\n",
      "Val F1: 0.9603, Val ROC-AUC: 0.9912\n",
      "Learning Rate: 0.000862\n",
      "\n",
      "Epoch 29/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2429, Train Acc: 0.9767\n",
      "Val Loss: 0.2705, Val Acc: 0.9599\n",
      "Val F1: 0.9600, Val ROC-AUC: 0.9913\n",
      "Learning Rate: 0.000851\n",
      "\n",
      "Epoch 30/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2425, Train Acc: 0.9771\n",
      "Val Loss: 0.2687, Val Acc: 0.9602\n",
      "Val F1: 0.9602, Val ROC-AUC: 0.9920\n",
      "Learning Rate: 0.000839\n",
      "\n",
      "Epoch 31/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2410, Train Acc: 0.9779\n",
      "Val Loss: 0.2710, Val Acc: 0.9612\n",
      "Val F1: 0.9612, Val ROC-AUC: 0.9912\n",
      "Learning Rate: 0.000826\n",
      "✓ New best model saved! (F1: 0.9612)\n",
      "\n",
      "Epoch 32/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2397, Train Acc: 0.9778\n",
      "Val Loss: 0.2699, Val Acc: 0.9601\n",
      "Val F1: 0.9602, Val ROC-AUC: 0.9915\n",
      "Learning Rate: 0.000814\n",
      "\n",
      "Epoch 33/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2371, Train Acc: 0.9805\n",
      "Val Loss: 0.2730, Val Acc: 0.9605\n",
      "Val F1: 0.9605, Val ROC-AUC: 0.9914\n",
      "Learning Rate: 0.000801\n",
      "\n",
      "Epoch 34/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2368, Train Acc: 0.9801\n",
      "Val Loss: 0.2813, Val Acc: 0.9521\n",
      "Val F1: 0.9522, Val ROC-AUC: 0.9898\n",
      "Learning Rate: 0.000787\n",
      "\n",
      "Epoch 35/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2369, Train Acc: 0.9802\n",
      "Val Loss: 0.2744, Val Acc: 0.9601\n",
      "Val F1: 0.9601, Val ROC-AUC: 0.9912\n",
      "Learning Rate: 0.000774\n",
      "\n",
      "Epoch 36/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2357, Train Acc: 0.9810\n",
      "Val Loss: 0.2714, Val Acc: 0.9604\n",
      "Val F1: 0.9604, Val ROC-AUC: 0.9916\n",
      "Learning Rate: 0.000760\n",
      "\n",
      "Epoch 37/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2337, Train Acc: 0.9823\n",
      "Val Loss: 0.2729, Val Acc: 0.9603\n",
      "Val F1: 0.9603, Val ROC-AUC: 0.9907\n",
      "Learning Rate: 0.000745\n",
      "\n",
      "Epoch 38/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2335, Train Acc: 0.9819\n",
      "Val Loss: 0.2746, Val Acc: 0.9600\n",
      "Val F1: 0.9600, Val ROC-AUC: 0.9914\n",
      "Learning Rate: 0.000731\n",
      "\n",
      "Epoch 39/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2324, Train Acc: 0.9825\n",
      "Val Loss: 0.2717, Val Acc: 0.9596\n",
      "Val F1: 0.9597, Val ROC-AUC: 0.9913\n",
      "Learning Rate: 0.000716\n",
      "\n",
      "Epoch 40/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2297, Train Acc: 0.9846\n",
      "Val Loss: 0.2726, Val Acc: 0.9616\n",
      "Val F1: 0.9615, Val ROC-AUC: 0.9914\n",
      "Learning Rate: 0.000701\n",
      "✓ New best model saved! (F1: 0.9615)\n",
      "\n",
      "Epoch 41/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2304, Train Acc: 0.9838\n",
      "Val Loss: 0.2772, Val Acc: 0.9571\n",
      "Val F1: 0.9572, Val ROC-AUC: 0.9905\n",
      "Learning Rate: 0.000686\n",
      "\n",
      "Epoch 42/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2272, Train Acc: 0.9861\n",
      "Val Loss: 0.2725, Val Acc: 0.9621\n",
      "Val F1: 0.9621, Val ROC-AUC: 0.9909\n",
      "Learning Rate: 0.000670\n",
      "✓ New best model saved! (F1: 0.9621)\n",
      "\n",
      "Epoch 43/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2284, Train Acc: 0.9852\n",
      "Val Loss: 0.2755, Val Acc: 0.9610\n",
      "Val F1: 0.9610, Val ROC-AUC: 0.9913\n",
      "Learning Rate: 0.000655\n",
      "\n",
      "Epoch 44/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2265, Train Acc: 0.9865\n",
      "Val Loss: 0.2882, Val Acc: 0.9547\n",
      "Val F1: 0.9546, Val ROC-AUC: 0.9901\n",
      "Learning Rate: 0.000639\n",
      "\n",
      "Epoch 45/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2267, Train Acc: 0.9866\n",
      "Val Loss: 0.2754, Val Acc: 0.9601\n",
      "Val F1: 0.9601, Val ROC-AUC: 0.9908\n",
      "Learning Rate: 0.000623\n",
      "\n",
      "Epoch 46/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2248, Train Acc: 0.9874\n",
      "Val Loss: 0.2771, Val Acc: 0.9587\n",
      "Val F1: 0.9587, Val ROC-AUC: 0.9907\n",
      "Learning Rate: 0.000607\n",
      "\n",
      "Epoch 47/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2249, Train Acc: 0.9872\n",
      "Val Loss: 0.2733, Val Acc: 0.9586\n",
      "Val F1: 0.9586, Val ROC-AUC: 0.9909\n",
      "Learning Rate: 0.000591\n",
      "\n",
      "Epoch 48/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2228, Train Acc: 0.9885\n",
      "Val Loss: 0.2746, Val Acc: 0.9603\n",
      "Val F1: 0.9603, Val ROC-AUC: 0.9897\n",
      "Learning Rate: 0.000575\n",
      "\n",
      "Epoch 49/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2210, Train Acc: 0.9897\n",
      "Val Loss: 0.2758, Val Acc: 0.9603\n",
      "Val F1: 0.9604, Val ROC-AUC: 0.9873\n",
      "Learning Rate: 0.000558\n",
      "\n",
      "Epoch 50/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2217, Train Acc: 0.9892\n",
      "Val Loss: 0.2808, Val Acc: 0.9590\n",
      "Val F1: 0.9589, Val ROC-AUC: 0.9901\n",
      "Learning Rate: 0.000542\n",
      "\n",
      "Epoch 51/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2204, Train Acc: 0.9898\n",
      "Val Loss: 0.2828, Val Acc: 0.9556\n",
      "Val F1: 0.9557, Val ROC-AUC: 0.9893\n",
      "Learning Rate: 0.000525\n",
      "\n",
      "Epoch 52/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2204, Train Acc: 0.9900\n",
      "Val Loss: 0.2748, Val Acc: 0.9599\n",
      "Val F1: 0.9600, Val ROC-AUC: 0.9890\n",
      "Learning Rate: 0.000509\n",
      "\n",
      "Epoch 53/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2191, Train Acc: 0.9907\n",
      "Val Loss: 0.2829, Val Acc: 0.9575\n",
      "Val F1: 0.9574, Val ROC-AUC: 0.9881\n",
      "Learning Rate: 0.000492\n",
      "\n",
      "Epoch 54/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2196, Train Acc: 0.9906\n",
      "Val Loss: 0.2813, Val Acc: 0.9582\n",
      "Val F1: 0.9582, Val ROC-AUC: 0.9892\n",
      "Learning Rate: 0.000476\n",
      "\n",
      "Epoch 55/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2180, Train Acc: 0.9914\n",
      "Val Loss: 0.2799, Val Acc: 0.9601\n",
      "Val F1: 0.9600, Val ROC-AUC: 0.9827\n",
      "Learning Rate: 0.000459\n",
      "\n",
      "Epoch 56/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2172, Train Acc: 0.9918\n",
      "Val Loss: 0.2756, Val Acc: 0.9611\n",
      "Val F1: 0.9611, Val ROC-AUC: 0.9867\n",
      "Learning Rate: 0.000443\n",
      "\n",
      "Epoch 57/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2170, Train Acc: 0.9919\n",
      "Val Loss: 0.2796, Val Acc: 0.9594\n",
      "Val F1: 0.9594, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000426\n",
      "\n",
      "Epoch 58/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2154, Train Acc: 0.9929\n",
      "Val Loss: 0.2802, Val Acc: 0.9604\n",
      "Val F1: 0.9604, Val ROC-AUC: 0.9816\n",
      "Learning Rate: 0.000410\n",
      "\n",
      "Epoch 59/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2163, Train Acc: 0.9924\n",
      "Val Loss: 0.2753, Val Acc: 0.9605\n",
      "Val F1: 0.9605, Val ROC-AUC: 0.9900\n",
      "Learning Rate: 0.000394\n",
      "\n",
      "Epoch 60/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2145, Train Acc: 0.9933\n",
      "Val Loss: 0.2808, Val Acc: 0.9588\n",
      "Val F1: 0.9588, Val ROC-AUC: 0.9828\n",
      "Learning Rate: 0.000378\n",
      "\n",
      "Epoch 61/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2146, Train Acc: 0.9932\n",
      "Val Loss: 0.2818, Val Acc: 0.9594\n",
      "Val F1: 0.9593, Val ROC-AUC: 0.9856\n",
      "Learning Rate: 0.000362\n",
      "\n",
      "Epoch 62/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2132, Train Acc: 0.9941\n",
      "Val Loss: 0.2792, Val Acc: 0.9578\n",
      "Val F1: 0.9578, Val ROC-AUC: 0.9862\n",
      "Learning Rate: 0.000346\n",
      "\n",
      "Epoch 63/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2139, Train Acc: 0.9937\n",
      "Val Loss: 0.2910, Val Acc: 0.9562\n",
      "Val F1: 0.9561, Val ROC-AUC: 0.9736\n",
      "Learning Rate: 0.000331\n",
      "\n",
      "Epoch 64/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2121, Train Acc: 0.9946\n",
      "Val Loss: 0.2853, Val Acc: 0.9573\n",
      "Val F1: 0.9573, Val ROC-AUC: 0.9792\n",
      "Learning Rate: 0.000315\n",
      "\n",
      "Epoch 65/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2119, Train Acc: 0.9949\n",
      "Val Loss: 0.2826, Val Acc: 0.9584\n",
      "Val F1: 0.9584, Val ROC-AUC: 0.9761\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 66/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2125, Train Acc: 0.9948\n",
      "Val Loss: 0.2803, Val Acc: 0.9589\n",
      "Val F1: 0.9589, Val ROC-AUC: 0.9860\n",
      "Learning Rate: 0.000285\n",
      "\n",
      "Epoch 67/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2116, Train Acc: 0.9953\n",
      "Val Loss: 0.2794, Val Acc: 0.9592\n",
      "Val F1: 0.9592, Val ROC-AUC: 0.9848\n",
      "Learning Rate: 0.000270\n",
      "\n",
      "Epoch 68/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2104, Train Acc: 0.9958\n",
      "Val Loss: 0.2811, Val Acc: 0.9601\n",
      "Val F1: 0.9600, Val ROC-AUC: 0.9814\n",
      "Learning Rate: 0.000256\n",
      "\n",
      "Epoch 69/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2104, Train Acc: 0.9958\n",
      "Val Loss: 0.2815, Val Acc: 0.9586\n",
      "Val F1: 0.9586, Val ROC-AUC: 0.9816\n",
      "Learning Rate: 0.000241\n",
      "\n",
      "Epoch 70/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2105, Train Acc: 0.9957\n",
      "Val Loss: 0.2847, Val Acc: 0.9569\n",
      "Val F1: 0.9569, Val ROC-AUC: 0.9721\n",
      "Learning Rate: 0.000227\n",
      "\n",
      "Epoch 71/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2098, Train Acc: 0.9959\n",
      "Val Loss: 0.2814, Val Acc: 0.9597\n",
      "Val F1: 0.9597, Val ROC-AUC: 0.9777\n",
      "Learning Rate: 0.000214\n",
      "\n",
      "Epoch 72/100\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2095, Train Acc: 0.9965\n",
      "Val Loss: 0.2856, Val Acc: 0.9591\n",
      "Val F1: 0.9591, Val ROC-AUC: 0.9670\n",
      "Learning Rate: 0.000200\n",
      "\n",
      "Early stopping at epoch 72\n",
      "Best F1: 0.9621 at epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final Test Results:\n",
      "============================================================\n",
      "Accuracy: 0.9600\n",
      "F1-score: 0.9600\n",
      "ROC-AUC: 0.9915\n",
      "Precision: 0.9600\n",
      "Recall: 0.9600\n",
      "Best epoch: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history, best_epoch = train_model(\n",
    "    model, train_hybrid_loader, val_hybrid_loader, criterion, optimizer, scheduler,\n",
    "    device, num_epochs=num_epochs, save_dir=save_dir, model_name='hybrid_cnn_mlp_v3_with_context', \n",
    "    early_stopping_patience=30, max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "checkpoint = torch.load(save_dir / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "test_metrics, test_preds, test_labels, test_probs = evaluate_model(model, test_hybrid_loader, criterion, device)\n",
    "\n",
    "# Save test metrics\n",
    "with open(save_dir / 'test_metrics.json', 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final Test Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions with Probabilities for Each Phoneme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions with probabilities to: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context/improved_models/hybrid_cnn_mlp_v3_with_context/test_predictions_with_probs.csv\n",
      "Total predictions: 19949\n",
      "Correct predictions: 19151\n",
      "Incorrect predictions: 798\n",
      "\n",
      "Summary Statistics:\n",
      "- Average confidence (correct): 0.9349\n",
      "- Average confidence (incorrect): 0.8121\n",
      "- High confidence errors (>0.8): 500\n",
      "- Low confidence errors (<0.6): 84\n"
     ]
    }
   ],
   "source": [
    "# Get test dataset to extract phoneme metadata\n",
    "test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "# Create predictions dataframe with probabilities\n",
    "predictions_data = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    predictions_data.append({\n",
    "        'phoneme_id': row['phoneme_id'],\n",
    "        'utterance_id': row.get('utterance_id', None),  # May not be in features.parquet\n",
    "        'phoneme': row.get('phoneme', row.get('class', None)),  # Use class if phoneme not available\n",
    "        'true_class': row['class'],\n",
    "        'true_class_encoded': int(test_labels[idx]),\n",
    "        'predicted_class_encoded': int(test_preds[idx]),\n",
    "        'predicted_class': 'd' if test_preds[idx] == 0 else 't',\n",
    "        'prob_class_0': float(test_probs[idx][0]),  # Probability of class 'd'\n",
    "        'prob_class_1': float(test_probs[idx][1]),  # Probability of class 't'\n",
    "        'max_prob': float(np.max(test_probs[idx])),\n",
    "        'is_correct': int(test_labels[idx] == test_preds[idx]),\n",
    "        'confidence': float(np.max(test_probs[idx])) if test_labels[idx] == test_preds[idx] else float(test_probs[idx][test_preds[idx]]),\n",
    "        'duration_ms': row.get('duration_ms', None)\n",
    "    })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Save to CSV\n",
    "predictions_df.to_csv(save_dir / 'test_predictions_with_probs.csv', index=False)\n",
    "print(f\"Saved predictions with probabilities to: {save_dir / 'test_predictions_with_probs.csv'}\")\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(f\"Correct predictions: {predictions_df['is_correct'].sum()}\")\n",
    "print(f\"Incorrect predictions: {(~predictions_df['is_correct'].astype(bool)).sum()}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_samples': len(predictions_df),\n",
    "    'correct_predictions': int(predictions_df['is_correct'].sum()),\n",
    "    'incorrect_predictions': int((~predictions_df['is_correct'].astype(bool)).sum()),\n",
    "    'accuracy': float(predictions_df['is_correct'].mean()),\n",
    "    'avg_confidence_correct': float(predictions_df[predictions_df['is_correct'] == 1]['confidence'].mean()),\n",
    "    'avg_confidence_incorrect': float(predictions_df[predictions_df['is_correct'] == 0]['confidence'].mean()),\n",
    "    'min_confidence_incorrect': float(predictions_df[predictions_df['is_correct'] == 0]['confidence'].min()),\n",
    "    'max_confidence_incorrect': float(predictions_df[predictions_df['is_correct'] == 0]['confidence'].max()),\n",
    "    'high_confidence_errors': int(((predictions_df['is_correct'] == 0) & (predictions_df['confidence'] > 0.8)).sum()),\n",
    "    'low_confidence_errors': int(((predictions_df['is_correct'] == 0) & (predictions_df['confidence'] < 0.6)).sum()),\n",
    "}\n",
    "\n",
    "with open(save_dir / 'predictions_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"- Average confidence (correct): {summary_stats['avg_confidence_correct']:.4f}\")\n",
    "print(f\"- Average confidence (incorrect): {summary_stats['avg_confidence_incorrect']:.4f}\")\n",
    "print(f\"- High confidence errors (>0.8): {summary_stats['high_confidence_errors']}\")\n",
    "print(f\"- Low confidence errors (<0.6): {summary_stats['low_confidence_errors']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions for Validation Set (for analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation predictions to: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context/improved_models/hybrid_cnn_mlp_v3_with_context/val_predictions_with_probs.csv\n"
     ]
    }
   ],
   "source": [
    "# Get validation predictions\n",
    "val_metrics, val_preds, val_labels, val_probs = evaluate_model(model, val_hybrid_loader, criterion, device)\n",
    "val_df = df[df['split'] == 'val'].reset_index(drop=True)\n",
    "\n",
    "val_predictions_data = []\n",
    "for idx, row in val_df.iterrows():\n",
    "    val_predictions_data.append({\n",
    "        'phoneme_id': row['phoneme_id'],\n",
    "        'utterance_id': row.get('utterance_id', None),  # May not be in features.parquet\n",
    "        'phoneme': row.get('phoneme', row.get('class', None)),  # Use class if phoneme not available\n",
    "        'true_class': row['class'],\n",
    "        'true_class_encoded': int(val_labels[idx]),\n",
    "        'predicted_class_encoded': int(val_preds[idx]),\n",
    "        'predicted_class': 'd' if val_preds[idx] == 0 else 't',\n",
    "        'prob_class_0': float(val_probs[idx][0]),\n",
    "        'prob_class_1': float(val_probs[idx][1]),\n",
    "        'max_prob': float(np.max(val_probs[idx])),\n",
    "        'is_correct': int(val_labels[idx] == val_preds[idx]),\n",
    "        'confidence': float(np.max(val_probs[idx])) if val_labels[idx] == val_preds[idx] else float(val_probs[idx][val_preds[idx]]),\n",
    "        'duration_ms': row.get('duration_ms', None)\n",
    "    })\n",
    "\n",
    "val_predictions_df = pd.DataFrame(val_predictions_data)\n",
    "val_predictions_df.to_csv(save_dir / 'val_predictions_with_probs.csv', index=False)\n",
    "print(f\"Saved validation predictions to: {save_dir / 'val_predictions_with_probs.csv'}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}