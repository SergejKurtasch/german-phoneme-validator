{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "93b7c7fd",
      "metadata": {},
      "source": [
        "# Download Thorsten-Voice TV-44kHz-Full\n",
        "\n",
        "This notebook downloads audio and builds a metadata table with direct links. Checkpoints and incremental progress are added for safe resume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "17c35e4c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install -q --upgrade datasets huggingface_hub pandas tqdm pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d5019d6d",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/SSanDisk/SpeechRec-German/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working dir: /Volumes/SSanDisk/SpeechRec-German\n",
            "Data dir: /Volumes/SSanDisk/SpeechRec-German/data_tv_44khz_full\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Audio, load_dataset\n",
        "from huggingface_hub import snapshot_download\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "REPO_ID = \"Thorsten-Voice/TV-44kHz-Full\"\n",
        "CONFIG = \"all\"  # use full dataset; see available configs in error message\n",
        "SPLIT = \"train\"\n",
        "TOTAL_ROWS_HINT = 39200  # public row count for progress bar\n",
        "CHUNK_SIZE = 1000  # rows per write/checkpoint batch\n",
        "\n",
        "BASE_DIR = Path.cwd()\n",
        "DATA_DIR = BASE_DIR / \"data_tv_44khz_full\"\n",
        "AUDIO_DIR = DATA_DIR / \"audio\"\n",
        "METADATA_CSV = DATA_DIR / \"tv_44khz_full_metadata.csv\"\n",
        "METADATA_PARQUET = DATA_DIR / \"tv_44khz_full_metadata.parquet\"\n",
        "CHECKPOINT_PATH = DATA_DIR / \"tv_44khz_full_checkpoint.json\"\n",
        "HF_BASE_URL = f\"https://huggingface.co/datasets/{REPO_ID}/resolve/main\"\n",
        "\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "AUDIO_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Working dir: {BASE_DIR}\")\n",
        "print(f\"Data dir: {DATA_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "43cad929",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Uncomment and run if the dataset requires authentication.\n",
        "# from huggingface_hub import login\n",
        "# login(token=\"<YOUR_HF_TOKEN>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7a9b8008",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_checkpoint():\n",
        "    if CHECKPOINT_PATH.exists():\n",
        "        with open(CHECKPOINT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "            try:\n",
        "                return int(json.load(f).get(\"last_index\", -1))\n",
        "            except Exception:\n",
        "                return -1\n",
        "    return -1\n",
        "\n",
        "def save_checkpoint(last_index: int):\n",
        "    CHECKPOINT_PATH.write_text(\n",
        "        json.dumps({\"last_index\": int(last_index), \"updated\": time.strftime(\"%Y-%m-%d %H:%M:%S\")}, ensure_ascii=False),\n",
        "        encoding=\"utf-8\",\n",
        "    )\n",
        "\n",
        "def append_chunk(chunk_rows):\n",
        "    if not chunk_rows:\n",
        "        return\n",
        "    header = not METADATA_CSV.exists()\n",
        "    df = pd.DataFrame(chunk_rows)\n",
        "    df.to_csv(METADATA_CSV, mode=\"a\", header=header, index=False)\n",
        "\n",
        "def row_to_meta(idx: int, row: dict):\n",
        "    audio_path = row[\"audio\"][\"path\"]\n",
        "    rel = audio_path.replace(\"hf://datasets/\", \"\")\n",
        "    repo_prefix = f\"{REPO_ID}/\"\n",
        "    subpath = rel[len(repo_prefix):] if rel.startswith(repo_prefix) else rel\n",
        "    audio_url = f\"{HF_BASE_URL}/{subpath}\"\n",
        "    local_path = str(AUDIO_DIR / subpath)\n",
        "\n",
        "    return {\n",
        "        \"idx\": idx,\n",
        "        \"id\": row.get(\"id\"),\n",
        "        \"subset\": row.get(\"subset\"),\n",
        "        \"style\": row.get(\"style\"),\n",
        "        \"text\": row.get(\"text\"),\n",
        "        \"samplerate\": row.get(\"samplerate\"),\n",
        "        \"durationSeconds\": row.get(\"durationSeconds\"),\n",
        "        \"recording_year_month\": row.get(\"recording_year-month\"),\n",
        "        \"microphone\": row.get(\"microphone\"),\n",
        "        \"language\": row.get(\"language\"),\n",
        "        \"comment\": row.get(\"comment\"),\n",
        "        \"audio_hub_path\": audio_path,\n",
        "        \"audio_url\": audio_url,\n",
        "        \"audio_local_path\": local_path,\n",
        "    }\n",
        "\n",
        "def iter_dataset(start_from: int = 0):\n",
        "    # Explicit config selection; load_dataset requires it for this repo\n",
        "    ds = load_dataset(REPO_ID, CONFIG, split=SPLIT, streaming=True)\n",
        "    ds = ds.cast_column(\"audio\", Audio(decode=False))\n",
        "    for idx, row in enumerate(ds):\n",
        "        if idx < start_from:\n",
        "            continue\n",
        "        yield idx, row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "65ce901d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resuming from index: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Metadata rows:   0%|          | 0/39200 [00:00<?, ?it/s]Some datasets params were ignored: ['homepage', 'license']. Make sure to use only valid params for the dataset builder and to have a up-to-date version of the `datasets` library.\n",
            "Metadata rows: 39248it [14:55, 43.81it/s]                           "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done. Last index saved: 39247\n",
            "Metadata file: /Volumes/SSanDisk/SpeechRec-German/data_tv_44khz_full/tv_44khz_full_metadata.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "start_from_checkpoint = load_checkpoint() + 1\n",
        "start_from_csv = 0\n",
        "if METADATA_CSV.exists():\n",
        "    try:\n",
        "        start_from_csv = len(pd.read_csv(METADATA_CSV))\n",
        "    except Exception:\n",
        "        start_from_csv = 0\n",
        "\n",
        "start_from = max(start_from_checkpoint, start_from_csv, 0)\n",
        "print(f\"Resuming from index: {start_from}\")\n",
        "\n",
        "buffer = []\n",
        "progress = tqdm(iter_dataset(start_from=start_from), initial=start_from, total=TOTAL_ROWS_HINT, desc=\"Metadata rows\")\n",
        "\n",
        "last_index = start_from - 1\n",
        "for idx, row in progress:\n",
        "    buffer.append(row_to_meta(idx, row))\n",
        "    last_index = idx\n",
        "    if len(buffer) >= CHUNK_SIZE:\n",
        "        append_chunk(buffer)\n",
        "        save_checkpoint(last_index)\n",
        "        buffer.clear()\n",
        "\n",
        "append_chunk(buffer)\n",
        "save_checkpoint(last_index)\n",
        "print(f\"Done. Last index saved: {last_index}\")\n",
        "print(f\"Metadata file: {METADATA_CSV}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Volumes/SSanDisk/SpeechRec-German/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:186: UserWarning: The `resume_download` argument is deprecated and ignored in `snapshot_download`. Downloads always resume whenever possible.\n",
            "  warnings.warn(\n",
            "/Volumes/SSanDisk/SpeechRec-German/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:202: UserWarning: The `local_dir_use_symlinks` argument is deprecated and ignored in `snapshot_download`. Downloading to a local directory does not use symlinks anymore.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting audio download (resumable)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 29 files:   0%|          | 0/29 [00:00<?, ?it/s]Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "Fetching 29 files: 100%|██████████| 29/29 [06:15<00:00, 12.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Audio stored under: /Volumes/SSanDisk/SpeechRec-German/data_tv_44khz_full/audio\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting audio download (resumable)...\")\n",
        "local_snapshot = snapshot_download(\n",
        "    repo_id=REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        "    local_dir=AUDIO_DIR,\n",
        "    local_dir_use_symlinks=False,\n",
        "    resume_download=True,\n",
        "    max_workers=8,\n",
        "    tqdm_class=tqdm,\n",
        ")\n",
        "print(f\"Audio stored under: {local_snapshot}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parquet saved: /Volumes/SSanDisk/SpeechRec-German/data_tv_44khz_full/tv_44khz_full_metadata.parquet (39248 rows)\n"
          ]
        }
      ],
      "source": [
        "if METADATA_CSV.exists():\n",
        "    df = pd.read_csv(METADATA_CSV)\n",
        "    df.to_parquet(METADATA_PARQUET, index=False)\n",
        "    print(f\"Parquet saved: {METADATA_PARQUET} ({len(df)} rows)\")\n",
        "else:\n",
        "    print(\"Metadata CSV not found yet.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
