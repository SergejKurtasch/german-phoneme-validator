{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-P Phoneme DL Data Preparation (with Context)\n",
    "\n",
    "Preparation of PyTorch datasets for deep learning models using phoneme audio with extended context windows:\n",
    "- **Extract features from context audio files** (±100ms context from `phoneme_wav_with_context`)\n",
    "- Extract spectrograms from context audio files\n",
    "- Create PyTorch Dataset classes for different input types\n",
    "- Train/Val/Test split with stratification\n",
    "- Data normalization\n",
    "- DataLoader creation with batch sampling\n",
    "- Handle class imbalance\n",
    "\n",
    "**Key difference from 02.2:** \n",
    "- Features are extracted **anew** from context audio files (~300ms duration) instead of using old features\n",
    "- Uses extended context windows (±100ms) for better capture of coarticulation, formant transitions, and VOT\n",
    "- All features (MFCC, energy, spectral, formants, quality metrics) reflect the extended context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parselmouth not installed. Will use LPC for formant extraction.\n",
      "Using MPS device\n",
      "Project root: /Volumes/SSanDisk/SpeechRec-German\n",
      "Phoneme audio directory (with context): /Volumes/SSanDisk/SpeechRec-German/artifacts/phoneme_wav_with_context\n",
      "Output directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context\n",
      "Features output directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/features\n",
      "Extract features: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import h5py\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import signal\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import optional libraries\n",
    "try:\n",
    "    import parselmouth\n",
    "    HAS_PARSELMOUTH = True\n",
    "except ImportError:\n",
    "    HAS_PARSELMOUTH = False\n",
    "    print(\"Warning: parselmouth not installed. Will use LPC for formant extraction.\")\n",
    "\n",
    "# Configuration\n",
    "# Determine project root (parent of notebooks directory)\nPROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "# Audio files with context are in the main artifacts directory\n",
    "PHONEME_WAV_DIR = PROJECT_ROOT / 'artifacts' / 'phoneme_wav_with_context'  # With context!\n",
    "PHONEMES_FILE = PROJECT_ROOT / 'artifacts' / 'phoneme_intervals.csv'  # Load from fresh phoneme intervals file\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'artifacts' / 'b-p_dl_models_with_context'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURES_OUTPUT_DIR = OUTPUT_DIR / 'features'\n",
    "FEATURES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Feature extraction parameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "MFCC_N_COEFFS = 13\n",
    "SPECTROGRAM_WINDOW_MS = 200  # For spectrograms (will be longer with context)\n",
    "\n",
    "# Files to save/load\n",
    "FEATURES_FILE = FEATURES_OUTPUT_DIR / 'features.parquet'\n",
    "SPECTROGRAMS_FILE = FEATURES_OUTPUT_DIR / 'spectrograms.h5'\n",
    "\n",
    "# Flag to control whether to extract features or load existing\n",
    "EXTRACT_FEATURES = True  # Set to False to skip extraction and load existing features\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU device\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Phoneme audio directory (with context): {PHONEME_WAV_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"Features output directory: {FEATURES_OUTPUT_DIR}\")\n",
    "print(f\"Extract features: {EXTRACT_FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Feature extraction functions (extracted from context audio files)\n",
    "def extract_mfcc_features(audio, sr=SAMPLE_RATE, n_mfcc=MFCC_N_COEFFS):\n",
    "    \"\"\"Extract MFCC features and their deltas.\"\"\"\n",
    "    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=n_mfcc, hop_length=HOP_LENGTH)\n",
    "    \n",
    "    # Handle short audio files: adjust delta width based on available frames\n",
    "    n_frames = mfcc.shape[1]\n",
    "    default_width = 9\n",
    "    \n",
    "    if n_frames < default_width:\n",
    "        if n_frames < 3:\n",
    "            calculated_width = 3\n",
    "        elif n_frames < 9:\n",
    "            calculated_width = n_frames if n_frames % 2 == 1 else n_frames - 1\n",
    "            calculated_width = max(3, calculated_width)\n",
    "        else:\n",
    "            calculated_width = 9\n",
    "        if calculated_width % 2 == 0:\n",
    "            calculated_width = max(3, calculated_width - 1)\n",
    "        delta_mfcc = librosa.feature.delta(mfcc, width=calculated_width, mode='nearest')\n",
    "        delta2_mfcc = librosa.feature.delta(mfcc, order=2, width=calculated_width, mode='nearest')\n",
    "    else:\n",
    "        delta_mfcc = librosa.feature.delta(mfcc)\n",
    "        delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "    \n",
    "    return {\n",
    "        'mfcc_mean': np.mean(mfcc, axis=1),\n",
    "        'mfcc_std': np.std(mfcc, axis=1),\n",
    "        'delta_mfcc_mean': np.mean(delta_mfcc, axis=1),\n",
    "        'delta_mfcc_std': np.std(delta_mfcc, axis=1),\n",
    "        'delta2_mfcc_mean': np.mean(delta2_mfcc, axis=1),\n",
    "        'delta2_mfcc_std': np.std(delta2_mfcc, axis=1),\n",
    "    }\n",
    "\n",
    "def extract_energy_features(audio, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract energy-related features.\"\"\"\n",
    "    rms = librosa.feature.rms(y=audio, hop_length=HOP_LENGTH)[0]\n",
    "    zcr = librosa.feature.zero_crossing_rate(audio, hop_length=HOP_LENGTH)[0]\n",
    "    \n",
    "    return {\n",
    "        'energy_rms': np.mean(rms),\n",
    "        'energy_rms_std': np.std(rms),\n",
    "        'energy_zcr': np.mean(zcr),\n",
    "        'energy_zcr_std': np.std(zcr),\n",
    "    }\n",
    "\n",
    "def extract_spectral_features(audio, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract spectral features.\"\"\"\n",
    "    centroid = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=HOP_LENGTH)[0]\n",
    "    rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, hop_length=HOP_LENGTH)[0]\n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr, hop_length=HOP_LENGTH)[0]\n",
    "    contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, hop_length=HOP_LENGTH)\n",
    "    \n",
    "    return {\n",
    "        'spectral_centroid': np.mean(centroid),\n",
    "        'spectral_centroid_std': np.std(centroid),\n",
    "        'spectral_rolloff': np.mean(rolloff),\n",
    "        'spectral_rolloff_std': np.std(rolloff),\n",
    "        'spectral_bandwidth': np.mean(bandwidth),\n",
    "        'spectral_bandwidth_std': np.std(bandwidth),\n",
    "        'spectral_contrast_mean': np.mean(contrast, axis=1),\n",
    "    }\n",
    "\n",
    "def extract_formants_lpc(audio, sr=SAMPLE_RATE, n_formants=4, order=10):\n",
    "    \"\"\"Extract formants using LPC (Linear Predictive Coding).\"\"\"\n",
    "    # Pre-emphasis filter\n",
    "    audio_pre = signal.lfilter([1, -0.97], 1, audio)\n",
    "    \n",
    "    # Frame the signal\n",
    "    frame_length = int(0.025 * sr)  # 25ms frames\n",
    "    hop_length_frame = int(0.010 * sr)  # 10ms hop\n",
    "    \n",
    "    formants_list = []\n",
    "    \n",
    "    for i in range(0, len(audio_pre) - frame_length, hop_length_frame):\n",
    "        frame = audio_pre[i:i+frame_length]\n",
    "        windowed = frame * signal.windows.hann(len(frame))\n",
    "        \n",
    "        try:\n",
    "            # Compute autocorrelation\n",
    "            autocorr = np.correlate(windowed, windowed, mode='full')\n",
    "            autocorr = autocorr[len(autocorr)//2:len(autocorr)//2+order+1]\n",
    "            \n",
    "            # Levinson-Durbin recursion\n",
    "            a = np.zeros(order + 1)\n",
    "            a[0] = 1.0\n",
    "            e = autocorr[0]\n",
    "            \n",
    "            for j in range(1, order + 1):\n",
    "                k = -np.sum(a[:j] * autocorr[j:0:-1]) / e\n",
    "                a[1:j+1] = a[1:j+1] + k * a[j-1::-1]\n",
    "                a[j] = k\n",
    "                e = e * (1 - k * k)\n",
    "            \n",
    "            roots = np.roots(a)\n",
    "            roots = roots[np.imag(roots) >= 0]\n",
    "            angles = np.angle(roots)\n",
    "            freqs = angles * (sr / (2 * np.pi))\n",
    "            magnitudes = np.abs(roots)\n",
    "            \n",
    "            # Filter: formants should have high magnitude and be in valid frequency range\n",
    "            freq_mag_pairs = [(f, m) for f, m in zip(freqs, magnitudes) if 50 < f < sr/2 and m > 0.7]\n",
    "            freq_mag_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "            freqs = [f for f, m in freq_mag_pairs]\n",
    "            \n",
    "            formants = freqs[:n_formants]\n",
    "            while len(formants) < n_formants:\n",
    "                formants.append(0.0)\n",
    "            \n",
    "            formants_list.append(formants[:n_formants])\n",
    "        except Exception:\n",
    "            formants_list.append([0.0] * n_formants)\n",
    "    \n",
    "    if len(formants_list) == 0:\n",
    "        return {\n",
    "            'formant_f1': 0.0, 'formant_f2': 0.0, 'formant_f3': 0.0, 'formant_f4': 0.0,\n",
    "            'formant_f1_std': 0.0, 'formant_f2_std': 0.0, 'formant_f3_std': 0.0, 'formant_f4_std': 0.0,\n",
    "        }\n",
    "    \n",
    "    formants_array = np.array(formants_list)\n",
    "    \n",
    "    return {\n",
    "        'formant_f1': np.mean(formants_array[:, 0]) if len(formants_array) > 0 and np.any(formants_array[:, 0] > 0) else 0.0,\n",
    "        'formant_f2': np.mean(formants_array[:, 1]) if len(formants_array) > 0 and np.any(formants_array[:, 1] > 0) else 0.0,\n",
    "        'formant_f3': np.mean(formants_array[:, 2]) if len(formants_array) > 0 and np.any(formants_array[:, 2] > 0) else 0.0,\n",
    "        'formant_f4': np.mean(formants_array[:, 3]) if len(formants_array) > 0 and np.any(formants_array[:, 3] > 0) else 0.0,\n",
    "        'formant_f1_std': np.std(formants_array[:, 0]) if len(formants_array) > 0 and np.any(formants_array[:, 0] > 0) else 0.0,\n",
    "        'formant_f2_std': np.std(formants_array[:, 1]) if len(formants_array) > 0 and np.any(formants_array[:, 1] > 0) else 0.0,\n",
    "        'formant_f3_std': np.std(formants_array[:, 2]) if len(formants_array) > 0 and np.any(formants_array[:, 2] > 0) else 0.0,\n",
    "        'formant_f4_std': np.std(formants_array[:, 3]) if len(formants_array) > 0 and np.any(formants_array[:, 3] > 0) else 0.0,\n",
    "    }\n",
    "\n",
    "def extract_formants_parselmouth(audio, sr=SAMPLE_RATE, n_formants=4):\n",
    "    \"\"\"Extract formants using Parselmouth (Praat).\"\"\"\n",
    "    if not HAS_PARSELMOUTH:\n",
    "        return extract_formants_lpc(audio, sr, n_formants)\n",
    "    \n",
    "    try:\n",
    "        sound = parselmouth.Sound(audio, sampling_frequency=sr)\n",
    "        formant = sound.to_formant_burg(time_step=0.01)\n",
    "        \n",
    "        formants_list = []\n",
    "        times = np.arange(0, sound.duration, 0.01)\n",
    "        \n",
    "        for t in times:\n",
    "            formants = []\n",
    "            for i in range(1, n_formants + 1):\n",
    "                try:\n",
    "                    f = formant.get_value_at_time(i, t)\n",
    "                    if f > 0:\n",
    "                        formants.append(f)\n",
    "                    else:\n",
    "                        formants.append(0.0)\n",
    "                except:\n",
    "                    formants.append(0.0)\n",
    "            formants_list.append(formants)\n",
    "        \n",
    "        formants_array = np.array(formants_list)\n",
    "        \n",
    "        return {\n",
    "            'formant_f1': np.mean(formants_array[:, 0]) if len(formants_array) > 0 and np.any(formants_array[:, 0] > 0) else 0.0,\n",
    "            'formant_f2': np.mean(formants_array[:, 1]) if len(formants_array) > 0 and np.any(formants_array[:, 1] > 0) else 0.0,\n",
    "            'formant_f3': np.mean(formants_array[:, 2]) if len(formants_array) > 0 and np.any(formants_array[:, 2] > 0) else 0.0,\n",
    "            'formant_f4': np.mean(formants_array[:, 3]) if len(formants_array) > 0 and np.any(formants_array[:, 3] > 0) else 0.0,\n",
    "            'formant_f1_std': np.std(formants_array[:, 0]) if len(formants_array) > 0 and np.any(formants_array[:, 0] > 0) else 0.0,\n",
    "            'formant_f2_std': np.std(formants_array[:, 1]) if len(formants_array) > 0 and np.any(formants_array[:, 1] > 0) else 0.0,\n",
    "            'formant_f3_std': np.std(formants_array[:, 2]) if len(formants_array) > 0 and np.any(formants_array[:, 2] > 0) else 0.0,\n",
    "            'formant_f4_std': np.std(formants_array[:, 3]) if len(formants_array) > 0 and np.any(formants_array[:, 3] > 0) else 0.0,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return extract_formants_lpc(audio, sr, n_formants)\n",
    "\n",
    "def extract_quality_metrics(audio, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract quality metrics for noise assessment.\"\"\"\n",
    "    stft = librosa.stft(audio, hop_length=HOP_LENGTH)\n",
    "    magnitude = np.abs(stft)\n",
    "    magnitude = magnitude + 1e-10\n",
    "    \n",
    "    geometric_mean = np.exp(np.mean(np.log(magnitude), axis=0))\n",
    "    arithmetic_mean = np.mean(magnitude, axis=0)\n",
    "    spectral_flatness = geometric_mean / (arithmetic_mean + 1e-10)\n",
    "    spectral_flatness_mean = np.mean(spectral_flatness)\n",
    "    \n",
    "    harmonic = librosa.effects.harmonic(audio)\n",
    "    percussive = librosa.effects.percussive(audio)\n",
    "    hnr = np.mean(harmonic**2) / (np.mean(percussive**2) + 1e-10)\n",
    "    \n",
    "    zcr = librosa.feature.zero_crossing_rate(audio, hop_length=HOP_LENGTH)[0]\n",
    "    zcr_mean = np.mean(zcr)\n",
    "    \n",
    "    rms = librosa.feature.rms(y=audio, hop_length=HOP_LENGTH)[0]\n",
    "    energy_std = np.std(rms)\n",
    "    energy_mean = np.mean(rms)\n",
    "    energy_cv = energy_std / (energy_mean + 1e-10)\n",
    "    \n",
    "    return {\n",
    "        'spectral_flatness': spectral_flatness_mean,\n",
    "        'harmonic_noise_ratio': hnr,\n",
    "        'zcr_mean': zcr_mean,\n",
    "        'energy_cv': energy_cv,\n",
    "    }\n",
    "\n",
    "def extract_all_features(audio_path, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract all features from an audio file.\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
    "        \n",
    "        features = {}\n",
    "        features.update(extract_mfcc_features(audio, sr))\n",
    "        features.update(extract_energy_features(audio, sr))\n",
    "        features.update(extract_spectral_features(audio, sr))\n",
    "        \n",
    "        if HAS_PARSELMOUTH:\n",
    "            features.update(extract_formants_parselmouth(audio, sr))\n",
    "        else:\n",
    "            features.update(extract_formants_lpc(audio, sr))\n",
    "        \n",
    "        features.update(extract_quality_metrics(audio, sr))\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def extract_spectrogram_window(audio_path, target_duration_ms=SPECTROGRAM_WINDOW_MS, sr=SAMPLE_RATE):\n",
    "    \"\"\"Extract mel-spectrogram with fixed window size.\"\"\"\n",
    "    try:\n",
    "        audio, _ = librosa.load(audio_path, sr=sr, mono=True)\n",
    "        audio_duration_ms = len(audio) / sr * 1000\n",
    "        \n",
    "        target_samples = int(target_duration_ms / 1000 * sr)\n",
    "        \n",
    "        if len(audio) < target_samples:\n",
    "            padding = target_samples - len(audio)\n",
    "            audio = np.pad(audio, (0, padding), mode='constant')\n",
    "        elif len(audio) > target_samples:\n",
    "            audio = audio[:target_samples]\n",
    "        \n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio, \n",
    "            sr=sr, \n",
    "            n_mels=N_MELS, \n",
    "            hop_length=HOP_LENGTH,\n",
    "            fmax=sr/2\n",
    "        )\n",
    "        \n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        \n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "print(\"Feature extraction functions defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Features from Context Audio Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading phoneme metadata from phoneme_intervals.csv...\n",
      "Phonemes shape (before filtering): (1337749, 5)\n",
      "Phonemes columns: ['utterance_id', 'phoneme', 'start_ms', 'end_ms', 'duration_ms']\n",
      "\n",
      "Filtering to only 'b' and 'p' phonemes...\n",
      "Phonemes shape (after filtering b/p): (36903, 5)\n",
      "Phoneme distribution:\n",
      "phoneme\n",
      "b    25874\n",
      "p    11029\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Creating phoneme_id column...\n",
      "Created 36903 unique phoneme IDs\n",
      "\n",
      "Creating class column...\n",
      "Class distribution:\n",
      "class\n",
      "b    25874\n",
      "p    11029\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updating audio paths to use context audio files...\n",
      "Creating audio_path column...\n",
      "\n",
      "Final dataset size: 36,903 phonemes with context audio\n",
      "\n",
      "Features file already exists: /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/features/features.parquet\n",
      "Set EXTRACT_FEATURES=False to skip extraction and load existing features.\n",
      "Proceeding with extraction (will overwrite existing file)...\n",
      "\n",
      "============================================================\n",
      "EXTRACTING FEATURES FROM CONTEXT AUDIO FILES\n",
      "============================================================\n",
      "This will process 36,903 phonemes with context windows\n",
      "Audio files are longer (~300ms) due to ±100ms context\n",
      "============================================================\n",
      "\n",
      "Using 8 parallel workers...\n",
      "Extracting features from context audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 36903/36903 [08:00<00:00, 76.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted features for 36,903 phonemes\n",
      "\n",
      "Feature columns: 32\n",
      "Feature shape: (36903, 32)\n",
      "\n",
      "After expanding arrays: 110 columns\n",
      "Feature shape: (36903, 110)\n",
      "\n",
      "Performing quality assessment...\n",
      "Detected 3,691 outliers (10.0%)\n",
      "\n",
      "Saving features to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/features/features.parquet...\n",
      "Features saved! Shape: (36903, 112)\n",
      "\n",
      "Extracting spectrograms from context audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting spectrograms: 100%|██████████| 36903/36903 [01:11<00:00, 518.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 36,903 spectrograms\n",
      "Spectrogram shape: (128, 7)\n",
      "\n",
      "Saving spectrograms to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/features/spectrograms.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving spectrograms: 100%|██████████| 36903/36903 [00:04<00:00, 8135.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms saved!\n"
     ]
    }
   ],
   "source": [
    "# Load phoneme metadata from fresh phoneme_intervals.csv file\n",
    "print(f\"Loading phoneme metadata from {PHONEMES_FILE.name}...\")\n",
    "df_phonemes = pd.read_csv(PHONEMES_FILE)\n",
    "print(f\"Phonemes shape (before filtering): {df_phonemes.shape}\")\n",
    "print(f\"Phonemes columns: {list(df_phonemes.columns)}\")\n",
    "\n",
    "# Filter to only 'b' and 'p' phonemes for this notebook\n",
    "print(\"\\nFiltering to only 'b' and 'p' phonemes...\")\n",
    "df_phonemes = df_phonemes[df_phonemes['phoneme'].isin(['b', 'p'])].copy()\n",
    "print(f\"Phonemes shape (after filtering b/p): {df_phonemes.shape}\")\n",
    "print(f\"Phoneme distribution:\")\n",
    "print(df_phonemes['phoneme'].value_counts())\n",
    "\n",
    "# Create phoneme_id column (unique identifier for each phoneme)\n",
    "print(\"\\nCreating phoneme_id column...\")\n",
    "df_phonemes['phoneme_id'] = range(len(df_phonemes))\n",
    "print(f\"Created {len(df_phonemes)} unique phoneme IDs\")\n",
    "\n",
    "# Create class column (same as phoneme for this notebook)\n",
    "print(\"\\nCreating class column...\")\n",
    "df_phonemes['class'] = df_phonemes['phoneme']\n",
    "print(f\"Class distribution:\")\n",
    "print(df_phonemes['class'].value_counts())\n",
    "\n",
    "# Update audio paths to use context audio directory\n",
    "print(\"\\nUpdating audio paths to use context audio files...\")\n",
    "def find_context_audio_path(row):\n",
    "    \"\"\"Find the corresponding audio file in phoneme_wav_with_context\"\"\"\n",
    "    utt_id = row['utterance_id']\n",
    "    phoneme = row.get('phoneme', row.get('class', ''))\n",
    "    start_ms = int(row['start_ms'])\n",
    "    end_ms = int(row['end_ms'])\n",
    "    \n",
    "    pattern = f\"{utt_id}__{phoneme}__{start_ms}-{end_ms}.wav\"\n",
    "    audio_path = PHONEME_WAV_DIR / pattern\n",
    "    \n",
    "    if audio_path.exists():\n",
    "        return str(audio_path)\n",
    "    \n",
    "    if PHONEME_WAV_DIR.exists():\n",
    "        matching_files = list(PHONEME_WAV_DIR.glob(f\"{utt_id}__{phoneme}__*.wav\"))\n",
    "        if matching_files:\n",
    "            return str(matching_files[0])\n",
    "    \n",
    "    original_path = Path(row.get('audio_path', ''))\n",
    "    if original_path.exists():\n",
    "        return str(original_path)\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Update audio paths\n",
    "if 'audio_path' in df_phonemes.columns:\n",
    "    print(\"Updating audio_path column...\")\n",
    "    df_phonemes['audio_path'] = df_phonemes.apply(find_context_audio_path, axis=1)\n",
    "    valid_paths = df_phonemes['audio_path'].notna()\n",
    "    print(f\"Found context audio files: {valid_paths.sum()} / {len(df_phonemes)}\")\n",
    "    if not valid_paths.all():\n",
    "        print(f\"Warning: {len(df_phonemes) - valid_paths.sum()} audio files not found in context directory\")\n",
    "else:\n",
    "    print(\"Creating audio_path column...\")\n",
    "    df_phonemes['audio_path'] = df_phonemes.apply(find_context_audio_path, axis=1)\n",
    "\n",
    "# Filter to only phonemes with valid audio paths\n",
    "df_phonemes = df_phonemes[df_phonemes['audio_path'].notna()].copy()\n",
    "print(f\"\\nFinal dataset size: {len(df_phonemes):,} phonemes with context audio\")\n",
    "\n",
    "# Check if features already exist\n",
    "if EXTRACT_FEATURES and FEATURES_FILE.exists():\n",
    "    print(f\"\\nFeatures file already exists: {FEATURES_FILE}\")\n",
    "    print(\"Set EXTRACT_FEATURES=False to skip extraction and load existing features.\")\n",
    "    print(\"Proceeding with extraction (will overwrite existing file)...\")\n",
    "\n",
    "if EXTRACT_FEATURES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EXTRACTING FEATURES FROM CONTEXT AUDIO FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"This will process {len(df_phonemes):,} phonemes with context windows\")\n",
    "    print(f\"Audio files are longer (~300ms) due to ±100ms context\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Helper function for parallel processing\n",
    "    def process_single_phoneme(row_data):\n",
    "        \"\"\"Process a single phoneme row and extract features.\"\"\"\n",
    "        idx, row = row_data\n",
    "        audio_path = row['audio_path']\n",
    "        if audio_path is None or not Path(audio_path).exists():\n",
    "            return None\n",
    "        \n",
    "        features = extract_all_features(audio_path)\n",
    "        if features is not None:\n",
    "            features['phoneme_id'] = row['phoneme_id']\n",
    "            features['class'] = row['class']\n",
    "            features['duration_ms'] = row['duration_ms']\n",
    "        return features\n",
    "    \n",
    "    # Use parallel processing\n",
    "    num_workers = os.cpu_count() or 4\n",
    "    print(f\"Using {num_workers} parallel workers...\")\n",
    "    \n",
    "    features_list = []\n",
    "    rows_to_process = [(idx, row) for idx, row in df_phonemes.iterrows()]\n",
    "    \n",
    "    print(\"Extracting features from context audio files...\")\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_row = {executor.submit(process_single_phoneme, row_data): row_data \n",
    "                      for row_data in rows_to_process}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_row), total=len(rows_to_process), desc=\"Extracting features\"):\n",
    "            try:\n",
    "                features = future.result()\n",
    "                if features is not None:\n",
    "                    features_list.append(features)\n",
    "            except Exception as e:\n",
    "                idx, row = future_to_row[future]\n",
    "                print(f\"Error processing phoneme {row.get('phoneme_id', 'unknown')}: {e}\")\n",
    "    \n",
    "    print(f\"\\nExtracted features for {len(features_list):,} phonemes\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_features = pd.DataFrame(features_list)\n",
    "    print(f\"\\nFeature columns: {len(df_features.columns)}\")\n",
    "    print(f\"Feature shape: {df_features.shape}\")\n",
    "    \n",
    "    # Expand array columns into separate columns\n",
    "    array_columns = []\n",
    "    for col in df_features.columns:\n",
    "        if df_features[col].dtype == 'object':\n",
    "            sample = df_features[col].iloc[0] if len(df_features) > 0 else None\n",
    "            if sample is not None and isinstance(sample, np.ndarray):\n",
    "                array_columns.append(col)\n",
    "    \n",
    "    # Expand array columns\n",
    "    for col in array_columns:\n",
    "        array_length = len(df_features[col].iloc[0])\n",
    "        for i in range(array_length):\n",
    "            new_col_name = f\"{col}_{i}\"\n",
    "            df_features[new_col_name] = df_features[col].apply(\n",
    "                lambda x: x[i] if isinstance(x, np.ndarray) and len(x) > i else np.nan\n",
    "            )\n",
    "        df_features = df_features.drop(columns=[col])\n",
    "    \n",
    "    print(f\"\\nAfter expanding arrays: {len(df_features.columns)} columns\")\n",
    "    print(f\"Feature shape: {df_features.shape}\")\n",
    "    \n",
    "    # Quality assessment and outlier detection\n",
    "    print(\"\\nPerforming quality assessment...\")\n",
    "    numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c not in ['phoneme_id', 'duration_ms']]\n",
    "    numeric_cols = [c for c in numeric_cols if df_features[c].std() > 1e-10]\n",
    "    \n",
    "    X_outlier = df_features[numeric_cols].fillna(0)\n",
    "    scaler_outlier = StandardScaler()\n",
    "    X_scaled = scaler_outlier.fit_transform(X_outlier)\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=RANDOM_STATE)\n",
    "    outlier_labels = iso_forest.fit_predict(X_scaled)\n",
    "    df_features['is_outlier_iso'] = outlier_labels == -1\n",
    "    print(f\"Detected {df_features['is_outlier_iso'].sum():,} outliers ({df_features['is_outlier_iso'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Quality score\n",
    "    df_features['quality_score'] = (\n",
    "        (1 - df_features['spectral_flatness'].fillna(0.5)) * 0.3 +\n",
    "        (df_features['harmonic_noise_ratio'].fillna(1.0) / (df_features['harmonic_noise_ratio'].fillna(1.0).max() + 1e-10)) * 0.3 +\n",
    "        (1 - df_features['zcr_mean'].fillna(0.5) / (df_features['zcr_mean'].fillna(0.5).max() + 1e-10)) * 0.2 +\n",
    "        (1 - df_features['energy_cv'].fillna(1.0) / (df_features['energy_cv'].fillna(1.0).max() + 1e-10)) * 0.2\n",
    "    )\n",
    "    \n",
    "    # Save features\n",
    "    print(f\"\\nSaving features to {FEATURES_FILE}...\")\n",
    "    df_features.to_parquet(FEATURES_FILE, index=False)\n",
    "    print(f\"Features saved! Shape: {df_features.shape}\")\n",
    "    \n",
    "    # Extract spectrograms from context audio\n",
    "    print(f\"\\nExtracting spectrograms from context audio files...\")\n",
    "    spectrograms_dict = {}\n",
    "    \n",
    "    for idx, row in tqdm(df_phonemes.iterrows(), total=len(df_phonemes), desc=\"Extracting spectrograms\"):\n",
    "        audio_path = row['audio_path']\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        if audio_path is None or not Path(audio_path).exists():\n",
    "            continue\n",
    "        \n",
    "        spec = extract_spectrogram_window(audio_path, target_duration_ms=SPECTROGRAM_WINDOW_MS)\n",
    "        if spec is not None:\n",
    "            spectrograms_dict[phoneme_id] = spec\n",
    "    \n",
    "    print(f\"\\nExtracted {len(spectrograms_dict):,} spectrograms\")\n",
    "    if spectrograms_dict:\n",
    "        print(f\"Spectrogram shape: {list(spectrograms_dict.values())[0].shape}\")\n",
    "    \n",
    "    # Save spectrograms\n",
    "    print(f\"\\nSaving spectrograms to {SPECTROGRAMS_FILE}...\")\n",
    "    with h5py.File(SPECTROGRAMS_FILE, 'w') as f:\n",
    "        for phoneme_id, spec in tqdm(spectrograms_dict.items(), desc=\"Saving spectrograms\"):\n",
    "            f.create_dataset(str(phoneme_id), data=spec, compression='gzip')\n",
    "    print(f\"Spectrograms saved!\")\n",
    "    \n",
    "else:\n",
    "    # Load existing features\n",
    "    print(f\"\\nLoading existing features from {FEATURES_FILE}...\")\n",
    "    df_features = pd.read_parquet(FEATURES_FILE)\n",
    "    print(f\"Features shape: {df_features.shape}\")\n",
    "    print(f\"Features columns: {len(df_features.columns)}\")\n",
    "    \n",
    "    # Load existing spectrograms\n",
    "    print(f\"\\nLoading existing spectrograms from {SPECTROGRAMS_FILE}...\")\n",
    "    spectrograms_dict = {}\n",
    "    with h5py.File(SPECTROGRAMS_FILE, 'r') as f:\n",
    "        phoneme_ids = list(f.keys())\n",
    "        for phoneme_id in tqdm(phoneme_ids, desc=\"Loading spectrograms\"):\n",
    "            spectrograms_dict[phoneme_id] = f[phoneme_id][:]\n",
    "    print(f\"Loaded {len(spectrograms_dict):,} spectrograms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge Features and Phoneme Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging features with phoneme metadata...\n",
      "Merged dataset shape: (36903, 119)\n",
      "\n",
      "'class' column found in merged DataFrame\n",
      "\n",
      "Class distribution:\n",
      "class\n",
      "b    25874\n",
      "p    11029\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution (%):\n",
      "class\n",
      "b    70.113541\n",
      "p    29.886459\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class encoding: {'b': np.int64(0), 'p': np.int64(1)}\n",
      "\n",
      "Phonemes with spectrograms: 36903 / 36903\n",
      "\n",
      "Number of feature columns: 109\n",
      "First 10 features: ['energy_rms', 'energy_rms_std', 'energy_zcr', 'energy_zcr_std', 'spectral_centroid', 'spectral_centroid_std', 'spectral_rolloff', 'spectral_rolloff_std', 'spectral_bandwidth', 'spectral_bandwidth_std']\n"
     ]
    }
   ],
   "source": [
    "# Merge features with phoneme metadata\n",
    "print(\"Merging features with phoneme metadata...\")\n",
    "df = df_phonemes.merge(df_features, on='phoneme_id', how='inner', suffixes=('', '_features'))\n",
    "print(f\"Merged dataset shape: {df.shape}\")\n",
    "\n",
    "# Handle duplicate columns from merge\n",
    "if 'class_features' in df.columns:\n",
    "    df = df.drop(columns=['class_features'])\n",
    "if 'class' not in df.columns:\n",
    "    if 'phoneme' in df.columns:\n",
    "        print(\"\\n'class' column not found, creating from 'phoneme' column...\")\n",
    "        df['class'] = df['phoneme']\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'class' nor 'phoneme' column found in merged DataFrame\")\n",
    "else:\n",
    "    print(\"\\n'class' column found in merged DataFrame\")\n",
    "\n",
    "# Filter to only b and p classes (exclude pf if present)\n",
    "if 'pf' in df['class'].values:\n",
    "    print(\"\\nFiltering out 'pf' class, keeping only 'b' and 'p'...\")\n",
    "    df = df[df['class'].isin(['b', 'p'])].copy()\n",
    "    print(f\"Dataset after filtering: {len(df)} samples\")\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "print(f\"\\nClass distribution (%):\")\n",
    "print(df['class'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "df['class_encoded'] = le.fit_transform(df['class'])  # b=0, p=1\n",
    "print(f\"\\nClass encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Check which phonemes have spectrograms\n",
    "df['has_spectrogram'] = df['phoneme_id'].isin(spectrograms_dict.keys())\n",
    "print(f\"\\nPhonemes with spectrograms: {df['has_spectrogram'].sum()} / {len(df)}\")\n",
    "\n",
    "# Get feature columns (exclude metadata and non-numeric columns)\n",
    "# Note: duration_ms_y (from features) should be included, but duration_ms_x (from phonemes) should be excluded\n",
    "exclude_cols = ['phoneme_id', 'utterance_id', 'phoneme', 'class', 'class_x', 'class_y', \n",
    "                'class_encoded', 'start_ms', 'end_ms', 'duration_ms', 'duration_ms_x', \n",
    "                'audio_path', 'is_outlier_iso', 'split', 'has_spectrogram',\n",
    "                'class_features']  # Exclude merge suffixes (but keep duration_ms_y)\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Filter to only numeric columns (using pd.api.types for consistency with data_loader.py)\n",
    "feature_cols = [col for col in feature_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "print(f\"\\nNumber of feature columns: {len(feature_cols)}\")\n",
    "print(f\"First 10 features: {feature_cols[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Val/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/feature_cols.json\n"
     ]
    }
   ],
   "source": [
    "# Save feature columns list (important for model loading)\n",
    "# This file is needed for loading models later\n",
    "with open(OUTPUT_DIR / 'feature_cols.json', 'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print(f\"Feature columns saved to {OUTPUT_DIR / 'feature_cols.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after filtering: 36903 samples\n",
      "\n",
      "Train set: 25,846 samples (70.0%)\n",
      "  Class distribution: [18122  7724]\n",
      "Val set: 5,521 samples (15.0%)\n",
      "  Class distribution: [3871 1650]\n",
      "Test set: 5,536 samples (15.0%)\n",
      "  Class distribution: [3881 1655]\n",
      "\n",
      "Split indices saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/split_indices.json\n"
     ]
    }
   ],
   "source": [
    "# Filter to only phonemes with spectrograms\n",
    "df = df[df['has_spectrogram']].copy()\n",
    "print(f\"Dataset after filtering: {len(df)} samples\")\n",
    "\n",
    "# Train/Val/Test split (70/15/15) with stratification\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df.index, df['class_encoded'], \n",
    "    test_size=0.15, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=df['class_encoded']\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.176,  # 0.176 ≈ 15/85\n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Create split column\n",
    "df['split'] = 'train'\n",
    "df.loc[X_val, 'split'] = 'val'\n",
    "df.loc[X_test, 'split'] = 'test'\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_train, 'class_encoded'])}\")\n",
    "print(f\"Val set: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_val, 'class_encoded'])}\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_test, 'class_encoded'])}\")\n",
    "\n",
    "# Save split indices\n",
    "split_indices = {\n",
    "    'train': [int(idx) for idx in X_train],\n",
    "    'val': [int(idx) for idx in X_val],\n",
    "    'test': [int(idx) for idx in X_test]\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'split_indices.json', 'w') as f:\n",
    "    json.dump(split_indices, f)\n",
    "print(f\"\\nSplit indices saved to {OUTPUT_DIR / 'split_indices.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create PyTorch Dataset Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset classes defined!\n"
     ]
    }
   ],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"Dataset for models using spectrograms only\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, split='train', transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        # Get spectrogram\n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        \n",
    "        # Add channel dimension if needed (for CNN: [1, 128, 7])\n",
    "        if len(spectrogram.shape) == 2:\n",
    "            spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return torch.from_numpy(spectrogram), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    \"\"\"Dataset for models using extracted features only\"\"\"\n",
    "    def __init__(self, df, feature_cols, scaler=None, split='train', fit_scaler=False):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "        # Extract features\n",
    "        X = self.df[feature_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Scale features\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            X = self.scaler.transform(X)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(self.df['class_encoded'].values).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    \"\"\"Dataset for hybrid models using both spectrograms and features\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, feature_cols, scaler=None, split='train', fit_scaler=False, transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.feature_cols = feature_cols\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract and scale features\n",
    "        X_features = self.df[feature_cols].values.astype(np.float32)\n",
    "        X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_features = self.scaler.fit_transform(X_features)\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            X_features = self.scaler.transform(X_features)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.X_features = torch.from_numpy(X_features)\n",
    "        self.y = torch.from_numpy(self.df['class_encoded'].values).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        # Get spectrogram\n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        if len(spectrogram.shape) == 2:\n",
    "            spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        features = self.X_features[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        return (torch.from_numpy(spectrogram), features), label\n",
    "\n",
    "\n",
    "class RawAudioDataset(Dataset):\n",
    "    \"\"\"Dataset for models using raw audio waveforms (with context)\"\"\"\n",
    "    def __init__(self, df, split='train', sample_rate=16000, max_length=None, transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = row['audio_path']\n",
    "        \n",
    "        # Load audio (now with context - longer duration)\n",
    "        try:\n",
    "            if pd.notna(audio_path) and Path(audio_path).exists():\n",
    "                audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)\n",
    "            else:\n",
    "                # If path is invalid, return zeros\n",
    "                audio = np.zeros(self.sample_rate // 5)  # 200ms of silence (longer for context)\n",
    "        except:\n",
    "            # If loading fails, return zeros\n",
    "            audio = np.zeros(self.sample_rate // 5)  # 200ms of silence\n",
    "        \n",
    "        # Normalize audio\n",
    "        if len(audio) > 0:\n",
    "            audio = audio / (np.abs(audio).max() + 1e-8)\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        # Note: With context, audio is longer, so max_length should be adjusted accordingly\n",
    "        if self.max_length is not None:\n",
    "            if len(audio) < self.max_length:\n",
    "                audio = np.pad(audio, (0, self.max_length - len(audio)), mode='constant')\n",
    "            else:\n",
    "                audio = audio[:self.max_length]\n",
    "        \n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return torch.from_numpy(audio.astype(np.float32)), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class ContextAudioDataset(Dataset):\n",
    "    \"\"\"Dataset for models using raw audio with context from original utterance\"\"\"\n",
    "    def __init__(self, df, split='train', sample_rate=16000, transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_audio_path = row['audio_path']\n",
    "        \n",
    "        # Load phoneme audio (already includes context from phoneme_wav_with_context)\n",
    "        try:\n",
    "            if pd.notna(phoneme_audio_path) and Path(phoneme_audio_path).exists():\n",
    "                phoneme_audio, sr = librosa.load(phoneme_audio_path, sr=self.sample_rate, mono=True)\n",
    "            else:\n",
    "                phoneme_audio = np.zeros(self.sample_rate // 5)  # 200ms\n",
    "        except:\n",
    "            phoneme_audio = np.zeros(self.sample_rate // 5)\n",
    "        \n",
    "        # The audio already contains context, so we use it directly\n",
    "        context_audio = phoneme_audio  # Already includes ±100ms context\n",
    "        \n",
    "        # Normalize\n",
    "        if len(phoneme_audio) > 0:\n",
    "            phoneme_audio = phoneme_audio / (np.abs(phoneme_audio).max() + 1e-8)\n",
    "        if len(context_audio) > 0:\n",
    "            context_audio = context_audio / (np.abs(context_audio).max() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            phoneme_audio = self.transform(phoneme_audio)\n",
    "            context_audio = self.transform(context_audio)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return (\n",
    "            torch.from_numpy(phoneme_audio.astype(np.float32)),\n",
    "            torch.from_numpy(context_audio.astype(np.float32))\n",
    "        ), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence models (LSTM, Transformer) using spectrograms as sequences\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, split='train', transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        # Get spectrogram: shape (128, 7) -> (7, 128) for sequence models\n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        spectrogram = spectrogram.T  # Transpose: (7, 128) - 7 time steps, 128 features\n",
    "        \n",
    "        # Normalize\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return torch.from_numpy(spectrogram), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "print(\"Dataset classes defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaler saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/feature_scaler.joblib\n",
      "\n",
      "Class weights: {0: np.float64(0.7131111356362433), 1: np.float64(1.6730968410150182)}\n",
      "Class weights saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/class_weights.json\n",
      "\n",
      "All datasets created!\n",
      "Train spectrogram dataset: 25846 samples\n",
      "Train feature dataset: 25846 samples\n",
      "Train hybrid dataset: 25846 samples\n",
      "Train sequence dataset: 25846 samples\n",
      "Train raw audio dataset: 25846 samples\n",
      "Train context audio dataset: 25846 samples\n"
     ]
    }
   ],
   "source": [
    "# Create feature scaler on training data\n",
    "train_df = df[df['split'] == 'train']\n",
    "X_train_features = train_df[feature_cols].values.astype(np.float32)\n",
    "X_train_features = np.nan_to_num(X_train_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "feature_scaler.fit(X_train_features)\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(feature_scaler, OUTPUT_DIR / 'feature_scaler.joblib')\n",
    "print(f\"Feature scaler saved to {OUTPUT_DIR / 'feature_scaler.joblib'}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(df[df['split'] == 'train']['class_encoded']),\n",
    "    y=df[df['split'] == 'train']['class_encoded']\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(f\"\\nClass weights: {class_weights_dict}\")\n",
    "\n",
    "# Save class weights\n",
    "with open(OUTPUT_DIR / 'class_weights.json', 'w') as f:\n",
    "    json.dump(class_weights_dict, f)\n",
    "print(f\"Class weights saved to {OUTPUT_DIR / 'class_weights.json'}\")\n",
    "\n",
    "# Create datasets\n",
    "train_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='train')\n",
    "val_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='val')\n",
    "test_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='test')\n",
    "\n",
    "train_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='train')\n",
    "val_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='val')\n",
    "test_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='test')\n",
    "\n",
    "train_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='train')\n",
    "val_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='val')\n",
    "test_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='test')\n",
    "\n",
    "train_sequence_ds = SequenceDataset(df, spectrograms_dict, split='train')\n",
    "val_sequence_ds = SequenceDataset(df, spectrograms_dict, split='val')\n",
    "test_sequence_ds = SequenceDataset(df, spectrograms_dict, split='test')\n",
    "\n",
    "# Note: With context, audio is longer (~300ms instead of ~100ms), so adjust max_length accordingly\n",
    "# Original phoneme ~100ms + 200ms context = ~300ms total\n",
    "# At 16kHz: 300ms = 4800 samples\n",
    "train_raw_audio_ds = RawAudioDataset(df, split='train', sample_rate=16000, max_length=4800)  # ~300ms at 16kHz\n",
    "val_raw_audio_ds = RawAudioDataset(df, split='val', sample_rate=16000, max_length=4800)\n",
    "test_raw_audio_ds = RawAudioDataset(df, split='test', sample_rate=16000, max_length=4800)\n",
    "\n",
    "train_context_audio_ds = ContextAudioDataset(df, split='train', sample_rate=16000)\n",
    "val_context_audio_ds = ContextAudioDataset(df, split='val', sample_rate=16000)\n",
    "test_context_audio_ds = ContextAudioDataset(df, split='test', sample_rate=16000)\n",
    "\n",
    "print(\"\\nAll datasets created!\")\n",
    "print(f\"Train spectrogram dataset: {len(train_spectrogram_ds)} samples\")\n",
    "print(f\"Train feature dataset: {len(train_feature_ds)} samples\")\n",
    "print(f\"Train hybrid dataset: {len(train_hybrid_ds)} samples\")\n",
    "print(f\"Train sequence dataset: {len(train_sequence_ds)} samples\")\n",
    "print(f\"Train raw audio dataset: {len(train_raw_audio_ds)} samples\")\n",
    "print(f\"Train context audio dataset: {len(train_context_audio_ds)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DataLoaders with Weighted Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataLoaders created!\n",
      "\n",
      "Train batches (spectrogram): 404\n",
      "Train batches (feature): 404\n",
      "Train batches (hybrid): 404\n",
      "\n",
      "Testing a batch from spectrogram dataset...\n",
      "Batch shape: torch.Size([64, 1, 128, 7]), Labels shape: torch.Size([64])\n",
      "\n",
      "Testing a batch from raw audio dataset (with context)...\n",
      "Audio batch shape: torch.Size([64, 4800]), Labels shape: torch.Size([64])\n",
      "Audio duration: 300.0ms (expected ~300ms with context)\n"
     ]
    }
   ],
   "source": [
    "# Compute sample weights for weighted sampling\n",
    "train_labels = df[df['split'] == 'train']['class_encoded'].values\n",
    "sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_spectrogram_loader = DataLoader(train_spectrogram_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_spectrogram_loader = DataLoader(val_spectrogram_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_spectrogram_loader = DataLoader(test_spectrogram_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_feature_loader = DataLoader(train_feature_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_feature_loader = DataLoader(val_feature_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_feature_loader = DataLoader(test_feature_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_hybrid_loader = DataLoader(train_hybrid_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_hybrid_loader = DataLoader(val_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_hybrid_loader = DataLoader(test_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_sequence_loader = DataLoader(train_sequence_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_sequence_loader = DataLoader(val_sequence_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_sequence_loader = DataLoader(test_sequence_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_raw_audio_loader = DataLoader(train_raw_audio_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_raw_audio_loader = DataLoader(val_raw_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_raw_audio_loader = DataLoader(test_raw_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_context_audio_loader = DataLoader(train_context_audio_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_context_audio_loader = DataLoader(val_context_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_context_audio_loader = DataLoader(test_context_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"All DataLoaders created!\")\n",
    "print(f\"\\nTrain batches (spectrogram): {len(train_spectrogram_loader)}\")\n",
    "print(f\"Train batches (feature): {len(train_feature_loader)}\")\n",
    "print(f\"Train batches (hybrid): {len(train_hybrid_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "print(\"\\nTesting a batch from spectrogram dataset...\")\n",
    "sample_batch = next(iter(train_spectrogram_loader))\n",
    "print(f\"Batch shape: {sample_batch[0].shape}, Labels shape: {sample_batch[1].shape}\")\n",
    "\n",
    "# Test raw audio batch (should be longer due to context)\n",
    "print(\"\\nTesting a batch from raw audio dataset (with context)...\")\n",
    "sample_audio_batch = next(iter(train_raw_audio_loader))\n",
    "print(f\"Audio batch shape: {sample_audio_batch[0].shape}, Labels shape: {sample_audio_batch[1].shape}\")\n",
    "print(f\"Audio duration: {sample_audio_batch[0].shape[1] / 16000 * 1000:.1f}ms (expected ~300ms with context)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Dataset Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models_with_context/dataset_info.json\n",
      "\n",
      "Dataset summary:\n",
      "  Total samples: 36903\n",
      "  Train: 25846\n",
      "  Val: 5521\n",
      "  Test: 5536\n",
      "  Features: 109\n",
      "  Spectrogram shape: [128, 7]\n",
      "  Context window: ±100ms\n",
      "  Audio max length: 4800 samples (~300ms)\n",
      "  Audio source: phoneme_wav_with_context\n"
     ]
    }
   ],
   "source": [
    "# Save dataset information\n",
    "dataset_info = {\n",
    "    'total_samples': len(df),\n",
    "    'train_samples': len(df[df['split'] == 'train']),\n",
    "    'val_samples': len(df[df['split'] == 'val']),\n",
    "    'test_samples': len(df[df['split'] == 'test']),\n",
    "    'n_features': len(feature_cols),\n",
    "    'spectrogram_shape': list(spectrograms_dict[list(spectrograms_dict.keys())[0]].shape),\n",
    "    'context_window_ms': 100,  # ±100ms context\n",
    "    'audio_max_length_samples': 4800,  # ~300ms at 16kHz (100ms phoneme + 200ms context)\n",
    "    'class_distribution': {\n",
    "        'train': df[df['split'] == 'train']['class'].value_counts().to_dict(),\n",
    "        'val': df[df['split'] == 'val']['class'].value_counts().to_dict(),\n",
    "        'test': df[df['split'] == 'test']['class'].value_counts().to_dict()\n",
    "    },\n",
    "    'class_weights': class_weights_dict,\n",
    "    'feature_columns': feature_cols,\n",
    "    'audio_source': 'phoneme_wav_with_context'\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'dataset_info.json', 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "\n",
    "print(f\"Dataset info saved to {OUTPUT_DIR / 'dataset_info.json'}\")\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Total samples: {dataset_info['total_samples']}\")\n",
    "print(f\"  Train: {dataset_info['train_samples']}\")\n",
    "print(f\"  Val: {dataset_info['val_samples']}\")\n",
    "print(f\"  Test: {dataset_info['test_samples']}\")\n",
    "print(f\"  Features: {dataset_info['n_features']}\")\n",
    "print(f\"  Spectrogram shape: {dataset_info['spectrogram_shape']}\")\n",
    "print(f\"  Context window: ±{dataset_info['context_window_ms']}ms\")\n",
    "print(f\"  Audio max length: {dataset_info['audio_max_length_samples']} samples (~{dataset_info['audio_max_length_samples']/16:.0f}ms)\")\n",
    "print(f\"  Audio source: {dataset_info['audio_source']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}