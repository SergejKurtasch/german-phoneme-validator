{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models from Improved Models\n",
    "\n",
    "Creating ensembles from improved V2 models:\n",
    "- Weighted voting ensemble\n",
    "- Stacking ensemble with meta-model\n",
    "- Comparison of different ensemble methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "Columns in df_phonemes: ['phoneme_id', 'utterance_id', 'phoneme', 'class', 'start_ms', 'end_ms', 'duration_ms', 'audio_path']\n",
      "Columns in df_features: ['energy_rms', 'energy_rms_std', 'energy_zcr', 'energy_zcr_std', 'spectral_centroid', 'spectral_centroid_std', 'spectral_rolloff', 'spectral_rolloff_std', 'spectral_bandwidth', 'spectral_bandwidth_std', 'formant_f1', 'formant_f2', 'formant_f3', 'formant_f4', 'formant_f1_std', 'formant_f2_std', 'formant_f3_std', 'formant_f4_std', 'spectral_flatness', 'harmonic_noise_ratio', 'zcr_mean', 'energy_cv', 'phoneme_id', 'class', 'duration_ms', 'mfcc_mean_0', 'mfcc_mean_1', 'mfcc_mean_2', 'mfcc_mean_3', 'mfcc_mean_4', 'mfcc_mean_5', 'mfcc_mean_6', 'mfcc_mean_7', 'mfcc_mean_8', 'mfcc_mean_9', 'mfcc_mean_10', 'mfcc_mean_11', 'mfcc_mean_12', 'mfcc_std_0', 'mfcc_std_1', 'mfcc_std_2', 'mfcc_std_3', 'mfcc_std_4', 'mfcc_std_5', 'mfcc_std_6', 'mfcc_std_7', 'mfcc_std_8', 'mfcc_std_9', 'mfcc_std_10', 'mfcc_std_11', 'mfcc_std_12', 'delta_mfcc_mean_0', 'delta_mfcc_mean_1', 'delta_mfcc_mean_2', 'delta_mfcc_mean_3', 'delta_mfcc_mean_4', 'delta_mfcc_mean_5', 'delta_mfcc_mean_6', 'delta_mfcc_mean_7', 'delta_mfcc_mean_8', 'delta_mfcc_mean_9', 'delta_mfcc_mean_10', 'delta_mfcc_mean_11', 'delta_mfcc_mean_12', 'delta_mfcc_std_0', 'delta_mfcc_std_1', 'delta_mfcc_std_2', 'delta_mfcc_std_3', 'delta_mfcc_std_4', 'delta_mfcc_std_5', 'delta_mfcc_std_6', 'delta_mfcc_std_7', 'delta_mfcc_std_8', 'delta_mfcc_std_9', 'delta_mfcc_std_10', 'delta_mfcc_std_11', 'delta_mfcc_std_12', 'delta2_mfcc_mean_0', 'delta2_mfcc_mean_1', 'delta2_mfcc_mean_2', 'delta2_mfcc_mean_3', 'delta2_mfcc_mean_4', 'delta2_mfcc_mean_5', 'delta2_mfcc_mean_6', 'delta2_mfcc_mean_7', 'delta2_mfcc_mean_8', 'delta2_mfcc_mean_9', 'delta2_mfcc_mean_10', 'delta2_mfcc_mean_11', 'delta2_mfcc_mean_12', 'delta2_mfcc_std_0', 'delta2_mfcc_std_1', 'delta2_mfcc_std_2', 'delta2_mfcc_std_3', 'delta2_mfcc_std_4', 'delta2_mfcc_std_5', 'delta2_mfcc_std_6', 'delta2_mfcc_std_7', 'delta2_mfcc_std_8', 'delta2_mfcc_std_9', 'delta2_mfcc_std_10', 'delta2_mfcc_std_11', 'delta2_mfcc_std_12', 'spectral_contrast_mean_0', 'spectral_contrast_mean_1', 'spectral_contrast_mean_2', 'spectral_contrast_mean_3', 'spectral_contrast_mean_4', 'spectral_contrast_mean_5', 'spectral_contrast_mean_6', 'is_outlier_iso', 'quality_score']\n",
      "Columns after merge: ['phoneme_id', 'utterance_id', 'phoneme', 'class_x', 'start_ms', 'end_ms', 'duration_ms_x', 'audio_path', 'energy_rms', 'energy_rms_std', 'energy_zcr', 'energy_zcr_std', 'spectral_centroid', 'spectral_centroid_std', 'spectral_rolloff', 'spectral_rolloff_std', 'spectral_bandwidth', 'spectral_bandwidth_std', 'formant_f1', 'formant_f2', 'formant_f3', 'formant_f4', 'formant_f1_std', 'formant_f2_std', 'formant_f3_std', 'formant_f4_std', 'spectral_flatness', 'harmonic_noise_ratio', 'zcr_mean', 'energy_cv', 'class_y', 'duration_ms_y', 'mfcc_mean_0', 'mfcc_mean_1', 'mfcc_mean_2', 'mfcc_mean_3', 'mfcc_mean_4', 'mfcc_mean_5', 'mfcc_mean_6', 'mfcc_mean_7', 'mfcc_mean_8', 'mfcc_mean_9', 'mfcc_mean_10', 'mfcc_mean_11', 'mfcc_mean_12', 'mfcc_std_0', 'mfcc_std_1', 'mfcc_std_2', 'mfcc_std_3', 'mfcc_std_4', 'mfcc_std_5', 'mfcc_std_6', 'mfcc_std_7', 'mfcc_std_8', 'mfcc_std_9', 'mfcc_std_10', 'mfcc_std_11', 'mfcc_std_12', 'delta_mfcc_mean_0', 'delta_mfcc_mean_1', 'delta_mfcc_mean_2', 'delta_mfcc_mean_3', 'delta_mfcc_mean_4', 'delta_mfcc_mean_5', 'delta_mfcc_mean_6', 'delta_mfcc_mean_7', 'delta_mfcc_mean_8', 'delta_mfcc_mean_9', 'delta_mfcc_mean_10', 'delta_mfcc_mean_11', 'delta_mfcc_mean_12', 'delta_mfcc_std_0', 'delta_mfcc_std_1', 'delta_mfcc_std_2', 'delta_mfcc_std_3', 'delta_mfcc_std_4', 'delta_mfcc_std_5', 'delta_mfcc_std_6', 'delta_mfcc_std_7', 'delta_mfcc_std_8', 'delta_mfcc_std_9', 'delta_mfcc_std_10', 'delta_mfcc_std_11', 'delta_mfcc_std_12', 'delta2_mfcc_mean_0', 'delta2_mfcc_mean_1', 'delta2_mfcc_mean_2', 'delta2_mfcc_mean_3', 'delta2_mfcc_mean_4', 'delta2_mfcc_mean_5', 'delta2_mfcc_mean_6', 'delta2_mfcc_mean_7', 'delta2_mfcc_mean_8', 'delta2_mfcc_mean_9', 'delta2_mfcc_mean_10', 'delta2_mfcc_mean_11', 'delta2_mfcc_mean_12', 'delta2_mfcc_std_0', 'delta2_mfcc_std_1', 'delta2_mfcc_std_2', 'delta2_mfcc_std_3', 'delta2_mfcc_std_4', 'delta2_mfcc_std_5', 'delta2_mfcc_std_6', 'delta2_mfcc_std_7', 'delta2_mfcc_std_8', 'delta2_mfcc_std_9', 'delta2_mfcc_std_10', 'delta2_mfcc_std_11', 'delta2_mfcc_std_12', 'spectral_contrast_mean_0', 'spectral_contrast_mean_1', 'spectral_contrast_mean_2', 'spectral_contrast_mean_3', 'spectral_contrast_mean_4', 'spectral_contrast_mean_5', 'spectral_contrast_mean_6', 'is_outlier_iso', 'quality_score']\n",
      "Warning: Using 'phoneme' column as target instead of 'class'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "PROJECT_ROOT = Path('/Volumes/SSanDisk/SpeechRec-German/without_context_windows')\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from models.hybrid.hybrid_cnn_mlp_v2 import HybridCNNMLP_V2\n",
    "from models.sequence.transformer_sequence_v2 import TransformerSequence_V2\n",
    "from models.specialized.formant_focused_v2 import FormantFocusedModel_V2\n",
    "from models.sequence.bilstm_attention_v2 import BiLSTMAttention_V2\n",
    "from utils.ensemble_utils import (\n",
    "    get_model_predictions, weighted_voting, simple_voting, \n",
    "    stacking_ensemble, evaluate_ensemble\n",
    ")\n",
    "from utils.data_loader import load_data, create_dataloaders\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'artifacts' / 'b-p_dl_models' / 'improved_models'\n",
    "ENSEMBLE_DIR = OUTPUT_DIR / 'ensembles'\n",
    "ENSEMBLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load data\n",
    "df, spectrograms_dict, feature_cols, feature_scaler, class_weights_dict = load_data(PROJECT_ROOT)\n",
    "dataloaders = create_dataloaders(df, spectrograms_dict, feature_cols, feature_scaler, class_weights_dict, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Improved Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 improved models\n"
     ]
    }
   ],
   "source": [
    "# Load all improved models\n",
    "models = []\n",
    "\n",
    "# Model 1: Hybrid CNN+MLP V2\n",
    "model1 = HybridCNNMLP_V2(n_features=len(feature_cols), num_classes=2).to(device)\n",
    "checkpoint1 = torch.load(OUTPUT_DIR / 'hybrid_cnn_mlp_v2' / 'best_model.pt')\n",
    "model1.load_state_dict(checkpoint1['model_state_dict'])\n",
    "models.append((model1, 'Hybrid CNN+MLP V2'))\n",
    "\n",
    "# Model 2: Transformer Sequence V2\n",
    "model2 = TransformerSequence_V2(\n",
    "    input_dim=128, d_model=256, nhead=8, num_layers=6, \n",
    "    dim_feedforward=1024, dropout=0.1, num_classes=2, batch_first=True\n",
    ").to(device)\n",
    "checkpoint2 = torch.load(OUTPUT_DIR / 'transformer_sequence_v2' / 'best_model.pt')\n",
    "model2.load_state_dict(checkpoint2['model_state_dict'])\n",
    "models.append((model2, 'Transformer Sequence V2'))\n",
    "\n",
    "# Model 3: Formant-focused V2\n",
    "model3 = FormantFocusedModel_V2(n_features=len(feature_cols), num_classes=2).to(device)\n",
    "model3.set_formant_indices(feature_cols, ['formant_f1', 'formant_f2', 'formant_f3'])\n",
    "checkpoint3 = torch.load(OUTPUT_DIR / 'formant_focused_v2' / 'best_model.pt')\n",
    "model3.load_state_dict(checkpoint3['model_state_dict'])\n",
    "models.append((model3, 'Formant-focused V2'))\n",
    "\n",
    "# Model 4: BiLSTM+Attention V2\n",
    "model4 = BiLSTMAttention_V2(\n",
    "    input_dim=128, hidden_dim=128, num_layers=3, \n",
    "    num_classes=2, dropout=0.3, num_heads=4\n",
    ").to(device)\n",
    "checkpoint4 = torch.load(OUTPUT_DIR / 'bilstm_attention_v2' / 'best_model.pt')\n",
    "model4.load_state_dict(checkpoint4['model_state_dict'])\n",
    "models.append((model4, 'BiLSTM+Attention V2'))\n",
    "\n",
    "print(f\"Loaded {len(models)} improved models\")\n",
    "\n",
    "# Prepare dataloaders for each model type\n",
    "model_loaders = {\n",
    "    'Hybrid CNN+MLP V2': dataloaders['hybrid'],\n",
    "    'Transformer Sequence V2': dataloaders['sequence'],\n",
    "    'Formant-focused V2': dataloaders['feature'],\n",
    "    'BiLSTM+Attention V2': dataloaders['sequence']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions from All Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid CNN+MLP V2 - Val predictions shape: (5335, 2), Test predictions shape: (5349, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Sequence V2 - Val predictions shape: (5335, 2), Test predictions shape: (5349, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formant-focused V2 - Val predictions shape: (5335, 2), Test predictions shape: (5349, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM+Attention V2 - Val predictions shape: (5335, 2), Test predictions shape: (5349, 2)\n",
      "\n",
      "Validation labels shape: (5335,)\n",
      "Test labels shape: (5349,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Get predictions on validation and test sets\n",
    "val_probs_list = []\n",
    "test_probs_list = []\n",
    "val_labels = None\n",
    "test_labels = None\n",
    "\n",
    "for model, name in models:\n",
    "    loader = model_loaders[name]\n",
    "    \n",
    "    # Validation predictions\n",
    "    val_probs, val_labels = get_model_predictions([(model, name)], loader['val'], device)\n",
    "    val_probs_list.append(val_probs[0])\n",
    "    \n",
    "    # Test predictions\n",
    "    test_probs, test_labels = get_model_predictions([(model, name)], loader['test'], device)\n",
    "    test_probs_list.append(test_probs[0])\n",
    "    \n",
    "    print(f\"{name} - Val predictions shape: {val_probs[0].shape}, Test predictions shape: {test_probs[0].shape}\")\n",
    "\n",
    "print(f\"\\nValidation labels shape: {val_labels.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Voting Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Voting - Acc: 0.9538, F1: 0.9537, ROC-AUC: 0.9881, Precision: 0.9536, Recall: 0.9538\n"
     ]
    }
   ],
   "source": [
    "# Simple voting (equal weights)\n",
    "simple_test_probs, simple_test_preds = simple_voting(test_probs_list)\n",
    "simple_metrics = evaluate_ensemble(simple_test_preds, simple_test_probs, test_labels, \"Simple Voting\")\n",
    "\n",
    "# Save results\n",
    "with open(ENSEMBLE_DIR / 'simple_voting_metrics.json', 'w') as f:\n",
    "    json.dump(simple_metrics, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Voting Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model F1 scores (used as weights):\n",
      "  Hybrid CNN+MLP V2: 0.9481\n",
      "  Transformer Sequence V2: 0.9472\n",
      "  Formant-focused V2: 0.9411\n",
      "  BiLSTM+Attention V2: 0.9331\n",
      "Weighted Voting - Acc: 0.9538, F1: 0.9537, ROC-AUC: 0.9881, Precision: 0.9536, Recall: 0.9538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load individual model metrics to determine weights\n",
    "model_metrics = {}\n",
    "for model, name in models:\n",
    "    loader = model_loaders[name]\n",
    "    test_probs, _ = get_model_predictions([(model, name)], loader['test'], device)\n",
    "    test_preds = np.argmax(test_probs[0], axis=1)\n",
    "    \n",
    "    # Calculate F1 score as weight\n",
    "    from sklearn.metrics import f1_score\n",
    "    f1 = f1_score(test_labels, test_preds, average='weighted')\n",
    "    model_metrics[name] = f1\n",
    "\n",
    "print(\"Model F1 scores (used as weights):\")\n",
    "for name, f1 in model_metrics.items():\n",
    "    print(f\"  {name}: {f1:.4f}\")\n",
    "\n",
    "# Use F1 scores as weights (normalized)\n",
    "# Fixed: correct unpacking - use model, name instead of name, _\n",
    "weights = [model_metrics[name] for _, name in models]\n",
    "weighted_test_probs, weighted_test_preds = weighted_voting(test_probs_list, weights=weights)\n",
    "weighted_metrics = evaluate_ensemble(weighted_test_preds, weighted_test_probs, test_labels, \"Weighted Voting\")\n",
    "\n",
    "# Save results\n",
    "with open(ENSEMBLE_DIR / 'weighted_voting_metrics.json', 'w') as f:\n",
    "    json.dump(weighted_metrics, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-model validation - Acc: 0.9588, F1: 0.9587\n",
      "Stacking (LogisticRegression) - Acc: 0.9549, F1: 0.9547, ROC-AUC: 0.9878, Precision: 0.9547, Recall: 0.9549\n",
      "Meta-model validation - Acc: 0.9588, F1: 0.9587\n",
      "Stacking (MLP) - Acc: 0.9533, F1: 0.9531, ROC-AUC: 0.9877, Precision: 0.9531, Recall: 0.9533\n"
     ]
    }
   ],
   "source": [
    "# Stacking with LogisticRegression meta-model\n",
    "stacking_test_preds, stacking_test_probs, meta_model = stacking_ensemble(\n",
    "    val_probs_list, val_labels,\n",
    "    val_probs_list, val_labels,  # Use val for meta-training (in practice, use separate fold)\n",
    "    test_probs_list, test_labels,\n",
    "    meta_model_type='logistic'\n",
    ")\n",
    "\n",
    "stacking_metrics = evaluate_ensemble(stacking_test_preds, stacking_test_probs, test_labels, \"Stacking (LogisticRegression)\")\n",
    "\n",
    "# Save results\n",
    "with open(ENSEMBLE_DIR / 'stacking_logistic_metrics.json', 'w') as f:\n",
    "    json.dump(stacking_metrics, f, indent=2)\n",
    "\n",
    "# Try MLP meta-model\n",
    "stacking_mlp_test_preds, stacking_mlp_test_probs, meta_model_mlp = stacking_ensemble(\n",
    "    val_probs_list, val_labels,\n",
    "    val_probs_list, val_labels,\n",
    "    test_probs_list, test_labels,\n",
    "    meta_model_type='mlp'\n",
    ")\n",
    "\n",
    "stacking_mlp_metrics = evaluate_ensemble(stacking_mlp_test_preds, stacking_mlp_test_probs, test_labels, \"Stacking (MLP)\")\n",
    "\n",
    "# Save results\n",
    "with open(ENSEMBLE_DIR / 'stacking_mlp_metrics.json', 'w') as f:\n",
    "    json.dump(stacking_mlp_metrics, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare All Ensemble Methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Ensemble Methods Comparison:\n",
      "============================================================\n",
      "             Method  accuracy       f1  roc_auc\n",
      "Stacking (Logistic)  0.954945 0.954735 0.987758\n",
      "      Simple Voting  0.953823 0.953694 0.988097\n",
      "    Weighted Voting  0.953823 0.953694 0.988106\n",
      "     Stacking (MLP)  0.953262 0.953127 0.987742\n",
      "\n",
      "Best ensemble method: Stacking (Logistic) (Accuracy: 0.9549)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect all results\n",
    "results = {\n",
    "    'Method': ['Simple Voting', 'Weighted Voting', 'Stacking (Logistic)', 'Stacking (MLP)'],\n",
    "    'accuracy': [\n",
    "        simple_metrics['accuracy'],\n",
    "        weighted_metrics['accuracy'],\n",
    "        stacking_metrics['accuracy'],\n",
    "        stacking_mlp_metrics['accuracy']\n",
    "    ],\n",
    "    'f1': [\n",
    "        simple_metrics['f1'],\n",
    "        weighted_metrics['f1'],\n",
    "        stacking_metrics['f1'],\n",
    "        stacking_mlp_metrics['f1']\n",
    "    ],\n",
    "    'roc_auc': [\n",
    "        simple_metrics['roc_auc'],\n",
    "        weighted_metrics['roc_auc'],\n",
    "        stacking_metrics['roc_auc'],\n",
    "        stacking_mlp_metrics['roc_auc']\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ensemble Methods Comparison:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Save comparison\n",
    "results_df.to_csv(ENSEMBLE_DIR / 'ensemble_comparison.csv', index=False)\n",
    "\n",
    "# Find best ensemble\n",
    "best_method = results_df.iloc[0]['Method']\n",
    "best_acc = results_df.iloc[0]['accuracy']\n",
    "print(f\"\\nBest ensemble method: {best_method} (Accuracy: {best_acc:.4f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}