{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B-P Phoneme DL Data Preparation\n",
    "\n",
    "Preparation of PyTorch datasets for deep learning models:\n",
    "- Load features.parquet and spectrograms.h5\n",
    "- Create PyTorch Dataset classes for different input types\n",
    "- Train/Val/Test split with stratification\n",
    "- Data normalization\n",
    "- DataLoader creation with batch sampling\n",
    "- Handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "Project root: /Volumes/SSanDisk/SpeechRec-German\n",
      "Data directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dataset\n",
      "Output directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import h5py\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "# Determine project root (parent of notebooks directory)\nPROJECT_ROOT = Path.cwd().parent if Path.cwd().name in ['notebooks', 'b-p_first_experiments'] else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / 'artifacts' / 'b-p_dataset'\n",
    "FEATURES_FILE = DATA_DIR / 'features' / 'features.parquet'\n",
    "SPECTROGRAMS_FILE = DATA_DIR / 'features' / 'spectrograms.h5'\n",
    "PHONEMES_FILE = DATA_DIR / 'filtered_phonemes.csv'\n",
    "PHONEME_WAV_DIR = PROJECT_ROOT / 'artifacts' / 'phoneme_wav'\n",
    "\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'artifacts' / 'b-p_dl_models'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU device\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading features...\n",
      "Features shape: (35660, 112)\n",
      "Features columns: 112\n",
      "\n",
      "Loading phoneme metadata...\n",
      "Phonemes shape: (35660, 8)\n",
      "Phonemes columns: ['phoneme_id', 'utterance_id', 'phoneme', 'class', 'start_ms', 'end_ms', 'duration_ms', 'audio_path']\n",
      "\n",
      "Merged dataset shape: (35660, 119)\n",
      "\n",
      "'class' column found in merged DataFrame\n",
      "\n",
      "Class distribution:\n",
      "class\n",
      "b    24917\n",
      "p    10743\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution (%):\n",
      "class\n",
      "b    69.873808\n",
      "p    30.126192\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class encoding: {'b': np.int64(0), 'p': np.int64(1)}\n",
      "\n",
      "Number of feature columns: 109\n",
      "First 10 features: ['energy_rms', 'energy_rms_std', 'energy_zcr', 'energy_zcr_std', 'spectral_centroid', 'spectral_centroid_std', 'spectral_rolloff', 'spectral_rolloff_std', 'spectral_bandwidth', 'spectral_bandwidth_std']\n"
     ]
    }
   ],
   "source": [
    "# Load features\n",
    "print(\"Loading features...\")\n",
    "df_features = pd.read_parquet(FEATURES_FILE)\n",
    "print(f\"Features shape: {df_features.shape}\")\n",
    "print(f\"Features columns: {len(df_features.columns)}\")\n",
    "\n",
    "# Load phoneme metadata\n",
    "print(\"\\nLoading phoneme metadata...\")\n",
    "df_phonemes = pd.read_csv(PHONEMES_FILE)\n",
    "print(f\"Phonemes shape: {df_phonemes.shape}\")\n",
    "print(f\"Phonemes columns: {list(df_phonemes.columns)}\")\n",
    "\n",
    "# Merge on phoneme_id\n",
    "df = df_phonemes.merge(df_features, on='phoneme_id', how='inner', suffixes=('', '_features'))\n",
    "print(f\"\\nMerged dataset shape: {df.shape}\")\n",
    "\n",
    "# Handle duplicate columns from merge (if 'class' exists in both, keep the one from df_phonemes)\n",
    "if 'class_features' in df.columns:\n",
    "    df = df.drop(columns=['class_features'])\n",
    "if 'class' not in df.columns:\n",
    "    if 'phoneme' in df.columns:\n",
    "        print(\"\\n'class' column not found, creating from 'phoneme' column...\")\n",
    "        df['class'] = df['phoneme']\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'class' nor 'phoneme' column found in merged DataFrame\")\n",
    "else:\n",
    "    print(\"\\n'class' column found in merged DataFrame\")\n",
    "\n",
    "# Filter to only b and p classes (exclude pf if present)\n",
    "if 'pf' in df['class'].values:\n",
    "    print(\"\\nFiltering out 'pf' class, keeping only 'b' and 'p'...\")\n",
    "    df = df[df['class'].isin(['b', 'p'])].copy()\n",
    "    print(f\"Dataset after filtering: {len(df)} samples\")\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "print(f\"\\nClass distribution (%):\")\n",
    "print(df['class'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "df['class_encoded'] = le.fit_transform(df['class'])  # b=0, p=1\n",
    "print(f\"\\nClass encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Get feature columns (exclude metadata and non-numeric columns)\n",
    "exclude_cols = ['phoneme_id', 'utterance_id', 'phoneme', 'class', 'class_encoded', \n",
    "                'start_ms', 'end_ms', 'duration_ms', 'audio_path', 'is_outlier_iso',\n",
    "                'class_x', 'class_y', 'class_features']  # Exclude merge suffixes\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Filter to only numeric columns\n",
    "numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
    "feature_cols = [col for col in feature_cols if col in numeric_cols]\n",
    "\n",
    "print(f\"\\nNumber of feature columns: {len(feature_cols)}\")\n",
    "print(f\"First 10 features: {feature_cols[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Spectrograms from H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spectrograms from H5...\n",
      "Found 35660 spectrograms in H5 file\n",
      "Spectrogram shape: (128, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectrograms: 100%|██████████| 35660/35660 [00:02<00:00, 16297.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 35660 spectrograms\n",
      "Phonemes with spectrograms: 35660 / 35660\n"
     ]
    }
   ],
   "source": [
    "# Load spectrograms from H5 file\n",
    "print(\"Loading spectrograms from H5...\")\n",
    "spectrograms_dict = {}\n",
    "with h5py.File(SPECTROGRAMS_FILE, 'r') as f:\n",
    "    phoneme_ids = list(f.keys())\n",
    "    print(f\"Found {len(phoneme_ids)} spectrograms in H5 file\")\n",
    "    \n",
    "    # Load first spectrogram to check shape\n",
    "    if phoneme_ids:\n",
    "        first_key = phoneme_ids[0]\n",
    "        first_spec = f[first_key][:]\n",
    "        print(f\"Spectrogram shape: {first_spec.shape}\")\n",
    "        \n",
    "        # Load all spectrograms\n",
    "        for phoneme_id in tqdm(phoneme_ids, desc=\"Loading spectrograms\"):\n",
    "            spectrograms_dict[phoneme_id] = f[phoneme_id][:]\n",
    "\n",
    "print(f\"\\nLoaded {len(spectrograms_dict)} spectrograms\")\n",
    "\n",
    "# Check which phonemes have spectrograms\n",
    "df['has_spectrogram'] = df['phoneme_id'].isin(spectrograms_dict.keys())\n",
    "print(f\"Phonemes with spectrograms: {df['has_spectrogram'].sum()} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Val/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after filtering: 35660 samples\n",
      "\n",
      "Train set: 24,976 samples (70.0%)\n",
      "  Class distribution: [17451  7525]\n",
      "Val set: 5,335 samples (15.0%)\n",
      "  Class distribution: [3728 1607]\n",
      "Test set: 5,349 samples (15.0%)\n",
      "  Class distribution: [3738 1611]\n",
      "\n",
      "Split indices saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models/split_indices.json\n"
     ]
    }
   ],
   "source": [
    "# Filter to only phonemes with spectrograms\n",
    "df = df[df['has_spectrogram']].copy()\n",
    "print(f\"Dataset after filtering: {len(df)} samples\")\n",
    "\n",
    "# Train/Val/Test split (70/15/15) with stratification\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df.index, df['class_encoded'], \n",
    "    test_size=0.15, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=df['class_encoded']\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.176,  # 0.176 ≈ 15/85\n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Create split column\n",
    "df['split'] = 'train'\n",
    "df.loc[X_val, 'split'] = 'val'\n",
    "df.loc[X_test, 'split'] = 'test'\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_train, 'class_encoded'])}\")\n",
    "print(f\"Val set: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_val, 'class_encoded'])}\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_test, 'class_encoded'])}\")\n",
    "\n",
    "# Save split indices\n",
    "split_indices = {\n",
    "    'train': [int(idx) for idx in X_train],\n",
    "    'val': [int(idx) for idx in X_val],\n",
    "    'test': [int(idx) for idx in X_test]\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'split_indices.json', 'w') as f:\n",
    "    json.dump(split_indices, f)\n",
    "print(f\"\\nSplit indices saved to {OUTPUT_DIR / 'split_indices.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create PyTorch Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset classes defined!\n"
     ]
    }
   ],
   "source": [
    "class SpectrogramDataset(Dataset):\n",
    "    \"\"\"Dataset for models using spectrograms only\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, split='train', transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        # Get spectrogram\n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        \n",
    "        # Add channel dimension if needed (for CNN: [1, 128, 7])\n",
    "        if len(spectrogram.shape) == 2:\n",
    "            spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return torch.from_numpy(spectrogram), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    \"\"\"Dataset for models using extracted features only\"\"\"\n",
    "    def __init__(self, df, feature_cols, scaler=None, split='train', fit_scaler=False):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.feature_cols = feature_cols\n",
    "        \n",
    "        # Extract features\n",
    "        X = self.df[feature_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Handle missing values\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        # Scale features\n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X = self.scaler.fit_transform(X)\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            X = self.scaler.transform(X)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(self.df['class_encoded'].values).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    \"\"\"Dataset for hybrid models using both spectrograms and features\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, feature_cols, scaler=None, split='train', fit_scaler=False, transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.feature_cols = feature_cols\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Extract and scale features\n",
    "        X_features = self.df[feature_cols].values.astype(np.float32)\n",
    "        X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_features = self.scaler.fit_transform(X_features)\n",
    "        elif scaler is not None:\n",
    "            self.scaler = scaler\n",
    "            X_features = self.scaler.transform(X_features)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.X_features = torch.from_numpy(X_features)\n",
    "        self.y = torch.from_numpy(self.df['class_encoded'].values).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        # Get spectrogram\n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        if len(spectrogram.shape) == 2:\n",
    "            spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        features = self.X_features[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        return (torch.from_numpy(spectrogram), features), label\n",
    "\n",
    "\n",
    "class RawAudioDataset(Dataset):\n",
    "    \"\"\"Dataset for models using raw audio waveforms\"\"\"\n",
    "    def __init__(self, df, split='train', sample_rate=16000, max_length=None, transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.max_length = max_length\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        audio_path = row['audio_path']\n",
    "        \n",
    "        # Load audio\n",
    "        try:\n",
    "            audio, sr = librosa.load(audio_path, sr=self.sample_rate, mono=True)\n",
    "        except:\n",
    "            # If loading fails, return zeros\n",
    "            audio = np.zeros(self.sample_rate // 10)  # 100ms of silence\n",
    "        \n",
    "        # Normalize audio\n",
    "        if len(audio) > 0:\n",
    "            audio = audio / (np.abs(audio).max() + 1e-8)\n",
    "        \n",
    "        # Pad or truncate to max_length\n",
    "        if self.max_length is not None:\n",
    "            if len(audio) < self.max_length:\n",
    "                audio = np.pad(audio, (0, self.max_length - len(audio)), mode='constant')\n",
    "            else:\n",
    "                audio = audio[:self.max_length]\n",
    "        \n",
    "        if self.transform:\n",
    "            audio = self.transform(audio)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return torch.from_numpy(audio.astype(np.float32)), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class ContextAudioDataset(Dataset):\n",
    "    \"\"\"Dataset for models using raw audio with context from original utterance\"\"\"\n",
    "    def __init__(self, df, split='train', sample_rate=16000, context_ms=500, transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.sample_rate = sample_rate\n",
    "        self.context_samples = int(context_ms * sample_rate / 1000)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_audio_path = row['audio_path']\n",
    "        \n",
    "        # Load phoneme audio\n",
    "        try:\n",
    "            phoneme_audio, sr = librosa.load(phoneme_audio_path, sr=self.sample_rate, mono=True)\n",
    "        except:\n",
    "            phoneme_audio = np.zeros(self.sample_rate // 10)\n",
    "        \n",
    "        # For now, use just the phoneme audio as context\n",
    "        # In a full implementation, you would load the full utterance and extract context\n",
    "        context_audio = phoneme_audio  # Placeholder\n",
    "        \n",
    "        # Normalize\n",
    "        if len(phoneme_audio) > 0:\n",
    "            phoneme_audio = phoneme_audio / (np.abs(phoneme_audio).max() + 1e-8)\n",
    "        if len(context_audio) > 0:\n",
    "            context_audio = context_audio / (np.abs(context_audio).max() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            phoneme_audio = self.transform(phoneme_audio)\n",
    "            context_audio = self.transform(context_audio)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return (\n",
    "            torch.from_numpy(phoneme_audio.astype(np.float32)),\n",
    "            torch.from_numpy(context_audio.astype(np.float32))\n",
    "        ), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence models (LSTM, Transformer) using spectrograms as sequences\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, split='train', transform=None):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        # Get spectrogram: shape (128, 7) -> (7, 128) for sequence models\n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        spectrogram = spectrogram.T  # Transpose: (7, 128) - 7 time steps, 128 features\n",
    "        \n",
    "        # Normalize\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        label = row['class_encoded']\n",
    "        \n",
    "        return torch.from_numpy(spectrogram), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "print(\"Dataset classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Datasets and Compute Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaler saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models/feature_scaler.joblib\n",
      "\n",
      "Class weights: {0: np.float64(0.7156036903329323), 1: np.float64(1.6595348837209303)}\n",
      "Class weights saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models/class_weights.json\n",
      "\n",
      "All datasets created!\n",
      "Train spectrogram dataset: 24976 samples\n",
      "Train feature dataset: 24976 samples\n",
      "Train hybrid dataset: 24976 samples\n",
      "Train sequence dataset: 24976 samples\n",
      "Train raw audio dataset: 24976 samples\n"
     ]
    }
   ],
   "source": [
    "# Create feature scaler on training data\n",
    "train_df = df[df['split'] == 'train']\n",
    "X_train_features = train_df[feature_cols].values.astype(np.float32)\n",
    "X_train_features = np.nan_to_num(X_train_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "feature_scaler.fit(X_train_features)\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(feature_scaler, OUTPUT_DIR / 'feature_scaler.joblib')\n",
    "print(f\"Feature scaler saved to {OUTPUT_DIR / 'feature_scaler.joblib'}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(df[df['split'] == 'train']['class_encoded']),\n",
    "    y=df[df['split'] == 'train']['class_encoded']\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(f\"\\nClass weights: {class_weights_dict}\")\n",
    "\n",
    "# Save class weights\n",
    "with open(OUTPUT_DIR / 'class_weights.json', 'w') as f:\n",
    "    json.dump(class_weights_dict, f)\n",
    "print(f\"Class weights saved to {OUTPUT_DIR / 'class_weights.json'}\")\n",
    "\n",
    "# Create datasets\n",
    "train_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='train')\n",
    "val_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='val')\n",
    "test_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='test')\n",
    "\n",
    "train_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='train')\n",
    "val_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='val')\n",
    "test_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='test')\n",
    "\n",
    "train_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='train')\n",
    "val_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='val')\n",
    "test_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='test')\n",
    "\n",
    "train_sequence_ds = SequenceDataset(df, spectrograms_dict, split='train')\n",
    "val_sequence_ds = SequenceDataset(df, spectrograms_dict, split='val')\n",
    "test_sequence_ds = SequenceDataset(df, spectrograms_dict, split='test')\n",
    "\n",
    "train_raw_audio_ds = RawAudioDataset(df, split='train', sample_rate=16000, max_length=3200)  # 200ms at 16kHz\n",
    "val_raw_audio_ds = RawAudioDataset(df, split='val', sample_rate=16000, max_length=3200)\n",
    "test_raw_audio_ds = RawAudioDataset(df, split='test', sample_rate=16000, max_length=3200)\n",
    "\n",
    "train_context_audio_ds = ContextAudioDataset(df, split='train', sample_rate=16000)\n",
    "val_context_audio_ds = ContextAudioDataset(df, split='val', sample_rate=16000)\n",
    "test_context_audio_ds = ContextAudioDataset(df, split='test', sample_rate=16000)\n",
    "\n",
    "print(\"\\nAll datasets created!\")\n",
    "print(f\"Train spectrogram dataset: {len(train_spectrogram_ds)} samples\")\n",
    "print(f\"Train feature dataset: {len(train_feature_ds)} samples\")\n",
    "print(f\"Train hybrid dataset: {len(train_hybrid_ds)} samples\")\n",
    "print(f\"Train sequence dataset: {len(train_sequence_ds)} samples\")\n",
    "print(f\"Train raw audio dataset: {len(train_raw_audio_ds)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DataLoaders with Weighted Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataLoaders created!\n",
      "\n",
      "Train batches (spectrogram): 391\n",
      "Train batches (feature): 391\n",
      "Train batches (hybrid): 391\n",
      "\n",
      "Testing a batch from spectrogram dataset...\n",
      "Batch shape: torch.Size([64, 1, 128, 7]), Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Compute sample weights for weighted sampling\n",
    "train_labels = df[df['split'] == 'train']['class_encoded'].values\n",
    "sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_spectrogram_loader = DataLoader(train_spectrogram_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_spectrogram_loader = DataLoader(val_spectrogram_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_spectrogram_loader = DataLoader(test_spectrogram_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_feature_loader = DataLoader(train_feature_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_feature_loader = DataLoader(val_feature_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_feature_loader = DataLoader(test_feature_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_hybrid_loader = DataLoader(train_hybrid_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_hybrid_loader = DataLoader(val_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_hybrid_loader = DataLoader(test_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_sequence_loader = DataLoader(train_sequence_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_sequence_loader = DataLoader(val_sequence_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_sequence_loader = DataLoader(test_sequence_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_raw_audio_loader = DataLoader(train_raw_audio_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_raw_audio_loader = DataLoader(val_raw_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_raw_audio_loader = DataLoader(test_raw_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_context_audio_loader = DataLoader(train_context_audio_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_context_audio_loader = DataLoader(val_context_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_context_audio_loader = DataLoader(test_context_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"All DataLoaders created!\")\n",
    "print(f\"\\nTrain batches (spectrogram): {len(train_spectrogram_loader)}\")\n",
    "print(f\"Train batches (feature): {len(train_feature_loader)}\")\n",
    "print(f\"Train batches (hybrid): {len(train_hybrid_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "print(\"\\nTesting a batch from spectrogram dataset...\")\n",
    "sample_batch = next(iter(train_spectrogram_loader))\n",
    "print(f\"Batch shape: {sample_batch[0].shape}, Labels shape: {sample_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Dataset Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/b-p_dl_models/dataset_info.json\n",
      "\n",
      "Dataset summary:\n",
      "  Total samples: 35660\n",
      "  Train: 24976\n",
      "  Val: 5335\n",
      "  Test: 5349\n",
      "  Features: 109\n",
      "  Spectrogram shape: [128, 7]\n"
     ]
    }
   ],
   "source": [
    "# Save dataset information\n",
    "dataset_info = {\n",
    "    'total_samples': len(df),\n",
    "    'train_samples': len(df[df['split'] == 'train']),\n",
    "    'val_samples': len(df[df['split'] == 'val']),\n",
    "    'test_samples': len(df[df['split'] == 'test']),\n",
    "    'n_features': len(feature_cols),\n",
    "    'spectrogram_shape': list(spectrograms_dict[list(spectrograms_dict.keys())[0]].shape),\n",
    "    'class_distribution': {\n",
    "        'train': df[df['split'] == 'train']['class'].value_counts().to_dict(),\n",
    "        'val': df[df['split'] == 'val']['class'].value_counts().to_dict(),\n",
    "        'test': df[df['split'] == 'test']['class'].value_counts().to_dict()\n",
    "    },\n",
    "    'class_weights': class_weights_dict,\n",
    "    'feature_columns': feature_cols\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'dataset_info.json', 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "\n",
    "print(f\"Dataset info saved to {OUTPUT_DIR / 'dataset_info.json'}\")\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Total samples: {dataset_info['total_samples']}\")\n",
    "print(f\"  Train: {dataset_info['train_samples']}\")\n",
    "print(f\"  Val: {dataset_info['val_samples']}\")\n",
    "print(f\"  Test: {dataset_info['test_samples']}\")\n",
    "print(f\"  Features: {dataset_info['n_features']}\")\n",
    "print(f\"  Spectrogram shape: {dataset_info['spectrogram_shape']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}