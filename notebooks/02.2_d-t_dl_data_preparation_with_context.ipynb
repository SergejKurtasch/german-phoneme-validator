{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D-T Phoneme DL Data Preparation (with Context) - Version 2\n",
    "\n",
    "**Version 2** - Extended feature extraction with VOT, burst features, low-frequency energy, and enhanced quality filtering.\n",
    "\n",
    "Preparation of PyTorch datasets for deep learning models using phoneme audio with extended context windows:\n",
    "- **Extract features from context audio files** (±100ms context from `phoneme_wav_with_context`)\n",
    "- Extract spectrograms from context audio files\n",
    "- Create PyTorch Dataset classes for different input types\n",
    "- Train/Val/Test split with stratification\n",
    "- Data normalization\n",
    "- DataLoader creation with batch sampling\n",
    "- Handle class imbalance\n",
    "\n",
    "**New features in Version 2:**\n",
    "- Voice Onset Time (VOT) extraction\n",
    "- Burst-specific features (ZCR around burst, burst spectral centroid)\n",
    "- Low-frequency energy (voicing) features\n",
    "- Plosive duration validation (closure/burst length checks)\n",
    "- Enhanced quality filtering (conservative thresholds)\n",
    "- VAD (Voice Activity Detection) support\n",
    "\n",
    "**Key difference from 02.1:** \n",
    "- Features are extracted **anew** from context audio files (~300ms duration) instead of using old features\n",
    "- Uses extended context windows (±100ms) for better capture of coarticulation, formant transitions, and VOT\n",
    "- All features (MFCC, energy, spectral, formants, quality metrics, VOT, burst features) reflect the extended context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: parselmouth not installed. Will use LPC for formant extraction.\n",
      "Warning: webrtcvad not installed. VAD features will be limited.\n",
      "Using MPS device\n",
      "Project root: /Volumes/SSanDisk/SpeechRec-German\n",
      "Phoneme audio directory (with context): /Volumes/SSanDisk/SpeechRec-German/artifacts/phoneme_wav_with_context\n",
      "Output directory (Version 2): /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2\n",
      "Features output directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/features\n",
      "Extract features: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import h5py\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from scipy import signal\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try to import optional libraries\n",
    "try:\n",
    "    import parselmouth\n",
    "    HAS_PARSELMOUTH = True\n",
    "except ImportError:\n",
    "    HAS_PARSELMOUTH = False\n",
    "    print(\"Warning: parselmouth not installed. Will use LPC for formant extraction.\")\n",
    "\n",
    "try:\n",
    "    import webrtcvad\n",
    "    HAS_WEBRTCVAD = True\n",
    "except ImportError:\n",
    "    HAS_WEBRTCVAD = False\n",
    "    print(\"Warning: webrtcvad not installed. VAD features will be limited.\")\n",
    "\n",
    "# Configuration\n",
    "# Determine project root (parent of notebooks directory)\nPROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "# Audio files with context are in the main artifacts directory\n",
    "PHONEME_WAV_DIR = PROJECT_ROOT / 'artifacts' / 'phoneme_wav_with_context'  # With context!\n",
    "PHONEMES_FILE = PROJECT_ROOT / 'artifacts' / 'phoneme_intervals.csv'  # Load from fresh phoneme intervals file\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = PROJECT_ROOT / 'artifacts' / 'd-t_dl_models_with_context_v2'\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURES_OUTPUT_DIR = OUTPUT_DIR / 'features'\n",
    "FEATURES_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FEATURES_FILE = FEATURES_OUTPUT_DIR / 'features.parquet'\n",
    "SPECTROGRAMS_FILE = FEATURES_OUTPUT_DIR / 'spectrograms.h5'\n",
    "\n",
    "# Flag to control whether to extract features or load existing\n",
    "EXTRACT_FEATURES = True  # Set to False to skip extraction and load existing features\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU device\")\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Phoneme audio directory (with context): {PHONEME_WAV_DIR}\")\n",
    "print(f\"Output directory (Version 2): {OUTPUT_DIR}\")\n",
    "print(f\"Features output directory: {FEATURES_OUTPUT_DIR}\")\n",
    "print(f\"Extract features: {EXTRACT_FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction functions imported from utils.dl_data_preparation\n"
     ]
    }
   ],
   "source": [
    "# Import feature extraction functions from utils module\n",
    "from utils.dl_data_preparation import (\n",
    "    extract_all_features,\n",
    "    extract_spectrogram_window,\n",
    "    SAMPLE_RATE, N_MELS, HOP_LENGTH, MFCC_N_COEFFS, SPECTROGRAM_WINDOW_MS\n",
    ")\n",
    "\n",
    "print(\"Feature extraction functions imported from utils.dl_data_preparation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Features from Context Audio Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading phoneme metadata from phoneme_intervals.csv...\n",
      "Phonemes shape (before filtering): (1337749, 5)\n",
      "Phonemes columns: ['utterance_id', 'phoneme', 'start_ms', 'end_ms', 'duration_ms']\n",
      "\n",
      "Filtering to only 'd' and 't' phonemes...\n",
      "Phonemes shape (after filtering d/t): (132992, 5)\n",
      "Phoneme distribution:\n",
      "phoneme\n",
      "t    74454\n",
      "d    58538\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Creating phoneme_id column...\n",
      "Created 132992 unique phoneme IDs\n",
      "\n",
      "Creating class column...\n",
      "Class distribution:\n",
      "class\n",
      "t    74454\n",
      "d    58538\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Updating audio paths to use context audio files...\n",
      "Creating audio_path column...\n",
      "\n",
      "Final dataset size: 132,992 phonemes with context audio\n",
      "\n",
      "Features file already exists: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/features/features.parquet\n",
      "Set EXTRACT_FEATURES=False to skip extraction and load existing features.\n",
      "Proceeding with extraction (will overwrite existing file)...\n",
      "\n",
      "============================================================\n",
      "EXTRACTING FEATURES FROM CONTEXT AUDIO FILES\n",
      "============================================================\n",
      "This will process 132,992 phonemes with context windows\n",
      "Audio files are longer (~300ms) due to ±100ms context\n",
      "============================================================\n",
      "\n",
      "Using 8 parallel workers...\n",
      "Extracting features from context audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 132992/132992 [36:04<00:00, 61.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted features for 132,992 phonemes\n",
      "\n",
      "Feature columns: 54\n",
      "Feature shape: (132992, 54)\n",
      "\n",
      "After expanding arrays: 132 columns\n",
      "Feature shape: (132992, 132)\n",
      "\n",
      "Performing quality assessment...\n",
      "Detected 13,300 outliers (10.0%)\n",
      "\n",
      "Saving features to /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/features/features.parquet...\n",
      "Features saved! Shape: (132992, 134)\n",
      "\n",
      "Extracting spectrograms from context audio files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting spectrograms: 100%|██████████| 132992/132992 [37:01<00:00, 59.85it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracted 132,992 spectrograms\n",
      "Spectrogram shape: (128, 7)\n",
      "\n",
      "Saving spectrograms to /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/features/spectrograms.h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving spectrograms: 100%|██████████| 132992/132992 [00:15<00:00, 8562.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectrograms saved!\n"
     ]
    }
   ],
   "source": [
    "from utils.dl_data_preparation import find_context_audio_path\n",
    "# Load phoneme metadata from fresh phoneme_intervals.csv file\n",
    "print(f\"Loading phoneme metadata from {PHONEMES_FILE.name}...\")\n",
    "df_phonemes = pd.read_csv(PHONEMES_FILE)\n",
    "print(f\"Phonemes shape (before filtering): {df_phonemes.shape}\")\n",
    "print(f\"Phonemes columns: {list(df_phonemes.columns)}\")\n",
    "\n",
    "# Filter to only 'd' and 't' phonemes for this notebook\n",
    "print(\"\\nFiltering to only 'd' and 't' phonemes...\")\n",
    "df_phonemes = df_phonemes[df_phonemes['phoneme'].isin(['d', 't'])].copy()\n",
    "print(f\"Phonemes shape (after filtering d/t): {df_phonemes.shape}\")\n",
    "print(f\"Phoneme distribution:\")\n",
    "print(df_phonemes['phoneme'].value_counts())\n",
    "\n",
    "# Create phoneme_id column (unique identifier for each phoneme)\n",
    "print(\"\\nCreating phoneme_id column...\")\n",
    "df_phonemes['phoneme_id'] = range(len(df_phonemes))\n",
    "print(f\"Created {len(df_phonemes)} unique phoneme IDs\")\n",
    "\n",
    "# Create class column (same as phoneme for this notebook)\n",
    "print(\"\\nCreating class column...\")\n",
    "df_phonemes['class'] = df_phonemes['phoneme']\n",
    "print(f\"Class distribution:\")\n",
    "print(df_phonemes['class'].value_counts())\n",
    "\n",
    "# Update audio paths to use context audio directory\n",
    "print(\"\\nUpdating audio paths to use context audio files...\")\n",
    "\n",
    "# Update audio paths\n",
    "if 'audio_path' in df_phonemes.columns:\n",
    "    print(\"Updating audio_path column...\")\n",
    "    df_phonemes['audio_path'] = df_phonemes.apply(lambda row: find_context_audio_path(row, PHONEME_WAV_DIR), axis=1)\n",
    "    valid_paths = df_phonemes['audio_path'].notna()\n",
    "    print(f\"Found context audio files: {valid_paths.sum()} / {len(df_phonemes)}\")\n",
    "    if not valid_paths.all():\n",
    "        print(f\"Warning: {len(df_phonemes) - valid_paths.sum()} audio files not found in context directory\")\n",
    "else:\n",
    "    print(\"Creating audio_path column...\")\n",
    "    df_phonemes['audio_path'] = df_phonemes.apply(lambda row: find_context_audio_path(row, PHONEME_WAV_DIR), axis=1)\n",
    "\n",
    "# Filter to only phonemes with valid audio paths\n",
    "df_phonemes = df_phonemes[df_phonemes['audio_path'].notna()].copy()\n",
    "print(f\"\\nFinal dataset size: {len(df_phonemes):,} phonemes with context audio\")\n",
    "\n",
    "# Check if features already exist\n",
    "if EXTRACT_FEATURES and FEATURES_FILE.exists():\n",
    "    print(f\"\\nFeatures file already exists: {FEATURES_FILE}\")\n",
    "    print(\"Set EXTRACT_FEATURES=False to skip extraction and load existing features.\")\n",
    "    print(\"Proceeding with extraction (will overwrite existing file)...\")\n",
    "\n",
    "if EXTRACT_FEATURES:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"EXTRACTING FEATURES FROM CONTEXT AUDIO FILES\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"This will process {len(df_phonemes):,} phonemes with context windows\")\n",
    "    print(f\"Audio files are longer (~300ms) due to ±100ms context\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Helper function for parallel processing\n",
    "    def process_single_phoneme(row_data):\n",
    "        \"\"\"Process a single phoneme row and extract features.\"\"\"\n",
    "        idx, row = row_data\n",
    "        audio_path = row['audio_path']\n",
    "        if audio_path is None or not Path(audio_path).exists():\n",
    "            return None\n",
    "        \n",
    "        features = extract_all_features(audio_path, phoneme_type='d-t')\n",
    "        if features is not None:\n",
    "            features['phoneme_id'] = row['phoneme_id']\n",
    "            features['class'] = row['class']\n",
    "            features['duration_ms'] = row['duration_ms']\n",
    "        return features\n",
    "    \n",
    "    # Use parallel processing\n",
    "    num_workers = os.cpu_count() or 4\n",
    "    print(f\"Using {num_workers} parallel workers...\")\n",
    "    \n",
    "    features_list = []\n",
    "    rows_to_process = [(idx, row) for idx, row in df_phonemes.iterrows()]\n",
    "    \n",
    "    print(\"Extracting features from context audio files...\")\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        future_to_row = {executor.submit(process_single_phoneme, row_data): row_data \n",
    "                      for row_data in rows_to_process}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_row), total=len(rows_to_process), desc=\"Extracting features\"):\n",
    "            try:\n",
    "                features = future.result()\n",
    "                if features is not None:\n",
    "                    features_list.append(features)\n",
    "            except Exception as e:\n",
    "                idx, row = future_to_row[future]\n",
    "                print(f\"Error processing phoneme {row.get('phoneme_id', 'unknown')}: {e}\")\n",
    "    \n",
    "    print(f\"\\nExtracted features for {len(features_list):,} phonemes\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_features = pd.DataFrame(features_list)\n",
    "    print(f\"\\nFeature columns: {len(df_features.columns)}\")\n",
    "    print(f\"Feature shape: {df_features.shape}\")\n",
    "    \n",
    "    # Expand array columns into separate columns\n",
    "    array_columns = []\n",
    "    for col in df_features.columns:\n",
    "        if df_features[col].dtype == 'object':\n",
    "            sample = df_features[col].iloc[0] if len(df_features) > 0 else None\n",
    "            if sample is not None and isinstance(sample, np.ndarray):\n",
    "                array_columns.append(col)\n",
    "    \n",
    "    # Expand array columns\n",
    "    for col in array_columns:\n",
    "        array_length = len(df_features[col].iloc[0])\n",
    "        for i in range(array_length):\n",
    "            new_col_name = f\"{col}_{i}\"\n",
    "            df_features[new_col_name] = df_features[col].apply(\n",
    "                lambda x: x[i] if isinstance(x, np.ndarray) and len(x) > i else np.nan\n",
    "            )\n",
    "        df_features = df_features.drop(columns=[col])\n",
    "    \n",
    "    print(f\"\\nAfter expanding arrays: {len(df_features.columns)} columns\")\n",
    "    print(f\"Feature shape: {df_features.shape}\")\n",
    "    \n",
    "    # Quality assessment and outlier detection\n",
    "    print(\"\\nPerforming quality assessment...\")\n",
    "    numeric_cols = df_features.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c not in ['phoneme_id', 'duration_ms']]\n",
    "    numeric_cols = [c for c in numeric_cols if df_features[c].std() > 1e-10]\n",
    "    \n",
    "    X_outlier = df_features[numeric_cols].fillna(0)\n",
    "    scaler_outlier = StandardScaler()\n",
    "    X_scaled = scaler_outlier.fit_transform(X_outlier)\n",
    "    \n",
    "    iso_forest = IsolationForest(contamination=0.1, random_state=RANDOM_STATE)\n",
    "    outlier_labels = iso_forest.fit_predict(X_scaled)\n",
    "    df_features['is_outlier_iso'] = outlier_labels == -1\n",
    "    print(f\"Detected {df_features['is_outlier_iso'].sum():,} outliers ({df_features['is_outlier_iso'].mean()*100:.1f}%)\")\n",
    "    \n",
    "    # Quality score\n",
    "    df_features['quality_score'] = (\n",
    "        (1 - df_features['spectral_flatness'].fillna(0.5)) * 0.3 +\n",
    "        (df_features['harmonic_noise_ratio'].fillna(1.0) / (df_features['harmonic_noise_ratio'].fillna(1.0).max() + 1e-10)) * 0.3 +\n",
    "        (1 - df_features['zcr_mean'].fillna(0.5) / (df_features['zcr_mean'].fillna(0.5).max() + 1e-10)) * 0.2 +\n",
    "        (1 - df_features['energy_cv'].fillna(1.0) / (df_features['energy_cv'].fillna(1.0).max() + 1e-10)) * 0.2\n",
    "    )\n",
    "    \n",
    "    # Save features\n",
    "    print(f\"\\nSaving features to {FEATURES_FILE}...\")\n",
    "    df_features.to_parquet(FEATURES_FILE, index=False)\n",
    "    print(f\"Features saved! Shape: {df_features.shape}\")\n",
    "    \n",
    "    # Extract spectrograms from context audio\n",
    "    print(f\"\\nExtracting spectrograms from context audio files...\")\n",
    "    spectrograms_dict = {}\n",
    "    \n",
    "    for idx, row in tqdm(df_phonemes.iterrows(), total=len(df_phonemes), desc=\"Extracting spectrograms\"):\n",
    "        audio_path = row['audio_path']\n",
    "        phoneme_id = row['phoneme_id']\n",
    "        \n",
    "        if audio_path is None or not Path(audio_path).exists():\n",
    "            continue\n",
    "        \n",
    "        spec = extract_spectrogram_window(audio_path, target_duration_ms=SPECTROGRAM_WINDOW_MS)\n",
    "        if spec is not None:\n",
    "            spectrograms_dict[phoneme_id] = spec\n",
    "    \n",
    "    print(f\"\\nExtracted {len(spectrograms_dict):,} spectrograms\")\n",
    "    if spectrograms_dict:\n",
    "        print(f\"Spectrogram shape: {list(spectrograms_dict.values())[0].shape}\")\n",
    "    \n",
    "    # Save spectrograms\n",
    "    print(f\"\\nSaving spectrograms to {SPECTROGRAMS_FILE}...\")\n",
    "    with h5py.File(SPECTROGRAMS_FILE, 'w') as f:\n",
    "        for phoneme_id, spec in tqdm(spectrograms_dict.items(), desc=\"Saving spectrograms\"):\n",
    "            f.create_dataset(str(phoneme_id), data=spec, compression='gzip')\n",
    "    print(f\"Spectrograms saved!\")\n",
    "    \n",
    "else:\n",
    "    # Load existing features\n",
    "    print(f\"\\nLoading existing features from {FEATURES_FILE}...\")\n",
    "    df_features = pd.read_parquet(FEATURES_FILE)\n",
    "    print(f\"Features shape: {df_features.shape}\")\n",
    "    print(f\"Features columns: {len(df_features.columns)}\")\n",
    "    \n",
    "    # Load existing spectrograms\n",
    "    print(f\"\\nLoading existing spectrograms from {SPECTROGRAMS_FILE}...\")\n",
    "    spectrograms_dict = {}\n",
    "    with h5py.File(SPECTROGRAMS_FILE, 'r') as f:\n",
    "        phoneme_ids = list(f.keys())\n",
    "        for phoneme_id in tqdm(phoneme_ids, desc=\"Loading spectrograms\"):\n",
    "            # Convert string keys to int to match DataFrame phoneme_id type\n",
    "            spectrograms_dict[int(phoneme_id)] = f[phoneme_id][:]\n",
    "    print(f\"Loaded {len(spectrograms_dict):,} spectrograms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Merge Features and Phoneme Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging features with phoneme metadata...\n",
      "Merged dataset shape: (132992, 141)\n",
      "\n",
      "'class' column found in merged DataFrame\n",
      "\n",
      "Class distribution:\n",
      "class\n",
      "t    74454\n",
      "d    58538\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution (%):\n",
      "class\n",
      "t    55.983819\n",
      "d    44.016181\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Class encoding: {'d': np.int64(0), 't': np.int64(1)}\n",
      "\n",
      "Phonemes with spectrograms: 132992 / 132992\n",
      "\n",
      "Number of feature columns: 130\n",
      "First 10 features: ['energy_rms', 'energy_rms_std', 'energy_zcr', 'energy_zcr_std', 'spectral_centroid', 'spectral_centroid_std', 'spectral_rolloff', 'spectral_rolloff_std', 'spectral_bandwidth', 'spectral_bandwidth_std']\n"
     ]
    }
   ],
   "source": [
    "# Merge features with phoneme metadata\n",
    "print(\"Merging features with phoneme metadata...\")\n",
    "df = df_phonemes.merge(df_features, on='phoneme_id', how='inner', suffixes=('', '_features'))\n",
    "print(f\"Merged dataset shape: {df.shape}\")\n",
    "\n",
    "# Handle duplicate columns from merge\n",
    "if 'class_features' in df.columns:\n",
    "    df = df.drop(columns=['class_features'])\n",
    "if 'class' not in df.columns:\n",
    "    if 'phoneme' in df.columns:\n",
    "        print(\"\\n'class' column not found, creating from 'phoneme' column...\")\n",
    "        df['class'] = df['phoneme']\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'class' nor 'phoneme' column found in merged DataFrame\")\n",
    "else:\n",
    "    print(\"\\n'class' column found in merged DataFrame\")\n",
    "\n",
    "# Filter to only d and t classes (exclude pf if present)\n",
    "if 'pf' in df['class'].values:\n",
    "    print(\"\\nFiltering out 'pf' class, keeping only 'd' and 't'...\")\n",
    "    df = df[df['class'].isin(['d', 't'])].copy()\n",
    "    print(f\"Dataset after filtering: {len(df)} samples\")\n",
    "\n",
    "# Check class distribution\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(df['class'].value_counts())\n",
    "print(f\"\\nClass distribution (%):\")\n",
    "print(df['class'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "df['class_encoded'] = le.fit_transform(df['class'])  # d=0, t=1\n",
    "print(f\"\\nClass encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "\n",
    "# Check which phonemes have spectrograms\n",
    "df['has_spectrogram'] = df['phoneme_id'].isin(spectrograms_dict.keys())\n",
    "print(f\"\\nPhonemes with spectrograms: {df['has_spectrogram'].sum()} / {len(df)}\")\n",
    "\n",
    "# Get feature columns (exclude metadata and non-numeric columns)\n",
    "# Note: duration_ms_y (from features) should be included, but duration_ms_x (from phonemes) should be excluded\n",
    "exclude_cols = ['phoneme_id', 'utterance_id', 'phoneme', 'class', 'class_x', 'class_y', \n",
    "                'class_encoded', 'start_ms', 'end_ms', 'duration_ms', 'duration_ms_x', \n",
    "                'audio_path', 'is_outlier_iso', 'split', 'has_spectrogram',\n",
    "                'class_features']  # Exclude merge suffixes (but keep duration_ms_y)\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "# Filter to only numeric columns (using pd.api.types for consistency with data_loader.py)\n",
    "feature_cols = [col for col in feature_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "print(f\"\\nNumber of feature columns: {len(feature_cols)}\")\n",
    "print(f\"First 10 features: {feature_cols[:10]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train/Val/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/feature_cols.json\n"
     ]
    }
   ],
   "source": [
    "# Save feature columns list (important for model loading)\n",
    "# This file is needed for loading models later\n",
    "with open(OUTPUT_DIR / 'feature_cols.json', 'w') as f:\n",
    "    json.dump(feature_cols, f, indent=2)\n",
    "print(f\"Feature columns saved to {OUTPUT_DIR / 'feature_cols.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset after filtering: 132992 samples\n",
      "\n",
      "Train set: 93,147 samples (70.0%)\n",
      "  Class distribution: [41000 52147]\n",
      "Val set: 19,896 samples (15.0%)\n",
      "  Class distribution: [ 8757 11139]\n",
      "Test set: 19,949 samples (15.0%)\n",
      "  Class distribution: [ 8781 11168]\n",
      "\n",
      "Split indices saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/split_indices.json\n"
     ]
    }
   ],
   "source": [
    "# Filter to only phonemes with spectrograms\n",
    "df = df[df['has_spectrogram']].copy()\n",
    "print(f\"Dataset after filtering: {len(df)} samples\")\n",
    "\n",
    "# Train/Val/Test split (70/15/15) with stratification\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df.index, df['class_encoded'], \n",
    "    test_size=0.15, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=df['class_encoded']\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.176,  # 0.176 ≈ 15/85\n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Create split column\n",
    "df['split'] = 'train'\n",
    "df.loc[X_val, 'split'] = 'val'\n",
    "df.loc[X_test, 'split'] = 'test'\n",
    "\n",
    "print(f\"\\nTrain set: {len(X_train):,} samples ({len(X_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_train, 'class_encoded'])}\")\n",
    "print(f\"Val set: {len(X_val):,} samples ({len(X_val)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_val, 'class_encoded'])}\")\n",
    "print(f\"Test set: {len(X_test):,} samples ({len(X_test)/len(df)*100:.1f}%)\")\n",
    "print(f\"  Class distribution: {np.bincount(df.loc[X_test, 'class_encoded'])}\")\n",
    "\n",
    "# Save split indices\n",
    "split_indices = {\n",
    "    'train': [int(idx) for idx in X_train],\n",
    "    'val': [int(idx) for idx in X_val],\n",
    "    'test': [int(idx) for idx in X_test]\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'split_indices.json', 'w') as f:\n",
    "    json.dump(split_indices, f)\n",
    "print(f\"\\nSplit indices saved to {OUTPUT_DIR / 'split_indices.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create PyTorch Dataset Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset classes imported from utils.dl_data_preparation\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset classes from utils module\n",
    "from utils.dl_data_preparation import (\n",
    "    SpectrogramDataset,\n",
    "    FeatureDataset,\n",
    "    HybridDataset,\n",
    "    RawAudioDataset,\n",
    "    ContextAudioDataset,\n",
    "    SequenceDataset\n",
    ")\n",
    "\n",
    "print(\"Dataset classes imported from utils.dl_data_preparation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaler saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/feature_scaler.joblib\n",
      "\n",
      "Class weights: {0: np.float64(1.1359390243902439), 1: np.float64(0.8931194507833624)}\n",
      "Class weights saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/class_weights.json\n",
      "\n",
      "All datasets created!\n",
      "Train spectrogram dataset: 93147 samples\n",
      "Train feature dataset: 93147 samples\n",
      "Train hybrid dataset: 93147 samples\n",
      "Train sequence dataset: 93147 samples\n",
      "Train raw audio dataset: 93147 samples\n",
      "Train context audio dataset: 93147 samples\n"
     ]
    }
   ],
   "source": [
    "# Create feature scaler on training data\n",
    "train_df = df[df['split'] == 'train']\n",
    "X_train_features = train_df[feature_cols].values.astype(np.float32)\n",
    "X_train_features = np.nan_to_num(X_train_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "feature_scaler = StandardScaler()\n",
    "feature_scaler.fit(X_train_features)\n",
    "\n",
    "# Save scaler\n",
    "import joblib\n",
    "joblib.dump(feature_scaler, OUTPUT_DIR / 'feature_scaler.joblib')\n",
    "print(f\"Feature scaler saved to {OUTPUT_DIR / 'feature_scaler.joblib'}\")\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(df[df['split'] == 'train']['class_encoded']),\n",
    "    y=df[df['split'] == 'train']['class_encoded']\n",
    ")\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "print(f\"\\nClass weights: {class_weights_dict}\")\n",
    "\n",
    "# Save class weights\n",
    "with open(OUTPUT_DIR / 'class_weights.json', 'w') as f:\n",
    "    json.dump(class_weights_dict, f)\n",
    "print(f\"Class weights saved to {OUTPUT_DIR / 'class_weights.json'}\")\n",
    "\n",
    "# Create datasets\n",
    "train_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='train')\n",
    "val_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='val')\n",
    "test_spectrogram_ds = SpectrogramDataset(df, spectrograms_dict, split='test')\n",
    "\n",
    "train_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='train')\n",
    "val_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='val')\n",
    "test_feature_ds = FeatureDataset(df, feature_cols, scaler=feature_scaler, split='test')\n",
    "\n",
    "train_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='train')\n",
    "val_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='val')\n",
    "test_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='test')\n",
    "\n",
    "train_sequence_ds = SequenceDataset(df, spectrograms_dict, split='train')\n",
    "val_sequence_ds = SequenceDataset(df, spectrograms_dict, split='val')\n",
    "test_sequence_ds = SequenceDataset(df, spectrograms_dict, split='test')\n",
    "\n",
    "# Note: With context, audio is longer (~300ms instead of ~100ms), so adjust max_length accordingly\n",
    "# Original phoneme ~100ms + 200ms context = ~300ms total\n",
    "# At 16kHz: 300ms = 4800 samples\n",
    "train_raw_audio_ds = RawAudioDataset(df, split='train', sample_rate=16000, max_length=4800)  # ~300ms at 16kHz\n",
    "val_raw_audio_ds = RawAudioDataset(df, split='val', sample_rate=16000, max_length=4800)\n",
    "test_raw_audio_ds = RawAudioDataset(df, split='test', sample_rate=16000, max_length=4800)\n",
    "\n",
    "train_context_audio_ds = ContextAudioDataset(df, split='train', sample_rate=16000)\n",
    "val_context_audio_ds = ContextAudioDataset(df, split='val', sample_rate=16000)\n",
    "test_context_audio_ds = ContextAudioDataset(df, split='test', sample_rate=16000)\n",
    "\n",
    "print(\"\\nAll datasets created!\")\n",
    "print(f\"Train spectrogram dataset: {len(train_spectrogram_ds)} samples\")\n",
    "print(f\"Train feature dataset: {len(train_feature_ds)} samples\")\n",
    "print(f\"Train hybrid dataset: {len(train_hybrid_ds)} samples\")\n",
    "print(f\"Train sequence dataset: {len(train_sequence_ds)} samples\")\n",
    "print(f\"Train raw audio dataset: {len(train_raw_audio_ds)} samples\")\n",
    "print(f\"Train context audio dataset: {len(train_context_audio_ds)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create DataLoaders with Weighted Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All DataLoaders created!\n",
      "\n",
      "Train batches (spectrogram): 1456\n",
      "Train batches (feature): 1456\n",
      "Train batches (hybrid): 1456\n",
      "\n",
      "Testing a batch from spectrogram dataset...\n",
      "Batch shape: torch.Size([64, 1, 128, 7]), Labels shape: torch.Size([64])\n",
      "\n",
      "Testing a batch from raw audio dataset (with context)...\n",
      "Audio batch shape: torch.Size([64, 4800]), Labels shape: torch.Size([64])\n",
      "Audio duration: 300.0ms (expected ~300ms with context)\n"
     ]
    }
   ],
   "source": [
    "# Compute sample weights for weighted sampling\n",
    "train_labels = df[df['split'] == 'train']['class_encoded'].values\n",
    "sample_weights = np.array([class_weights[label] for label in train_labels])\n",
    "sampler = WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_spectrogram_loader = DataLoader(train_spectrogram_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_spectrogram_loader = DataLoader(val_spectrogram_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_spectrogram_loader = DataLoader(test_spectrogram_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_feature_loader = DataLoader(train_feature_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_feature_loader = DataLoader(val_feature_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_feature_loader = DataLoader(test_feature_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_hybrid_loader = DataLoader(train_hybrid_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_hybrid_loader = DataLoader(val_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_hybrid_loader = DataLoader(test_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_sequence_loader = DataLoader(train_sequence_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_sequence_loader = DataLoader(val_sequence_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_sequence_loader = DataLoader(test_sequence_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_raw_audio_loader = DataLoader(train_raw_audio_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_raw_audio_loader = DataLoader(val_raw_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_raw_audio_loader = DataLoader(test_raw_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "train_context_audio_loader = DataLoader(train_context_audio_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_context_audio_loader = DataLoader(val_context_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_context_audio_loader = DataLoader(test_context_audio_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"All DataLoaders created!\")\n",
    "print(f\"\\nTrain batches (spectrogram): {len(train_spectrogram_loader)}\")\n",
    "print(f\"Train batches (feature): {len(train_feature_loader)}\")\n",
    "print(f\"Train batches (hybrid): {len(train_hybrid_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "print(\"\\nTesting a batch from spectrogram dataset...\")\n",
    "sample_batch = next(iter(train_spectrogram_loader))\n",
    "print(f\"Batch shape: {sample_batch[0].shape}, Labels shape: {sample_batch[1].shape}\")\n",
    "\n",
    "# Test raw audio batch (should be longer due to context)\n",
    "print(\"\\nTesting a batch from raw audio dataset (with context)...\")\n",
    "sample_audio_batch = next(iter(train_raw_audio_loader))\n",
    "print(f\"Audio batch shape: {sample_audio_batch[0].shape}, Labels shape: {sample_audio_batch[1].shape}\")\n",
    "print(f\"Audio duration: {sample_audio_batch[0].shape[1] / 16000 * 1000:.1f}ms (expected ~300ms with context)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Dataset Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info saved to /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/dataset_info.json\n",
      "\n",
      "Dataset summary:\n",
      "  Total samples: 132992\n",
      "  Train: 93147\n",
      "  Val: 19896\n",
      "  Test: 19949\n",
      "  Features: 130\n",
      "  Spectrogram shape: [128, 7]\n",
      "  Context window: ±100ms\n",
      "  Audio max length: 4800 samples (~300ms)\n",
      "  Audio source: phoneme_wav_with_context\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Save dataset information\n",
    "dataset_info = {\n",
    "    'total_samples': len(df),\n",
    "    'train_samples': len(df[df['split'] == 'train']),\n",
    "    'val_samples': len(df[df['split'] == 'val']),\n",
    "    'test_samples': len(df[df['split'] == 'test']),\n",
    "    'n_features': len(feature_cols),\n",
    "    'spectrogram_shape': list(spectrograms_dict[list(spectrograms_dict.keys())[0]].shape),\n",
    "    'context_window_ms': 100,  # ±100ms context\n",
    "    'audio_max_length_samples': 4800,  # ~300ms at 16kHz (100ms phoneme + 200ms context)\n",
    "    'class_distribution': {\n",
    "        'train': df[df['split'] == 'train']['class'].value_counts().to_dict(),\n",
    "        'val': df[df['split'] == 'val']['class'].value_counts().to_dict(),\n",
    "        'test': df[df['split'] == 'test']['class'].value_counts().to_dict()\n",
    "    },\n",
    "    'class_weights': class_weights_dict,\n",
    "    'feature_columns': feature_cols,\n",
    "    'audio_source': 'phoneme_wav_with_context'\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / 'dataset_info.json', 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)\n",
    "\n",
    "print(f\"Dataset info saved to {OUTPUT_DIR / 'dataset_info.json'}\")\n",
    "print(f\"\\nDataset summary:\")\n",
    "print(f\"  Total samples: {dataset_info['total_samples']}\")\n",
    "print(f\"  Train: {dataset_info['train_samples']}\")\n",
    "print(f\"  Val: {dataset_info['val_samples']}\")\n",
    "print(f\"  Test: {dataset_info['test_samples']}\")\n",
    "print(f\"  Features: {dataset_info['n_features']}\")\n",
    "print(f\"  Spectrogram shape: {dataset_info['spectrogram_shape']}\")\n",
    "print(f\"  Context window: ±{dataset_info['context_window_ms']}ms\")\n",
    "print(f\"  Audio max length: {dataset_info['audio_max_length_samples']} samples (~{dataset_info['audio_max_length_samples']/16:.0f}ms)\")\n",
    "print(f\"  Audio source: {dataset_info['audio_source']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfa310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}