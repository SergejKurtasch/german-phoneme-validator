{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved Hybrid CNN+MLP Training (V4.3 Enhanced) with Context Windows\n",
    "\n",
    "**Enhanced version** with Focal Loss, SpecAugment, and improved architecture:\n",
    "\n",
    "**Key improvements in V4.3:**\n",
    "1. **Focal Loss**: Replaces LabelSmoothingCrossEntropy to focus on hard examples (70%+ high-confidence errors)\n",
    "2. **SpecAugment**: Frequency and time masking for spectrogram augmentation during training\n",
    "3. **Enhanced Architecture**:\n",
    "   - Multi-head attention in cross-attention fusion\n",
    "   - Residual connections in MLP branch\n",
    "   - Enhanced SE blocks in CNN branch\n",
    "\n",
    "**Expected improvements:**\n",
    "- Better handling of hard examples (Focal Loss)\n",
    "- Improved generalization (SpecAugment)\n",
    "- Better feature fusion (Multi-head attention)\n",
    "- More stable training (Residual connections)\n",
    "- Target: Accuracy > 0.976 (from 0.9660)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n",
      "Data directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2\n",
      "Features directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/features\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "import joblib\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import math\n",
    "\n",
    "# Project root\n",
    "PROJECT_ROOT = Path('/Volumes/SSanDisk/SpeechRec-German')\n",
    "\n",
    "# Data directory (with context v2 - includes VOT, burst features)\n",
    "DATA_DIR = PROJECT_ROOT / 'artifacts' / 'd-t_dl_models_with_context_v2'\n",
    "FEATURES_DIR = DATA_DIR / 'features'\n",
    "\n",
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"Using MPS device\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"Using CPU device\")\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Features directory: {FEATURES_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data with Context Windows (V2 - with VOT and Burst Features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (132992, 134)\n",
      "Feature columns (loaded): 130\n",
      "Warning: 1 feature columns are missing from DataFrame\n",
      "Missing columns: ['duration_ms_features']...\n",
      "Note: 'duration_ms_features' is missing - this is expected if duration_ms wasn't duplicated during merge.\n",
      "      This column is not a real feature and can be safely ignored.\n",
      "Feature columns (filtered): 129\n",
      "Warning: Feature count mismatch. Scaler expects 130 features, but we have 129\n",
      "This is OK if some features were removed from the dataset. The scaler will be applied to available features.\n",
      "\n",
      "Metadata columns present: ['phoneme_id', 'class', 'duration_ms']\n",
      "\n",
      "Class encoding: {'d': np.int64(0), 't': np.int64(1)}\n",
      "Class distribution:\n",
      "class\n",
      "t    74454\n",
      "d    58538\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Split distribution:\n",
      "split\n",
      "train    93147\n",
      "test     19949\n",
      "val      19896\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading spectrograms: 100%|██████████| 132992/132992 [00:45<00:00, 2944.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 132,992 spectrograms\n",
      "Spectrogram shape: (128, 7)\n",
      "\n",
      "Dataset after filtering for spectrograms: 132992 samples\n"
     ]
    }
   ],
   "source": [
    "# Load feature columns\n",
    "with open(DATA_DIR / 'feature_cols.json', 'r') as f:\n",
    "    feature_cols = json.load(f)\n",
    "\n",
    "# Load feature scaler\n",
    "feature_scaler = joblib.load(DATA_DIR / 'feature_scaler.joblib')\n",
    "\n",
    "# Load class weights\n",
    "with open(DATA_DIR / 'class_weights.json', 'r') as f:\n",
    "    class_weights_dict = json.load(f)\n",
    "\n",
    "# Load features DataFrame (from 02.2 - includes VOT, burst features)\n",
    "df = pd.read_parquet(FEATURES_DIR / 'features.parquet')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Feature columns (loaded): {len(feature_cols)}\")\n",
    "\n",
    "# Filter feature_cols to only include columns that exist in DataFrame\n",
    "original_feature_count = len(feature_cols)\n",
    "feature_cols = [col for col in feature_cols if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "if len(feature_cols) != original_feature_count:\n",
    "    missing_cols = set([col for col in json.load(open(DATA_DIR / 'feature_cols.json', 'r')) if col not in df.columns])\n",
    "    print(f\"Warning: {original_feature_count - len(feature_cols)} feature columns are missing from DataFrame\")\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {list(missing_cols)[:10]}...\")\n",
    "        \n",
    "    if 'duration_ms_features' in missing_cols:\n",
    "        print(\"Note: 'duration_ms_features' is missing - this is expected if duration_ms wasn't duplicated during merge.\")\n",
    "        print(\"      This column is not a real feature and can be safely ignored.\")\n",
    "\n",
    "print(f\"Feature columns (filtered): {len(feature_cols)}\")\n",
    "\n",
    "# Verify feature count matches scaler\n",
    "if hasattr(feature_scaler, 'n_features_in_'):\n",
    "    if len(feature_cols) != feature_scaler.n_features_in_:\n",
    "        print(f\"Warning: Feature count mismatch. Scaler expects {feature_scaler.n_features_in_} features, but we have {len(feature_cols)}\")\n",
    "        print(\"This is OK if some features were removed from the dataset. The scaler will be applied to available features.\")\n",
    "\n",
    "# Check what metadata columns we have\n",
    "metadata_cols = ['phoneme_id', 'class', 'duration_ms', 'phoneme', 'utterance_id']\n",
    "present_metadata = [col for col in metadata_cols if col in df.columns]\n",
    "print(f\"\\nMetadata columns present: {present_metadata}\")\n",
    "\n",
    "# Handle class column\n",
    "if 'class' not in df.columns:\n",
    "    if 'phoneme' in df.columns:\n",
    "        df['class'] = df['phoneme']\n",
    "        print(\"Created 'class' column from 'phoneme'\")\n",
    "    else:\n",
    "        raise ValueError(\"Neither 'class' nor 'phoneme' column found in features.parquet.\")\n",
    "\n",
    "# Filter to only d and t classes\n",
    "if 'pf' in df['class'].values:\n",
    "    df = df[df['class'].isin(['d', 't'])].copy()\n",
    "    print(f\"Dataset after filtering to d/t: {len(df)} samples\")\n",
    "\n",
    "# Encode target\n",
    "le = LabelEncoder()\n",
    "df['class_encoded'] = le.fit_transform(df['class'])  # d=0, t=1\n",
    "print(f\"\\nClass encoding: {dict(zip(le.classes_, le.transform(le.classes_)))}\")\n",
    "print(f\"Class distribution:\\n{df['class'].value_counts()}\")\n",
    "\n",
    "# Load split indices\n",
    "with open(DATA_DIR / 'split_indices.json', 'r') as f:\n",
    "    split_indices = json.load(f)\n",
    "\n",
    "# Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Create split column based on indices\n",
    "df['split'] = 'train'\n",
    "if len(df) > max(split_indices['val'] + split_indices['test']):\n",
    "    df.loc[split_indices['val'], 'split'] = 'val'\n",
    "    df.loc[split_indices['test'], 'split'] = 'test'\n",
    "else:\n",
    "    print(\"Warning: Split indices may not match DataFrame indices. Using phoneme_id matching...\")\n",
    "    val_ids = set(df.loc[split_indices['val'], 'phoneme_id'].values) if len(df) > max(split_indices['val']) else set()\n",
    "    test_ids = set(df.loc[split_indices['test'], 'phoneme_id'].values) if len(df) > max(split_indices['test']) else set()\n",
    "    df.loc[df['phoneme_id'].isin(val_ids), 'split'] = 'val'\n",
    "    df.loc[df['phoneme_id'].isin(test_ids), 'split'] = 'test'\n",
    "\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df['split'].value_counts())\n",
    "\n",
    "# Load spectrograms\n",
    "spectrograms_dict = {}\n",
    "with h5py.File(FEATURES_DIR / 'spectrograms.h5', 'r') as f:\n",
    "    phoneme_ids = list(f.keys())\n",
    "    for phoneme_id in tqdm(phoneme_ids, desc=\"Loading spectrograms\"):\n",
    "        spectrograms_dict[phoneme_id] = f[phoneme_id][:]\n",
    "\n",
    "print(f\"\\nLoaded {len(spectrograms_dict):,} spectrograms\")\n",
    "if spectrograms_dict:\n",
    "    print(f\"Spectrogram shape: {list(spectrograms_dict.values())[0].shape}\")\n",
    "\n",
    "# Filter to only phonemes with spectrograms\n",
    "df['phoneme_id_str'] = df['phoneme_id'].astype(str)\n",
    "df['has_spectrogram'] = df['phoneme_id_str'].isin(spectrograms_dict.keys())\n",
    "df = df[df['has_spectrogram']].copy()\n",
    "print(f\"\\nDataset after filtering for spectrograms: {len(df)} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define SpecAugment for Spectrogram Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpecAugment class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class SpecAugment:\n",
    "    \"\"\"\n",
    "    SpecAugment: Simple spectrogram augmentation for speech recognition.\n",
    "    Applies frequency masking and time masking to spectrograms.\n",
    "    Adaptively adjusts parameters based on spectrogram dimensions.\n",
    "    \"\"\"\n",
    "    def __init__(self, F=27, T=40, m_F=2, m_T=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            F: Maximum frequency mask width (will be clamped to H-1)\n",
    "            T: Maximum time mask width (will be clamped to W-1)\n",
    "            m_F: Number of frequency masks\n",
    "            m_T: Number of time masks\n",
    "        \"\"\"\n",
    "        self.F = F\n",
    "        self.T = T\n",
    "        self.m_F = m_F\n",
    "        self.m_T = m_T\n",
    "    \n",
    "    def __call__(self, spectrogram):\n",
    "        \"\"\"\n",
    "        Apply SpecAugment to spectrogram.\n",
    "        Args:\n",
    "            spectrogram: numpy array of shape (C, H, W) or (H, W)\n",
    "        Returns:\n",
    "            Augmented spectrogram\n",
    "        \"\"\"\n",
    "        # Handle different input shapes\n",
    "        if len(spectrogram.shape) == 2:\n",
    "            # (H, W) -> (1, H, W)\n",
    "            spec = np.expand_dims(spectrogram, axis=0)\n",
    "            squeeze_output = True\n",
    "        else:\n",
    "            spec = spectrogram.copy()\n",
    "            squeeze_output = False\n",
    "        \n",
    "        C, H, W = spec.shape\n",
    "        \n",
    "        # Adaptively adjust mask sizes to fit spectrogram dimensions\n",
    "        # Ensure we can always apply at least some masking\n",
    "        max_F = max(1, min(self.F, H - 1))  # At least 1, at most H-1\n",
    "        max_T = max(1, min(self.T, W - 1))  # At least 1, at most W-1\n",
    "        \n",
    "        # Apply frequency masking\n",
    "        for _ in range(self.m_F):\n",
    "            if max_F > 0 and H > 0:\n",
    "                f = np.random.randint(0, max_F + 1)\n",
    "                if f > 0 and H - f > 0:\n",
    "                    f0 = np.random.randint(0, H - f + 1)\n",
    "                    spec[:, f0:f0+f, :] = 0\n",
    "        \n",
    "        # Apply time masking\n",
    "        for _ in range(self.m_T):\n",
    "            if max_T > 0 and W > 0:\n",
    "                t = np.random.randint(0, max_T + 1)\n",
    "                if t > 0 and W - t > 0:\n",
    "                    t0 = np.random.randint(0, W - t + 1)\n",
    "                    spec[:, :, t0:t0+t] = 0\n",
    "        \n",
    "        if squeeze_output:\n",
    "            spec = np.squeeze(spec, axis=0)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "print(\"SpecAugment class defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset Classes with SpecAugment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count mismatch detected: 129 features in DataFrame vs 130 in scaler\n",
      "Retraining scaler on train split with current features...\n",
      "Scaler retrained on 129 features\n",
      "Train dataset: 93147 samples (with SpecAugment)\n",
      "Val dataset: 19896 samples\n",
      "Test dataset: 19949 samples\n",
      "\n",
      "Train batches: 1456\n",
      "Val batches: 311\n",
      "Test batches: 312\n",
      "\n",
      "Sample batch - Spectrogram shape: torch.Size([64, 1, 128, 7]), Features shape: torch.Size([64, 129]), Labels shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class HybridDataset(Dataset):\n",
    "    \"\"\"Dataset for hybrid models using both spectrograms and features with SpecAugment\"\"\"\n",
    "    def __init__(self, df, spectrograms_dict, feature_cols, scaler=None, split='train', fit_scaler=False, transform=None, use_specaugment=False):\n",
    "        self.df = df[df['split'] == split].reset_index(drop=True)\n",
    "        self.spectrograms_dict = spectrograms_dict\n",
    "        self.transform = transform\n",
    "        self.split = split\n",
    "        self.use_specaugment = use_specaugment and (split == 'train')  # Only apply to training\n",
    "        \n",
    "        # Initialize SpecAugment if needed\n",
    "        if self.use_specaugment:\n",
    "            self.specaugment = SpecAugment(F=27, T=40, m_F=2, m_T=2)\n",
    "        \n",
    "        self.feature_cols = [col for col in feature_cols if col in self.df.columns and pd.api.types.is_numeric_dtype(self.df[col])]\n",
    "        if len(self.feature_cols) != len(feature_cols):\n",
    "            missing = set(feature_cols) - set(self.feature_cols)\n",
    "            print(f\"Warning: {len(missing)} feature columns missing from DataFrame: {list(missing)[:5]}...\")\n",
    "        \n",
    "        X_features = self.df[self.feature_cols].values.astype(np.float32)\n",
    "        X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        \n",
    "        if fit_scaler:\n",
    "            self.scaler = StandardScaler()\n",
    "            X_features = self.scaler.fit_transform(X_features)\n",
    "        elif scaler is not None:\n",
    "            if hasattr(scaler, 'n_features_in_') and X_features.shape[1] != scaler.n_features_in_:\n",
    "                print(f\"Warning: Feature count mismatch ({X_features.shape[1]} vs {scaler.n_features_in_}). Retraining scaler on current features.\")\n",
    "                self.scaler = StandardScaler()\n",
    "                X_features = self.scaler.fit_transform(X_features)\n",
    "            else:\n",
    "                self.scaler = scaler\n",
    "                X_features = self.scaler.transform(X_features)\n",
    "        else:\n",
    "            self.scaler = None\n",
    "        \n",
    "        self.X_features = torch.from_numpy(X_features)\n",
    "        self.y = torch.from_numpy(self.df['class_encoded'].values).long()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        phoneme_id = str(row['phoneme_id'])\n",
    "        \n",
    "        spectrogram = self.spectrograms_dict[phoneme_id].astype(np.float32)\n",
    "        if len(spectrogram.shape) == 2:\n",
    "            spectrogram = np.expand_dims(spectrogram, axis=0)\n",
    "        spectrogram = (spectrogram - spectrogram.min()) / (spectrogram.max() - spectrogram.min() + 1e-8)\n",
    "        \n",
    "        # Apply SpecAugment during training\n",
    "        if self.use_specaugment:\n",
    "            spectrogram = self.specaugment(spectrogram)\n",
    "        \n",
    "        if self.transform:\n",
    "            spectrogram = self.transform(spectrogram)\n",
    "        \n",
    "        features = self.X_features[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        return (torch.from_numpy(spectrogram), features), label\n",
    "\n",
    "# Check and retrain scaler if needed\n",
    "train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "train_feature_cols = [col for col in feature_cols if col in train_df.columns and pd.api.types.is_numeric_dtype(train_df[col])]\n",
    "feature_cols = train_feature_cols\n",
    "\n",
    "if hasattr(feature_scaler, 'n_features_in_') and len(feature_cols) != feature_scaler.n_features_in_:\n",
    "    print(f\"Feature count mismatch detected: {len(feature_cols)} features in DataFrame vs {feature_scaler.n_features_in_} in scaler\")\n",
    "    print(\"Retraining scaler on train split with current features...\")\n",
    "    X_train_features = train_df[feature_cols].values.astype(np.float32)\n",
    "    X_train_features = np.nan_to_num(X_train_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    feature_scaler = StandardScaler()\n",
    "    feature_scaler.fit(X_train_features)\n",
    "    print(f\"Scaler retrained on {len(feature_cols)} features\")\n",
    "else:\n",
    "    print(f\"Using existing scaler with {feature_scaler.n_features_in_} features\")\n",
    "\n",
    "# Create datasets with SpecAugment for training\n",
    "train_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='train', use_specaugment=True)\n",
    "val_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='val', use_specaugment=False)\n",
    "test_hybrid_ds = HybridDataset(df, spectrograms_dict, feature_cols, scaler=feature_scaler, split='test', use_specaugment=False)\n",
    "\n",
    "print(f\"Train dataset: {len(train_hybrid_ds)} samples (with SpecAugment)\")\n",
    "print(f\"Val dataset: {len(val_hybrid_ds)} samples\")\n",
    "print(f\"Test dataset: {len(test_hybrid_ds)} samples\")\n",
    "\n",
    "# Create weighted sampler\n",
    "train_labels = df[df['split'] == 'train']['class_encoded'].values\n",
    "class_weights_array = np.array([class_weights_dict.get(str(i), class_weights_dict.get(i, 1.0)) for i in range(2)])\n",
    "sample_weights = np.array([class_weights_array[label] for label in train_labels])\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "BATCH_SIZE = 64\n",
    "train_hybrid_loader = DataLoader(train_hybrid_ds, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\n",
    "val_hybrid_loader = DataLoader(val_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_hybrid_loader = DataLoader(test_hybrid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nTrain batches: {len(train_hybrid_loader)}\")\n",
    "print(f\"Val batches: {len(val_hybrid_loader)}\")\n",
    "print(f\"Test batches: {len(test_hybrid_loader)}\")\n",
    "\n",
    "# Test a batch\n",
    "sample_batch = next(iter(train_hybrid_loader))\n",
    "print(f\"\\nSample batch - Spectrogram shape: {sample_batch[0][0].shape}, Features shape: {sample_batch[0][1].shape}, Labels shape: {sample_batch[1].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Enhanced Model Architecture V4.3 with Multi-Head Attention and Residual Connections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture V4.3 (Enhanced) defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define Residual Block for CNN\n",
    "class ResidualBlock2D(nn.Module):\n",
    "    \"\"\"Residual block for CNN branch\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock2D, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Enhanced Channel Attention Module (SE block with improved design)\n",
    "class EnhancedChannelAttention(nn.Module):\n",
    "    \"\"\"Enhanced Channel attention module with improved design\"\"\"\n",
    "    def __init__(self, channels, reduction=8):\n",
    "        super(EnhancedChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # Use smaller reduction for better capacity\n",
    "        reduced_dim = max(1, channels // reduction)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, reduced_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(reduced_dim, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        avg_out = self.fc(self.avg_pool(x).view(b, c))\n",
    "        max_out = self.fc(self.max_pool(x).view(b, c))\n",
    "        out = avg_out + max_out\n",
    "        return x * out.view(b, c, 1, 1)\n",
    "\n",
    "\n",
    "# Define Feature Attention Module (SE-like for MLP features)\n",
    "class FeatureAttention(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation attention for feature vectors\"\"\"\n",
    "    def __init__(self, n_features, reduction=8):\n",
    "        super(FeatureAttention, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        reduced_dim = max(1, n_features // reduction)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_features, reduced_dim, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(reduced_dim, n_features, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        attention_weights = self.fc(x)\n",
    "        return x * attention_weights\n",
    "\n",
    "\n",
    "# Define Multi-Scale Convolution Block\n",
    "class MultiScaleConvBlock(nn.Module):\n",
    "    \"\"\"Multi-scale convolution with parallel 3x3 and 5x5 kernels\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(MultiScaleConvBlock, self).__init__()\n",
    "        self.conv3x3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv5x5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 2, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(out_channels // 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out3x3 = self.conv3x3(x)\n",
    "        out5x5 = self.conv5x5(x)\n",
    "        return torch.cat([out3x3, out5x5], dim=1)\n",
    "\n",
    "\n",
    "# Multi-Head Cross-Attention Fusion Module\n",
    "class MultiHeadCrossAttentionFusion(nn.Module):\n",
    "    \"\"\"Multi-head cross-attention between CNN and MLP outputs\"\"\"\n",
    "    def __init__(self, cnn_dim, mlp_dim, hidden_dim=256, num_heads=4):\n",
    "        super(MultiHeadCrossAttentionFusion, self).__init__()\n",
    "        self.cnn_dim = cnn_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads\n",
    "        \n",
    "        assert hidden_dim % num_heads == 0, \"hidden_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Projections for multi-head attention\n",
    "        self.cnn_to_qkv = nn.Linear(cnn_dim, hidden_dim * 3)\n",
    "        self.mlp_to_qkv = nn.Linear(mlp_dim, hidden_dim * 3)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "        # Projections back to original dimensions\n",
    "        self.cnn_proj = nn.Linear(hidden_dim, cnn_dim)\n",
    "        self.mlp_proj = nn.Linear(hidden_dim, mlp_dim)\n",
    "        \n",
    "    def forward(self, cnn_out, mlp_out):\n",
    "        # cnn_out: (batch, cnn_dim)\n",
    "        # mlp_out: (batch, mlp_dim)\n",
    "        batch_size = cnn_out.size(0)\n",
    "        \n",
    "        # CNN enhanced by MLP (multi-head attention)\n",
    "        cnn_qkv = self.cnn_to_qkv(cnn_out).reshape(batch_size, 3, self.num_heads, self.head_dim).permute(1, 0, 2, 3)\n",
    "        cnn_q, cnn_k, cnn_v = cnn_qkv[0], cnn_qkv[1], cnn_qkv[2]  # (batch, num_heads, head_dim)\n",
    "        \n",
    "        mlp_qkv = self.mlp_to_qkv(mlp_out).reshape(batch_size, 3, self.num_heads, self.head_dim).permute(1, 0, 2, 3)\n",
    "        mlp_q, mlp_k, mlp_v = mlp_qkv[0], mlp_qkv[1], mlp_qkv[2]\n",
    "        \n",
    "        # Cross-attention: CNN queries attend to MLP keys/values\n",
    "        scores = torch.matmul(cnn_q, mlp_k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        cnn_attended = torch.matmul(attn_weights, mlp_v)  # (batch, num_heads, head_dim)\n",
    "        cnn_attended = cnn_attended.transpose(1, 2).contiguous().view(batch_size, self.hidden_dim)\n",
    "        cnn_enhanced = cnn_out + self.cnn_proj(self.norm1(cnn_attended))\n",
    "        \n",
    "        # MLP enhanced by CNN (multi-head attention)\n",
    "        scores2 = torch.matmul(mlp_q, cnn_k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attn_weights2 = F.softmax(scores2, dim=-1)\n",
    "        mlp_attended = torch.matmul(attn_weights2, cnn_v)\n",
    "        mlp_attended = mlp_attended.transpose(1, 2).contiguous().view(batch_size, self.hidden_dim)\n",
    "        mlp_enhanced = mlp_out + self.mlp_proj(self.norm2(mlp_attended))\n",
    "        \n",
    "        return cnn_enhanced, mlp_enhanced\n",
    "\n",
    "\n",
    "# Define Hybrid CNN+MLP Model V4.3\n",
    "class HybridCNNMLP_V4_3(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Hybrid model: CNN for spectrograms + MLP for features\n",
    "    Version 4.3 Improvements:\n",
    "    - Multi-Head Cross-Attention Fusion\n",
    "    - Residual connections in MLP branch\n",
    "    - Enhanced SE blocks in CNN branch\n",
    "    - SpecAugment support\n",
    "    - Focal Loss support\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features=129, num_classes=2, dropout=0.4):\n",
    "        super(HybridCNNMLP_V4_3, self).__init__()\n",
    "        \n",
    "        # Multi-Scale CNN branch with enhanced attention\n",
    "        self.cnn_initial = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)  # (64, 64, 3)\n",
    "        )\n",
    "        \n",
    "        # Multi-scale block\n",
    "        self.multiscale = MultiScaleConvBlock(64, 128)\n",
    "        \n",
    "        self.cnn_branch = nn.Sequential(\n",
    "            ResidualBlock2D(128, 128),\n",
    "            EnhancedChannelAttention(128, reduction=8),\n",
    "            nn.MaxPool2d(2, 2),  # (128, 32, 1)\n",
    "            \n",
    "            ResidualBlock2D(128, 256),\n",
    "            EnhancedChannelAttention(256, reduction=8),\n",
    "            ResidualBlock2D(256, 512),\n",
    "            EnhancedChannelAttention(512, reduction=8),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # MLP branch with feature attention and residual connections\n",
    "        self.feature_attention = FeatureAttention(n_features, reduction=8)\n",
    "        \n",
    "        # First layer\n",
    "        self.mlp_layer1 = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Second layer with residual\n",
    "        self.mlp_layer2 = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.75)\n",
    "        )\n",
    "        self.mlp_residual1 = nn.Linear(256, 512)  # For residual connection\n",
    "        \n",
    "        # Third layer with residual\n",
    "        self.mlp_layer3 = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5)\n",
    "        )\n",
    "        self.mlp_residual2 = nn.Linear(512, 256)  # For residual connection\n",
    "        \n",
    "        # Final layer\n",
    "        self.mlp_layer4 = nn.Linear(256, 128)\n",
    "        \n",
    "        # Multi-head cross-attention fusion\n",
    "        self.cross_attention = MultiHeadCrossAttentionFusion(cnn_dim=512, mlp_dim=128, hidden_dim=256, num_heads=4)\n",
    "        \n",
    "        # Enhanced Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(512 + 128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            \n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.75),\n",
    "            \n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout * 0.5),\n",
    "            \n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        spectrogram, features = x\n",
    "        \n",
    "        # CNN branch with multi-scale\n",
    "        cnn_init = self.cnn_initial(spectrogram)\n",
    "        cnn_multiscale = self.multiscale(cnn_init)\n",
    "        cnn_out = self.cnn_branch(cnn_multiscale)  # (batch, 512)\n",
    "        \n",
    "        # MLP branch with feature attention and residual connections\n",
    "        features_attended = self.feature_attention(features)\n",
    "        \n",
    "        mlp = self.mlp_layer1(features_attended)  # (batch, 256)\n",
    "        mlp_input1 = mlp  # Save input for residual\n",
    "        mlp = self.mlp_layer2(mlp) + self.mlp_residual1(mlp_input1)  # (batch, 512) with residual\n",
    "        mlp_input2 = mlp  # Save input for residual\n",
    "        mlp = self.mlp_layer3(mlp) + self.mlp_residual2(mlp_input2)  # (batch, 256) with residual\n",
    "        mlp_out = self.mlp_layer4(mlp)  # (batch, 128)\n",
    "        \n",
    "        # Multi-head cross-attention fusion\n",
    "        cnn_enhanced, mlp_enhanced = self.cross_attention(cnn_out, mlp_out)\n",
    "        \n",
    "        # Concatenate enhanced outputs\n",
    "        fused = torch.cat([cnn_enhanced, mlp_enhanced], dim=1)  # (batch, 640)\n",
    "        \n",
    "        # Final classification\n",
    "        out = self.fusion(fused)  # (batch, 2)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Return model configuration\"\"\"\n",
    "        return {\n",
    "            'model_type': 'HybridCNNMLP_V4_3',\n",
    "            'num_classes': 2,\n",
    "            'n_features': 129,\n",
    "            'input_shapes': {\n",
    "                'spectrogram': (1, 128, 7),\n",
    "                'features': (129,)\n",
    "            },\n",
    "            'version': '4.3'\n",
    "        }\n",
    "\n",
    "print(\"Model architecture V4.3 (Enhanced) defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FocalLoss class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Focal Loss for addressing class imbalance and hard examples.\n",
    "    FL(p_t) = -alpha * (1 - p_t)^gamma * log(p_t)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight  # Class weights\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: (N, C) logits\n",
    "            target: (N,) class indices\n",
    "        Returns:\n",
    "            Focal loss value\n",
    "        \"\"\"\n",
    "        log_prob = F.log_softmax(pred, dim=1)\n",
    "        prob = torch.exp(log_prob)\n",
    "        \n",
    "        # Get probability of true class\n",
    "        prob_t = prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute focal weight: (1 - p_t)^gamma\n",
    "        focal_weight = (1 - prob_t) ** self.gamma\n",
    "        \n",
    "        # Compute cross entropy\n",
    "        ce_loss = -log_prob.gather(1, target.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Apply class weights if provided\n",
    "        if self.weight is not None:\n",
    "            class_weights = self.weight[target]\n",
    "            ce_loss = ce_loss * class_weights\n",
    "        \n",
    "        # Apply alpha weighting\n",
    "        alpha_t = self.alpha if self.alpha is not None else 1.0\n",
    "        if isinstance(alpha_t, (float, int)):\n",
    "            alpha_t = torch.tensor(alpha_t, device=pred.device)\n",
    "        if isinstance(alpha_t, torch.Tensor) and len(alpha_t.shape) == 0:\n",
    "            # Scalar alpha - apply uniformly\n",
    "            focal_loss = alpha_t * focal_weight * ce_loss\n",
    "        else:\n",
    "            # Per-class alpha\n",
    "            alpha_t = alpha_t[target]\n",
    "            focal_loss = alpha_t * focal_weight * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "print(\"FocalLoss class defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training utilities defined successfully!\n"
     ]
    }
   ],
   "source": [
    "# Training utilities\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, max_grad_norm=None):\n",
    "    \"\"\"Train for one epoch with optional gradient clipping\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        if isinstance(batch[0], (tuple, list)) and len(batch[0]) == 2:\n",
    "            inputs = tuple(x.to(device) for x in batch[0])\n",
    "        else:\n",
    "            inputs = batch[0].to(device)\n",
    "        \n",
    "        labels = batch[1].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        if max_grad_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\", leave=False):\n",
    "            if isinstance(batch[0], (tuple, list)) and len(batch[0]) == 2:\n",
    "                inputs = tuple(x.to(device) for x in batch[0])\n",
    "            else:\n",
    "                inputs = batch[0].to(device)\n",
    "            \n",
    "            labels = batch[1].to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        roc_auc = roc_auc_score(all_labels, np.array(all_probs)[:, 1])\n",
    "    except:\n",
    "        roc_auc = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "    \n",
    "    return metrics, all_preds, all_labels, all_probs\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler,\n",
    "                device, num_epochs, save_dir, model_name, early_stopping_patience=20, max_grad_norm=None):\n",
    "    \"\"\"Train model with early stopping and optional gradient clipping\"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    training_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device, max_grad_norm)\n",
    "        val_metrics, _, _, _ = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': train_loss,\n",
    "            'train_accuracy': train_acc,\n",
    "            'val_loss': val_metrics['loss'],\n",
    "            'val_accuracy': val_metrics['accuracy'],\n",
    "            'val_precision': val_metrics['precision'],\n",
    "            'val_recall': val_metrics['recall'],\n",
    "            'val_f1': val_metrics['f1'],\n",
    "            'val_roc_auc': val_metrics['roc_auc'],\n",
    "            'learning_rate': current_lr\n",
    "        }\n",
    "        training_history.append(epoch_metrics)\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_metrics['loss']:.4f}, Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Val F1: {val_metrics['f1']:.4f}, Val ROC-AUC: {val_metrics['roc_auc']:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "        \n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_f1': best_val_f1,\n",
    "                'val_metrics': val_metrics\n",
    "            }, save_dir / 'best_model.pt')\n",
    "            \n",
    "            print(f\"✓ New best model saved! (F1: {best_val_f1:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                print(f\"Best F1: {best_val_f1:.4f} at epoch {best_epoch}\")\n",
    "                break\n",
    "    \n",
    "    with open(save_dir / 'training_history.json', 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "    \n",
    "    config = model.get_config() if hasattr(model, 'get_config') else {}\n",
    "    config.update({\n",
    "        'best_epoch': best_epoch,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'num_epochs': num_epochs\n",
    "    })\n",
    "    with open(save_dir / 'config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=2)\n",
    "    \n",
    "    return training_history, best_epoch\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    metrics, preds, labels, probs = validate(model, test_loader, criterion, device)\n",
    "    \n",
    "    precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
    "    recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
    "    f1_per_class = f1_score(labels, preds, average=None, zero_division=0)\n",
    "    \n",
    "    metrics['precision_d'] = float(precision_per_class[0])\n",
    "    metrics['precision_t'] = float(precision_per_class[1])\n",
    "    metrics['recall_d'] = float(recall_per_class[0])\n",
    "    metrics['recall_t'] = float(recall_per_class[1])\n",
    "    metrics['f1_d'] = float(f1_per_class[0])\n",
    "    metrics['f1_t'] = float(f1_per_class[1])\n",
    "    metrics['confusion_matrix'] = confusion_matrix(labels, preds).tolist()\n",
    "    \n",
    "    return metrics, preds, labels, probs\n",
    "\n",
    "\n",
    "class WarmupCosineScheduler:\n",
    "    \"\"\"Learning rate scheduler with warmup and cosine annealing\"\"\"\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.total_epochs = total_epochs\n",
    "        self.min_lr = min_lr\n",
    "        self.base_lr = optimizer.param_groups[0]['lr']\n",
    "        self.current_epoch = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.current_epoch += 1\n",
    "        \n",
    "        if self.current_epoch <= self.warmup_epochs:\n",
    "            lr = self.base_lr * (self.current_epoch / self.warmup_epochs)\n",
    "        else:\n",
    "            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
    "            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    \n",
    "    def get_last_lr(self):\n",
    "        return [self.optimizer.param_groups[0]['lr']]\n",
    "\n",
    "print(\"Training utilities defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model and Training Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: HybridCNNMLP_V4_3\n",
      "Total parameters: 6,579,554\n",
      "Trainable parameters: 6,579,554\n",
      "Number of features: 129\n",
      "\n",
      "Training configuration:\n",
      "- Epochs: 200\n",
      "- Warmup epochs: 5\n",
      "- Initial LR: 0.0005\n",
      "- Loss function: Focal Loss (alpha=0.25, gamma=2.0)\n",
      "- Gradient clipping: 1.0\n",
      "- Early stopping patience: 20\n",
      "- Dropout: 0.4\n",
      "- SpecAugment: Enabled for training\n",
      "- Context windows: ±100ms (V2 with VOT and burst features)\n",
      "- Save directory: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/improved_models/hybrid_cnn_mlp_v4_3_enhanced\n"
     ]
    }
   ],
   "source": [
    "# Create model V4.3 with automatic feature count detection\n",
    "model = HybridCNNMLP_V4_3(n_features=len(feature_cols), num_classes=2, dropout=0.4).to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: {model.get_config()['model_type']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "\n",
    "# Prepare class weights for loss function\n",
    "class_weights = torch.tensor([\n",
    "    class_weights_dict.get('0', class_weights_dict.get(0, 1.0)), \n",
    "    class_weights_dict.get('1', class_weights_dict.get(1, 1.0))\n",
    "], dtype=torch.float32).to(device)\n",
    "\n",
    "# Loss function: Focal Loss with class weights\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0, weight=class_weights, reduction='mean')\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler with warmup and cosine annealing\n",
    "num_epochs = 200\n",
    "warmup_epochs = 5\n",
    "scheduler = WarmupCosineScheduler(optimizer, warmup_epochs=warmup_epochs, total_epochs=num_epochs, min_lr=1e-6)\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = DATA_DIR / 'improved_models'\n",
    "save_dir = OUTPUT_DIR / 'hybrid_cnn_mlp_v4_3_enhanced'\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"\\nTraining configuration:\")\n",
    "print(f\"- Epochs: {num_epochs}\")\n",
    "print(f\"- Warmup epochs: {warmup_epochs}\")\n",
    "print(f\"- Initial LR: {optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"- Loss function: Focal Loss (alpha=0.25, gamma=2.0)\")\n",
    "print(f\"- Gradient clipping: 1.0\")\n",
    "print(f\"- Early stopping patience: 20\")\n",
    "print(f\"- Dropout: 0.4\")\n",
    "print(f\"- SpecAugment: Enabled for training\")\n",
    "print(f\"- Context windows: ±100ms (V2 with VOT and burst features)\")\n",
    "print(f\"- Save directory: {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0190, Train Acc: 0.8804\n",
      "Val Loss: 0.0148, Val Acc: 0.9100\n",
      "Val F1: 0.9104, Val ROC-AUC: 0.9746\n",
      "Learning Rate: 0.000100\n",
      "✓ New best model saved! (F1: 0.9104)\n",
      "\n",
      "Epoch 2/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0143, Train Acc: 0.9126\n",
      "Val Loss: 0.0121, Val Acc: 0.9238\n",
      "Val F1: 0.9240, Val ROC-AUC: 0.9807\n",
      "Learning Rate: 0.000200\n",
      "✓ New best model saved! (F1: 0.9240)\n",
      "\n",
      "Epoch 3/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0137, Train Acc: 0.9172\n",
      "Val Loss: 0.0121, Val Acc: 0.9261\n",
      "Val F1: 0.9263, Val ROC-AUC: 0.9811\n",
      "Learning Rate: 0.000300\n",
      "✓ New best model saved! (F1: 0.9263)\n",
      "\n",
      "Epoch 4/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0134, Train Acc: 0.9189\n",
      "Val Loss: 0.0123, Val Acc: 0.9206\n",
      "Val F1: 0.9209, Val ROC-AUC: 0.9819\n",
      "Learning Rate: 0.000400\n",
      "\n",
      "Epoch 5/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0128, Train Acc: 0.9243\n",
      "Val Loss: 0.0113, Val Acc: 0.9334\n",
      "Val F1: 0.9334, Val ROC-AUC: 0.9832\n",
      "Learning Rate: 0.000500\n",
      "✓ New best model saved! (F1: 0.9334)\n",
      "\n",
      "Epoch 6/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0127, Train Acc: 0.9249\n",
      "Val Loss: 0.0121, Val Acc: 0.9322\n",
      "Val F1: 0.9323, Val ROC-AUC: 0.9835\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 7/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0120, Train Acc: 0.9288\n",
      "Val Loss: 0.0114, Val Acc: 0.9299\n",
      "Val F1: 0.9301, Val ROC-AUC: 0.9839\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 8/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0118, Train Acc: 0.9320\n",
      "Val Loss: 0.0108, Val Acc: 0.9325\n",
      "Val F1: 0.9326, Val ROC-AUC: 0.9844\n",
      "Learning Rate: 0.000500\n",
      "\n",
      "Epoch 9/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0115, Train Acc: 0.9324\n",
      "Val Loss: 0.0108, Val Acc: 0.9334\n",
      "Val F1: 0.9335, Val ROC-AUC: 0.9845\n",
      "Learning Rate: 0.000499\n",
      "✓ New best model saved! (F1: 0.9335)\n",
      "\n",
      "Epoch 10/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0114, Train Acc: 0.9333\n",
      "Val Loss: 0.0106, Val Acc: 0.9358\n",
      "Val F1: 0.9360, Val ROC-AUC: 0.9846\n",
      "Learning Rate: 0.000499\n",
      "✓ New best model saved! (F1: 0.9360)\n",
      "\n",
      "Epoch 11/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0112, Train Acc: 0.9342\n",
      "Val Loss: 0.0105, Val Acc: 0.9375\n",
      "Val F1: 0.9376, Val ROC-AUC: 0.9853\n",
      "Learning Rate: 0.000499\n",
      "✓ New best model saved! (F1: 0.9376)\n",
      "\n",
      "Epoch 12/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0111, Train Acc: 0.9355\n",
      "Val Loss: 0.0112, Val Acc: 0.9345\n",
      "Val F1: 0.9347, Val ROC-AUC: 0.9846\n",
      "Learning Rate: 0.000498\n",
      "\n",
      "Epoch 13/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0110, Train Acc: 0.9362\n",
      "Val Loss: 0.0106, Val Acc: 0.9382\n",
      "Val F1: 0.9383, Val ROC-AUC: 0.9851\n",
      "Learning Rate: 0.000498\n",
      "✓ New best model saved! (F1: 0.9383)\n",
      "\n",
      "Epoch 14/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0110, Train Acc: 0.9350\n",
      "Val Loss: 0.0104, Val Acc: 0.9402\n",
      "Val F1: 0.9403, Val ROC-AUC: 0.9856\n",
      "Learning Rate: 0.000497\n",
      "✓ New best model saved! (F1: 0.9403)\n",
      "\n",
      "Epoch 15/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0108, Train Acc: 0.9378\n",
      "Val Loss: 0.0105, Val Acc: 0.9361\n",
      "Val F1: 0.9362, Val ROC-AUC: 0.9852\n",
      "Learning Rate: 0.000497\n",
      "\n",
      "Epoch 16/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0109, Train Acc: 0.9367\n",
      "Val Loss: 0.0106, Val Acc: 0.9365\n",
      "Val F1: 0.9366, Val ROC-AUC: 0.9848\n",
      "Learning Rate: 0.000496\n",
      "\n",
      "Epoch 17/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0109, Train Acc: 0.9368\n",
      "Val Loss: 0.0105, Val Acc: 0.9342\n",
      "Val F1: 0.9344, Val ROC-AUC: 0.9857\n",
      "Learning Rate: 0.000495\n",
      "\n",
      "Epoch 18/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0106, Train Acc: 0.9373\n",
      "Val Loss: 0.0103, Val Acc: 0.9369\n",
      "Val F1: 0.9371, Val ROC-AUC: 0.9861\n",
      "Learning Rate: 0.000495\n",
      "\n",
      "Epoch 19/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0106, Train Acc: 0.9392\n",
      "Val Loss: 0.0102, Val Acc: 0.9394\n",
      "Val F1: 0.9395, Val ROC-AUC: 0.9858\n",
      "Learning Rate: 0.000494\n",
      "\n",
      "Epoch 20/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0104, Train Acc: 0.9405\n",
      "Val Loss: 0.0102, Val Acc: 0.9399\n",
      "Val F1: 0.9399, Val ROC-AUC: 0.9857\n",
      "Learning Rate: 0.000493\n",
      "\n",
      "Epoch 21/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0104, Train Acc: 0.9391\n",
      "Val Loss: 0.0104, Val Acc: 0.9358\n",
      "Val F1: 0.9360, Val ROC-AUC: 0.9863\n",
      "Learning Rate: 0.000492\n",
      "\n",
      "Epoch 22/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0104, Train Acc: 0.9400\n",
      "Val Loss: 0.0103, Val Acc: 0.9371\n",
      "Val F1: 0.9373, Val ROC-AUC: 0.9860\n",
      "Learning Rate: 0.000491\n",
      "\n",
      "Epoch 23/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0103, Train Acc: 0.9402\n",
      "Val Loss: 0.0101, Val Acc: 0.9371\n",
      "Val F1: 0.9373, Val ROC-AUC: 0.9864\n",
      "Learning Rate: 0.000490\n",
      "\n",
      "Epoch 24/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0104, Train Acc: 0.9402\n",
      "Val Loss: 0.0104, Val Acc: 0.9350\n",
      "Val F1: 0.9352, Val ROC-AUC: 0.9858\n",
      "Learning Rate: 0.000488\n",
      "\n",
      "Epoch 25/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0103, Train Acc: 0.9402\n",
      "Val Loss: 0.0102, Val Acc: 0.9408\n",
      "Val F1: 0.9408, Val ROC-AUC: 0.9863\n",
      "Learning Rate: 0.000487\n",
      "✓ New best model saved! (F1: 0.9408)\n",
      "\n",
      "Epoch 26/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0102, Train Acc: 0.9410\n",
      "Val Loss: 0.0102, Val Acc: 0.9396\n",
      "Val F1: 0.9397, Val ROC-AUC: 0.9860\n",
      "Learning Rate: 0.000486\n",
      "\n",
      "Epoch 27/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0102, Train Acc: 0.9411\n",
      "Val Loss: 0.0106, Val Acc: 0.9372\n",
      "Val F1: 0.9373, Val ROC-AUC: 0.9862\n",
      "Learning Rate: 0.000484\n",
      "\n",
      "Epoch 28/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0103, Train Acc: 0.9398\n",
      "Val Loss: 0.0104, Val Acc: 0.9372\n",
      "Val F1: 0.9373, Val ROC-AUC: 0.9856\n",
      "Learning Rate: 0.000483\n",
      "\n",
      "Epoch 29/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0100, Train Acc: 0.9414\n",
      "Val Loss: 0.0102, Val Acc: 0.9390\n",
      "Val F1: 0.9391, Val ROC-AUC: 0.9861\n",
      "Learning Rate: 0.000482\n",
      "\n",
      "Epoch 30/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9416\n",
      "Val Loss: 0.0101, Val Acc: 0.9393\n",
      "Val F1: 0.9394, Val ROC-AUC: 0.9864\n",
      "Learning Rate: 0.000480\n",
      "\n",
      "Epoch 31/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0102, Train Acc: 0.9408\n",
      "Val Loss: 0.0104, Val Acc: 0.9347\n",
      "Val F1: 0.9349, Val ROC-AUC: 0.9858\n",
      "Learning Rate: 0.000478\n",
      "\n",
      "Epoch 32/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9417\n",
      "Val Loss: 0.0101, Val Acc: 0.9416\n",
      "Val F1: 0.9417, Val ROC-AUC: 0.9863\n",
      "Learning Rate: 0.000477\n",
      "✓ New best model saved! (F1: 0.9417)\n",
      "\n",
      "Epoch 33/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9413\n",
      "Val Loss: 0.0098, Val Acc: 0.9395\n",
      "Val F1: 0.9397, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000475\n",
      "\n",
      "Epoch 34/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9414\n",
      "Val Loss: 0.0101, Val Acc: 0.9383\n",
      "Val F1: 0.9385, Val ROC-AUC: 0.9865\n",
      "Learning Rate: 0.000473\n",
      "\n",
      "Epoch 35/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9413\n",
      "Val Loss: 0.0099, Val Acc: 0.9388\n",
      "Val F1: 0.9390, Val ROC-AUC: 0.9866\n",
      "Learning Rate: 0.000471\n",
      "\n",
      "Epoch 36/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9413\n",
      "Val Loss: 0.0100, Val Acc: 0.9399\n",
      "Val F1: 0.9400, Val ROC-AUC: 0.9869\n",
      "Learning Rate: 0.000470\n",
      "\n",
      "Epoch 37/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0099, Train Acc: 0.9422\n",
      "Val Loss: 0.0100, Val Acc: 0.9388\n",
      "Val F1: 0.9389, Val ROC-AUC: 0.9866\n",
      "Learning Rate: 0.000468\n",
      "\n",
      "Epoch 38/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0098, Train Acc: 0.9431\n",
      "Val Loss: 0.0102, Val Acc: 0.9389\n",
      "Val F1: 0.9390, Val ROC-AUC: 0.9863\n",
      "Learning Rate: 0.000466\n",
      "\n",
      "Epoch 39/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0100, Train Acc: 0.9435\n",
      "Val Loss: 0.0105, Val Acc: 0.9380\n",
      "Val F1: 0.9382, Val ROC-AUC: 0.9859\n",
      "Learning Rate: 0.000463\n",
      "\n",
      "Epoch 40/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0099, Train Acc: 0.9426\n",
      "Val Loss: 0.0109, Val Acc: 0.9230\n",
      "Val F1: 0.9233, Val ROC-AUC: 0.9861\n",
      "Learning Rate: 0.000461\n",
      "\n",
      "Epoch 41/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0100, Train Acc: 0.9423\n",
      "Val Loss: 0.0102, Val Acc: 0.9381\n",
      "Val F1: 0.9383, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000459\n",
      "\n",
      "Epoch 42/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9425\n",
      "Val Loss: 0.0099, Val Acc: 0.9416\n",
      "Val F1: 0.9416, Val ROC-AUC: 0.9867\n",
      "Learning Rate: 0.000457\n",
      "\n",
      "Epoch 43/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0097, Train Acc: 0.9449\n",
      "Val Loss: 0.0100, Val Acc: 0.9383\n",
      "Val F1: 0.9385, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000455\n",
      "\n",
      "Epoch 44/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0101, Train Acc: 0.9423\n",
      "Val Loss: 0.0100, Val Acc: 0.9422\n",
      "Val F1: 0.9423, Val ROC-AUC: 0.9866\n",
      "Learning Rate: 0.000452\n",
      "✓ New best model saved! (F1: 0.9423)\n",
      "\n",
      "Epoch 45/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0099, Train Acc: 0.9440\n",
      "Val Loss: 0.0098, Val Acc: 0.9430\n",
      "Val F1: 0.9430, Val ROC-AUC: 0.9870\n",
      "Learning Rate: 0.000450\n",
      "✓ New best model saved! (F1: 0.9430)\n",
      "\n",
      "Epoch 46/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0097, Train Acc: 0.9441\n",
      "Val Loss: 0.0102, Val Acc: 0.9398\n",
      "Val F1: 0.9399, Val ROC-AUC: 0.9860\n",
      "Learning Rate: 0.000448\n",
      "\n",
      "Epoch 47/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0096, Train Acc: 0.9443\n",
      "Val Loss: 0.0101, Val Acc: 0.9397\n",
      "Val F1: 0.9398, Val ROC-AUC: 0.9863\n",
      "Learning Rate: 0.000445\n",
      "\n",
      "Epoch 48/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0097, Train Acc: 0.9440\n",
      "Val Loss: 0.0099, Val Acc: 0.9409\n",
      "Val F1: 0.9410, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000442\n",
      "\n",
      "Epoch 49/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0097, Train Acc: 0.9442\n",
      "Val Loss: 0.0099, Val Acc: 0.9404\n",
      "Val F1: 0.9405, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000440\n",
      "\n",
      "Epoch 50/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0097, Train Acc: 0.9437\n",
      "Val Loss: 0.0102, Val Acc: 0.9397\n",
      "Val F1: 0.9399, Val ROC-AUC: 0.9865\n",
      "Learning Rate: 0.000437\n",
      "\n",
      "Epoch 51/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0096, Train Acc: 0.9442\n",
      "Val Loss: 0.0101, Val Acc: 0.9395\n",
      "Val F1: 0.9396, Val ROC-AUC: 0.9867\n",
      "Learning Rate: 0.000435\n",
      "\n",
      "Epoch 52/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0096, Train Acc: 0.9442\n",
      "Val Loss: 0.0100, Val Acc: 0.9413\n",
      "Val F1: 0.9414, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000432\n",
      "\n",
      "Epoch 53/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0095, Train Acc: 0.9459\n",
      "Val Loss: 0.0099, Val Acc: 0.9396\n",
      "Val F1: 0.9398, Val ROC-AUC: 0.9870\n",
      "Learning Rate: 0.000429\n",
      "\n",
      "Epoch 54/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0096, Train Acc: 0.9451\n",
      "Val Loss: 0.0102, Val Acc: 0.9344\n",
      "Val F1: 0.9346, Val ROC-AUC: 0.9869\n",
      "Learning Rate: 0.000426\n",
      "\n",
      "Epoch 55/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0095, Train Acc: 0.9457\n",
      "Val Loss: 0.0099, Val Acc: 0.9386\n",
      "Val F1: 0.9387, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000423\n",
      "\n",
      "Epoch 56/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0095, Train Acc: 0.9446\n",
      "Val Loss: 0.0098, Val Acc: 0.9420\n",
      "Val F1: 0.9421, Val ROC-AUC: 0.9869\n",
      "Learning Rate: 0.000420\n",
      "\n",
      "Epoch 57/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0097, Train Acc: 0.9434\n",
      "Val Loss: 0.0103, Val Acc: 0.9366\n",
      "Val F1: 0.9368, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000417\n",
      "\n",
      "Epoch 58/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0095, Train Acc: 0.9459\n",
      "Val Loss: 0.0100, Val Acc: 0.9400\n",
      "Val F1: 0.9401, Val ROC-AUC: 0.9869\n",
      "Learning Rate: 0.000414\n",
      "\n",
      "Epoch 59/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0092, Train Acc: 0.9470\n",
      "Val Loss: 0.0100, Val Acc: 0.9379\n",
      "Val F1: 0.9381, Val ROC-AUC: 0.9870\n",
      "Learning Rate: 0.000411\n",
      "\n",
      "Epoch 60/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0095, Train Acc: 0.9452\n",
      "Val Loss: 0.0107, Val Acc: 0.9324\n",
      "Val F1: 0.9327, Val ROC-AUC: 0.9866\n",
      "Learning Rate: 0.000408\n",
      "\n",
      "Epoch 61/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0097, Train Acc: 0.9442\n",
      "Val Loss: 0.0099, Val Acc: 0.9430\n",
      "Val F1: 0.9431, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000405\n",
      "✓ New best model saved! (F1: 0.9431)\n",
      "\n",
      "Epoch 62/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0095, Train Acc: 0.9445\n",
      "Val Loss: 0.0100, Val Acc: 0.9367\n",
      "Val F1: 0.9369, Val ROC-AUC: 0.9872\n",
      "Learning Rate: 0.000402\n",
      "\n",
      "Epoch 63/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0092, Train Acc: 0.9468\n",
      "Val Loss: 0.0102, Val Acc: 0.9391\n",
      "Val F1: 0.9392, Val ROC-AUC: 0.9868\n",
      "Learning Rate: 0.000399\n",
      "\n",
      "Epoch 64/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0095, Train Acc: 0.9452\n",
      "Val Loss: 0.0105, Val Acc: 0.9364\n",
      "Val F1: 0.9366, Val ROC-AUC: 0.9870\n",
      "Learning Rate: 0.000396\n",
      "\n",
      "Epoch 65/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0094, Train Acc: 0.9457\n",
      "Val Loss: 0.0104, Val Acc: 0.9394\n",
      "Val F1: 0.9395, Val ROC-AUC: 0.9865\n",
      "Learning Rate: 0.000392\n",
      "\n",
      "Epoch 66/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0094, Train Acc: 0.9457\n",
      "Val Loss: 0.0101, Val Acc: 0.9411\n",
      "Val F1: 0.9412, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000389\n",
      "\n",
      "Epoch 67/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0093, Train Acc: 0.9460\n",
      "Val Loss: 0.0099, Val Acc: 0.9424\n",
      "Val F1: 0.9425, Val ROC-AUC: 0.9874\n",
      "Learning Rate: 0.000386\n",
      "\n",
      "Epoch 68/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0094, Train Acc: 0.9462\n",
      "Val Loss: 0.0102, Val Acc: 0.9390\n",
      "Val F1: 0.9391, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000382\n",
      "\n",
      "Epoch 69/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0094, Train Acc: 0.9469\n",
      "Val Loss: 0.0099, Val Acc: 0.9415\n",
      "Val F1: 0.9416, Val ROC-AUC: 0.9873\n",
      "Learning Rate: 0.000379\n",
      "\n",
      "Epoch 70/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0092, Train Acc: 0.9471\n",
      "Val Loss: 0.0098, Val Acc: 0.9411\n",
      "Val F1: 0.9413, Val ROC-AUC: 0.9875\n",
      "Learning Rate: 0.000375\n",
      "\n",
      "Epoch 71/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0092, Train Acc: 0.9466\n",
      "Val Loss: 0.0099, Val Acc: 0.9432\n",
      "Val F1: 0.9433, Val ROC-AUC: 0.9872\n",
      "Learning Rate: 0.000372\n",
      "✓ New best model saved! (F1: 0.9433)\n",
      "\n",
      "Epoch 72/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0091, Train Acc: 0.9485\n",
      "Val Loss: 0.0103, Val Acc: 0.9347\n",
      "Val F1: 0.9349, Val ROC-AUC: 0.9866\n",
      "Learning Rate: 0.000368\n",
      "\n",
      "Epoch 73/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0090, Train Acc: 0.9476\n",
      "Val Loss: 0.0097, Val Acc: 0.9454\n",
      "Val F1: 0.9455, Val ROC-AUC: 0.9876\n",
      "Learning Rate: 0.000365\n",
      "✓ New best model saved! (F1: 0.9455)\n",
      "\n",
      "Epoch 74/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0089, Train Acc: 0.9490\n",
      "Val Loss: 0.0100, Val Acc: 0.9436\n",
      "Val F1: 0.9437, Val ROC-AUC: 0.9872\n",
      "Learning Rate: 0.000361\n",
      "\n",
      "Epoch 75/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0093, Train Acc: 0.9469\n",
      "Val Loss: 0.0098, Val Acc: 0.9409\n",
      "Val F1: 0.9410, Val ROC-AUC: 0.9873\n",
      "Learning Rate: 0.000357\n",
      "\n",
      "Epoch 76/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0092, Train Acc: 0.9466\n",
      "Val Loss: 0.0100, Val Acc: 0.9382\n",
      "Val F1: 0.9383, Val ROC-AUC: 0.9871\n",
      "Learning Rate: 0.000354\n",
      "\n",
      "Epoch 77/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0089, Train Acc: 0.9485\n",
      "Val Loss: 0.0096, Val Acc: 0.9434\n",
      "Val F1: 0.9435, Val ROC-AUC: 0.9877\n",
      "Learning Rate: 0.000350\n",
      "\n",
      "Epoch 78/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0088, Train Acc: 0.9494\n",
      "Val Loss: 0.0101, Val Acc: 0.9445\n",
      "Val F1: 0.9445, Val ROC-AUC: 0.9874\n",
      "Learning Rate: 0.000346\n",
      "\n",
      "Epoch 79/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0090, Train Acc: 0.9490\n",
      "Val Loss: 0.0097, Val Acc: 0.9429\n",
      "Val F1: 0.9430, Val ROC-AUC: 0.9875\n",
      "Learning Rate: 0.000343\n",
      "\n",
      "Epoch 80/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0090, Train Acc: 0.9478\n",
      "Val Loss: 0.0098, Val Acc: 0.9432\n",
      "Val F1: 0.9432, Val ROC-AUC: 0.9874\n",
      "Learning Rate: 0.000339\n",
      "\n",
      "Epoch 81/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0090, Train Acc: 0.9480\n",
      "Val Loss: 0.0097, Val Acc: 0.9442\n",
      "Val F1: 0.9442, Val ROC-AUC: 0.9875\n",
      "Learning Rate: 0.000335\n",
      "\n",
      "Epoch 82/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0087, Train Acc: 0.9503\n",
      "Val Loss: 0.0100, Val Acc: 0.9400\n",
      "Val F1: 0.9401, Val ROC-AUC: 0.9875\n",
      "Learning Rate: 0.000331\n",
      "\n",
      "Epoch 83/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0089, Train Acc: 0.9483\n",
      "Val Loss: 0.0098, Val Acc: 0.9391\n",
      "Val F1: 0.9393, Val ROC-AUC: 0.9873\n",
      "Learning Rate: 0.000328\n",
      "\n",
      "Epoch 84/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0088, Train Acc: 0.9497\n",
      "Val Loss: 0.0097, Val Acc: 0.9439\n",
      "Val F1: 0.9439, Val ROC-AUC: 0.9873\n",
      "Learning Rate: 0.000324\n",
      "\n",
      "Epoch 85/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0088, Train Acc: 0.9496\n",
      "Val Loss: 0.0097, Val Acc: 0.9422\n",
      "Val F1: 0.9423, Val ROC-AUC: 0.9874\n",
      "Learning Rate: 0.000320\n",
      "\n",
      "Epoch 86/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0089, Train Acc: 0.9493\n",
      "Val Loss: 0.0099, Val Acc: 0.9398\n",
      "Val F1: 0.9400, Val ROC-AUC: 0.9873\n",
      "Learning Rate: 0.000316\n",
      "\n",
      "Epoch 87/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0087, Train Acc: 0.9504\n",
      "Val Loss: 0.0097, Val Acc: 0.9446\n",
      "Val F1: 0.9447, Val ROC-AUC: 0.9876\n",
      "Learning Rate: 0.000312\n",
      "\n",
      "Epoch 88/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0087, Train Acc: 0.9498\n",
      "Val Loss: 0.0096, Val Acc: 0.9448\n",
      "Val F1: 0.9448, Val ROC-AUC: 0.9875\n",
      "Learning Rate: 0.000308\n",
      "\n",
      "Epoch 89/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0087, Train Acc: 0.9506\n",
      "Val Loss: 0.0099, Val Acc: 0.9428\n",
      "Val F1: 0.9429, Val ROC-AUC: 0.9872\n",
      "Learning Rate: 0.000304\n",
      "\n",
      "Epoch 90/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0085, Train Acc: 0.9509\n",
      "Val Loss: 0.0098, Val Acc: 0.9421\n",
      "Val F1: 0.9422, Val ROC-AUC: 0.9874\n",
      "Learning Rate: 0.000300\n",
      "\n",
      "Epoch 91/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0087, Train Acc: 0.9504\n",
      "Val Loss: 0.0096, Val Acc: 0.9433\n",
      "Val F1: 0.9434, Val ROC-AUC: 0.9878\n",
      "Learning Rate: 0.000296\n",
      "\n",
      "Epoch 92/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0085, Train Acc: 0.9508\n",
      "Val Loss: 0.0096, Val Acc: 0.9452\n",
      "Val F1: 0.9453, Val ROC-AUC: 0.9880\n",
      "Learning Rate: 0.000293\n",
      "\n",
      "Epoch 93/200\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0086, Train Acc: 0.9508\n",
      "Val Loss: 0.0095, Val Acc: 0.9448\n",
      "Val F1: 0.9448, Val ROC-AUC: 0.9877\n",
      "Learning Rate: 0.000289\n",
      "\n",
      "Early stopping at epoch 93\n",
      "Best F1: 0.9455 at epoch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final Test Results:\n",
      "============================================================\n",
      "Accuracy: 0.9442\n",
      "F1-score: 0.9443\n",
      "ROC-AUC: 0.9872\n",
      "Precision: 0.9445\n",
      "Recall: 0.9442\n",
      "Best epoch: 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "history, best_epoch = train_model(\n",
    "    model, train_hybrid_loader, val_hybrid_loader, criterion, optimizer, scheduler,\n",
    "    device, num_epochs=num_epochs, save_dir=save_dir, model_name='hybrid_cnn_mlp_v4_3_enhanced', \n",
    "    early_stopping_patience=20, max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "# Load best model and evaluate on test set\n",
    "checkpoint = torch.load(save_dir / 'best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "test_metrics, test_preds, test_labels, test_probs = evaluate_model(model, test_hybrid_loader, criterion, device)\n",
    "\n",
    "# Save test metrics\n",
    "with open(save_dir / 'test_metrics.json', 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Final Test Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1-score: {test_metrics['f1']:.4f}\")\n",
    "print(f\"ROC-AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"Best epoch: {best_epoch}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Predictions with Probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions with probabilities to: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/improved_models/hybrid_cnn_mlp_v4_3_enhanced/test_predictions_with_probs.csv\n",
      "Total predictions: 19949\n",
      "Correct predictions: 18836\n",
      "Incorrect predictions: 1113\n",
      "\n",
      "Summary Statistics:\n",
      "- Average confidence (correct): 0.8499\n",
      "- Average confidence (incorrect): 0.6387\n",
      "- High confidence errors (>0.8): 115\n",
      "- Low confidence errors (<0.6): 498\n"
     ]
    }
   ],
   "source": [
    "# Get test dataset to extract phoneme metadata\n",
    "test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "# Create predictions dataframe with probabilities\n",
    "predictions_data = []\n",
    "for idx, row in test_df.iterrows():\n",
    "    predictions_data.append({\n",
    "        'phoneme_id': row['phoneme_id'],\n",
    "        'utterance_id': row.get('utterance_id', None),\n",
    "        'phoneme': row.get('phoneme', row.get('class', None)),\n",
    "        'true_class': row['class'],\n",
    "        'true_class_encoded': int(test_labels[idx]),\n",
    "        'predicted_class_encoded': int(test_preds[idx]),\n",
    "        'predicted_class': 'd' if test_preds[idx] == 0 else 't',\n",
    "        'prob_class_0': float(test_probs[idx][0]),\n",
    "        'prob_class_1': float(test_probs[idx][1]),\n",
    "        'max_prob': float(np.max(test_probs[idx])),\n",
    "        'is_correct': int(test_labels[idx] == test_preds[idx]),\n",
    "        'confidence': float(np.max(test_probs[idx])) if test_labels[idx] == test_preds[idx] else float(test_probs[idx][test_preds[idx]]),\n",
    "        'duration_ms': row.get('duration_ms', None)\n",
    "    })\n",
    "\n",
    "predictions_df = pd.DataFrame(predictions_data)\n",
    "\n",
    "# Save to CSV\n",
    "predictions_df.to_csv(save_dir / 'test_predictions_with_probs.csv', index=False)\n",
    "print(f\"Saved predictions with probabilities to: {save_dir / 'test_predictions_with_probs.csv'}\")\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(f\"Correct predictions: {predictions_df['is_correct'].sum()}\")\n",
    "print(f\"Incorrect predictions: {(~predictions_df['is_correct'].astype(bool)).sum()}\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'total_samples': len(predictions_df),\n",
    "    'correct_predictions': int(predictions_df['is_correct'].sum()),\n",
    "    'incorrect_predictions': int((~predictions_df['is_correct'].astype(bool)).sum()),\n",
    "    'accuracy': float(predictions_df['is_correct'].mean()),\n",
    "    'avg_confidence_correct': float(predictions_df[predictions_df['is_correct'] == 1]['confidence'].mean()),\n",
    "    'avg_confidence_incorrect': float(predictions_df[predictions_df['is_correct'] == 0]['confidence'].mean()),\n",
    "    'min_confidence_incorrect': float(predictions_df[predictions_df['is_correct'] == 0]['confidence'].min()),\n",
    "    'max_confidence_incorrect': float(predictions_df[predictions_df['is_correct'] == 0]['confidence'].max()),\n",
    "    'high_confidence_errors': int(((predictions_df['is_correct'] == 0) & (predictions_df['confidence'] > 0.8)).sum()),\n",
    "    'low_confidence_errors': int(((predictions_df['is_correct'] == 0) & (predictions_df['confidence'] < 0.6)).sum()),\n",
    "}\n",
    "\n",
    "with open(save_dir / 'predictions_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"- Average confidence (correct): {summary_stats['avg_confidence_correct']:.4f}\")\n",
    "print(f\"- Average confidence (incorrect): {summary_stats['avg_confidence_incorrect']:.4f}\")\n",
    "print(f\"- High confidence errors (>0.8): {summary_stats['high_confidence_errors']}\")\n",
    "print(f\"- Low confidence errors (<0.6): {summary_stats['low_confidence_errors']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Validation Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved validation predictions to: /Volumes/SSanDisk/SpeechRec-German/artifacts/d-t_dl_models_with_context_v2/improved_models/hybrid_cnn_mlp_v4_3_enhanced/val_predictions_with_probs.csv\n"
     ]
    }
   ],
   "source": [
    "# Get validation predictions\n",
    "val_metrics, val_preds, val_labels, val_probs = evaluate_model(model, val_hybrid_loader, criterion, device)\n",
    "val_df = df[df['split'] == 'val'].reset_index(drop=True)\n",
    "\n",
    "val_predictions_data = []\n",
    "for idx, row in val_df.iterrows():\n",
    "    val_predictions_data.append({\n",
    "        'phoneme_id': row['phoneme_id'],\n",
    "        'utterance_id': row.get('utterance_id', None),\n",
    "        'phoneme': row.get('phoneme', row.get('class', None)),\n",
    "        'true_class': row['class'],\n",
    "        'true_class_encoded': int(val_labels[idx]),\n",
    "        'predicted_class_encoded': int(val_preds[idx]),\n",
    "        'predicted_class': 'd' if val_preds[idx] == 0 else 't',\n",
    "        'prob_class_0': float(val_probs[idx][0]),\n",
    "        'prob_class_1': float(val_probs[idx][1]),\n",
    "        'max_prob': float(np.max(val_probs[idx])),\n",
    "        'is_correct': int(val_labels[idx] == val_preds[idx]),\n",
    "        'confidence': float(np.max(val_probs[idx])) if val_labels[idx] == val_preds[idx] else float(val_probs[idx][val_preds[idx]]),\n",
    "        'duration_ms': row.get('duration_ms', None)\n",
    "    })\n",
    "\n",
    "val_predictions_df = pd.DataFrame(val_predictions_data)\n",
    "val_predictions_df.to_csv(save_dir / 'val_predictions_with_probs.csv', index=False)\n",
    "print(f\"Saved validation predictions to: {save_dir / 'val_predictions_with_probs.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis\n",
    "\n",
    "Visualize confusion matrix to understand model errors per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create confusion matrix for test set\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "# Get class names from label encoder\n",
    "class_names = le.classes_\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Confusion matrix with counts\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_xlabel('Predicted Class', fontsize=12)\n",
    "axes[0].set_ylabel('True Class', fontsize=12)\n",
    "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 2: Confusion matrix with percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "sns.heatmap(\n",
    "    cm_percent, \n",
    "    annot=True, \n",
    "    fmt='.1f', \n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_xlabel('Predicted Class', fontsize=12)\n",
    "axes[1].set_ylabel('True Class', fontsize=12)\n",
    "axes[1].set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(save_dir / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "logger.info(f\"Confusion matrix saved to: {save_dir / 'confusion_matrix.png'}\")\n",
    "plt.show()\n",
    "\n",
    "# Print detailed statistics\n",
    "logger.info(f\"\\n{'='*60}\")\n",
    "logger.info(f\"Confusion Matrix Analysis:\")\n",
    "logger.info(f\"{'='*60}\")\n",
    "for i, true_class in enumerate(class_names):\n",
    "    total_true = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    errors = total_true - correct\n",
    "    accuracy_per_class = (correct / total_true * 100) if total_true > 0 else 0\n",
    "    \n",
    "    logger.info(f\"\\nTrue Class: {true_class}\")\n",
    "    logger.info(f\"  Total samples: {total_true}\")\n",
    "    logger.info(f\"  Correctly predicted: {correct} ({accuracy_per_class:.2f}%)\")\n",
    "    logger.info(f\"  Incorrectly predicted: {errors}\")\n",
    "    \n",
    "    # Show error breakdown\n",
    "    for j, pred_class in enumerate(class_names):\n",
    "        if i != j and cm[i, j] > 0:\n",
    "            error_pct = (cm[i, j] / total_true * 100) if total_true > 0 else 0\n",
    "            logger.info(f\"    → Misclassified as '{pred_class}': {cm[i, j]} ({error_pct:.2f}%)\")\n",
    "\n",
    "# Calculate overall metrics\n",
    "total_samples = cm.sum()\n",
    "correct_predictions = cm.trace()\n",
    "total_errors = total_samples - correct_predictions\n",
    "\n",
    "logger.info(f\"\\n{'='*60}\")\n",
    "logger.info(f\"Overall Statistics:\")\n",
    "logger.info(f\"  Total test samples: {total_samples}\")\n",
    "logger.info(f\"  Correct predictions: {correct_predictions} ({correct_predictions/total_samples*100:.2f}%)\")\n",
    "logger.info(f\"  Total errors: {total_errors} ({total_errors/total_samples*100:.2f}%)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}